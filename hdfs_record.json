{"version": 2, "width": 210, "height": 46, "timestamp": 1611026015, "env": {"SHELL": "/bin/bash", "TERM": "xterm"}}
[0.219433, "o", "\u001b]0;root@k8snode1:~/bigdata/docker-hadoop\u0007\u001b[?1034h[root@k8snode1 docker-hadoop]# "]
[3.431739, "o", "t"]
[3.737261, "o", "m"]
[3.976933, "o", "u"]
[4.127167, "o", "x"]
[4.335381, "o", " "]
[4.980101, "o", "a"]
[5.438014, "o", "\r\n"]
[5.44206, "o", "\u001b[?1049h\u001b(B\u001b[m\u001b[?1l\u001b>\u001b[H\u001b[2J\u001b[?12l\u001b[?25h\u001b[?1000l\u001b[?1006l\u001b[?1005l\u001b[c\u001b[>4;1m\u001b[?1004h\u001b]112\u0007\u001b[?25l\u001b[1;1H[root@k8snode1 docker-hadoop]# \u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[30m\u001b[42m[1] 0:root@k8snode1:~/bigdata/docker-hadoop* 1:root@k8snode1:~/bigdata/docker-hadoop-                                                                                                   \"k8snode1\" 11:13 19-Jan-21\u001b(B\u001b[m\u001b[46;1H\u001b[1;46r\u001b[H\u001b[31C\u001b[?12l\u001b[?25h"]
[9.955739, "o", "\u001b[1;45r\u001b[H\u001b[31Cl\u001b[1;46r\u001b[H\u001b[32C"]
[10.040092, "o", "\u001b[1;45r\u001b[H\u001b[32Cs\u001b[1;46r\u001b[H\u001b[33C"]
[10.587939, "o", "\r\n"]
[10.591684, "o", "\u001b[1;45r\u001b[H\r\n\u001b[34m\u001b[1mbase\u001b(B\u001b[m  \u001b[34m\u001b[1mdatanode\u001b(B\u001b[m  docker-compose-v3.yml  docker-compose.yml  hadoop.env  hdfs_record.json  hd.json  \u001b[34m\u001b[1mhistoryserver\u001b(B\u001b[m  Makefile  \u001b[34m\u001b[1mnamenode\u001b(B\u001b[m  \u001b[34m\u001b[1mnginx\u001b(B\u001b[m  \u001b[34m\u001b[1mnodemanager\u001b(B\u001b[m  README.md  \u001b[34m\u001b[1mresourcemanager\u001b(B\u001b[m  \u001b[34m\u001b[1msubmit\u001b[1;46r\u001b[H\u001b[2B\u001b(B\u001b[m"]
[10.591952, "o", "\u001b[1;45r\u001b[H\u001b[2B[root@k8snode1 docker-hadoop]# \u001b[1;46r\u001b[H\u001b[3;32H"]
[13.020315, "o", "\r\u001b[9P\u001b[1;45r\u001b[H\u001b[2B(reverse-i-search)`':\u001b[1;46r\u001b[H\u001b[3;23H"]
[13.775811, "o", "\u001b[1;45r\u001b[H\u001b[3;20Hu': docker-compose up\u001b[1;46r\u001b[H\u001b[3;39H"]
[13.901528, "o", "\u001b[18D\u001b[1@\u001b[1;45r\u001b[H\u001b[3;21Hp\u001b[1;46r\u001b[H\u001b[3;40H"]
[15.550833, "o", "\r\u001b[7@\u001b[1;45r\u001b[H\u001b[2B[root@k8snode1 docker-hadoop]#\u001b[1;46r\u001b[H\u001b[3B"]
[16.249427, "o", "\u001b[1;45r\u001b[H\u001b[3BCreating network \"docker-hadoop_default\" with the default driver\u001b[1;46r\u001b[H\u001b[4B"]
[16.484458, "o", "\u001b[1;45r\u001b[H\u001b[4BCreating datanode2 ... \u001b[1;46r\u001b[H\u001b[5B"]
[16.497249, "o", "\u001b[1;45r\u001b[H\u001b[5BCreating namenode  ... \u001b[1;46r\u001b[H\u001b[6B"]
[16.510914, "o", "\u001b[1;45r\u001b[H\u001b[6BCreating datanode3 ... \u001b[1;46r\u001b[H\u001b[7B"]
[16.51474, "o", "\u001b[1;45r\u001b[H\u001b[7BCreating historyserver ... \u001b[1;46r\u001b[H\u001b[8B"]
[16.52263, "o", "\u001b[1;45r\u001b[H\u001b[8BCreating resourcemanager ... \u001b[1;46r\u001b[H\u001b[9B"]
[16.530145, "o", "\u001b[1;45r\u001b[H\u001b[9BCreating nodemanager     ... \u001b[1;46r\u001b[H\u001b[10B"]
[16.548609, "o", "\u001b[1;45r\u001b[H\u001b[10BCreating datanode        ... \u001b[1;46r\u001b[H\u001b[11B"]
[17.616653, "o", "\u001b[6d\u001b[K\u001b[1;45r\u001b[H\u001b[5BCreating namenode        ... \u001b[32mdone\u001b[1;46r\u001b[H\u001b[11B\u001b(B\u001b[m"]
[17.689158, "o", "\u001b[5d\u001b[K\u001b[1;45r\u001b[H\u001b[4BCreating datanode2       ... \u001b[32mdone\u001b[1;46r\u001b[H\u001b[11B\u001b(B\u001b[m"]
[17.712654, "o", "\u001b[3A\u001b[K\u001b[1;45r\u001b[H\u001b[8BCreating resourcemanager ... \u001b[32mdone\u001b[1;46r\u001b[H\u001b[11B\u001b(B\u001b[m"]
[17.858976, "o", "\u001b[4A\u001b[K\u001b[1;45r\u001b[H\u001b[7BCreating historyserver   ... \u001b[32mdone\u001b[1;46r\u001b[H\u001b[11B\u001b(B\u001b[m"]
[17.9135, "o", "\u001b[2A\u001b[K\u001b[1;45r\u001b[H\u001b[9BCreating nodemanager     ... \u001b[32mdone\u001b[1;46r\u001b[H\u001b[11B\u001b(B\u001b[m"]
[17.941576, "o", "\u001b[5A\u001b[K\u001b[1;45r\u001b[H\u001b[6BCreating datanode3       ... \u001b[32mdone\u001b[1;46r\u001b[H\u001b[11B\u001b(B\u001b[m"]
[17.988405, "o", "\u001b[A\u001b[K\u001b[1;45r\u001b[H\u001b[10BCreating datanode        ... \u001b[32mdone\u001b[1;46r\u001b[H\u001b[11B\u001b(B\u001b[m"]
[17.988965, "o", "\u001b[1;45r\u001b[H\u001b[11BAttaching to namenode, datanode2, resourcemanager, historyserver, nodemanager, datanode3, datanode\u001b[1;46r\u001b[H\u001b[12B"]
[18.00404, "o", "\u001b[1;45r\u001b[H\u001b[12B\u001b[36mdatanode           |\u001b[39m Configuring core\r\n\u001b[33mdatanode2          |\u001b[39m Configuring core\r\n\u001b[33mdatanode2          |\u001b[39m  - Setting hadoop.proxyuser.hue.hosts=*\r\n\u001b[33mdatanode2          |\u001b[39m  - Setting fs.defaultFS=hdfs://namenode:9000\r\n\u001b[33mdatanode2          |\u001b[39m  - Setting hadoop.http.staticuser.user=root\r\n\u001b[33mdatanode2          |\u001b[39m  - Setting io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec\r\n\u001b[33mdatanode2          |\u001b[39m  - Setting hadoop.proxyuser.hue.groups=*\r\n\u001b[33mdatanode2          |\u001b[39m Configuring hdfs\r\n\u001b[33mdatanode2          |\u001b[39m  - Setting dfs.datanode.data.dir=file:///hadoop/dfs/data\r\n\u001b[33mdatanode2          |\u001b[39m  - Setting dfs.namenode.datanode.registration.ip-hostname-check=false\r\n\u001b[36mdatanode           |\u001b[39m  - Setting hadoop.proxyuser.hue.hosts=*\r\n\u001b[33mdatanode2          |\u001b[39m  - Setting dfs.webhdfs.enabled=true\r\n\u001b[33mdatanode2          |\u001b[39m  - Setting dfs.permissions.enabled=false\r\n\u001b[33mdatanode2          |\u001b[39m Configuring yarn\r\n\u001b[33"]
[18.00447, "o", "mdatanode2          |\u001b[39m  - Setting yarn.timeline-service.enabled=true\r\n\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-vcores=4\r\n\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\r\n\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\r\n\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\r\n\u001b[32mdatanode3          |\u001b[39m Configuring core\r\n\u001b[32mdatanode3          |\u001b[39m  - Setting hadoop.proxyuser.hue.hosts=*\r\n\u001b[32mdatanode3          |\u001b[39m  - Setting fs.defaultFS=hdfs://namenode:9000\r\n\u001b[32mdatanode3          |\u001b[39m  - Setting hadoop.http.staticuser.user=root\r\n\u001b[32mdatanode3          |\u001b[39m  - Setting io.compression.codecs=org.apac"]
[18.005992, "o", "he.hadoop.io.compress.SnappyCodec\r\n\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\r\n\u001b[35mhistoryserver      |\u001b[39m Configuring core\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting hadoop.proxyuser.hue.hosts=*\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting fs.defaultFS=hdfs://namenode:9000\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting hadoop.http.staticuser.user=root\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting hadoop.proxyuser.hue.groups=*\r\n\u001b[35mhistoryserver      |\u001b[39m Configuring hdfs\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting dfs.namenode.datanode.registration.ip-hostname-check=false\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting dfs.webhdfs.enabled=true\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m Configuring core\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting hadoop.proxyuser.hue.hosts=*\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting fs.defaultFS=hdfs://namenode:9000\r\n\u001b[36m\u001b[1mno"]
[18.006294, "o", "demanager        |\u001b(B\u001b[m  - Setting hadoop.http.staticuser.user=root\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec\r\n\u001b[34mnamenode           |\u001b[39m Configuring core\r\n\u001b[34mnamenode           |\u001b[39m  - Setting hadoop.proxyuser.hue.hosts=*\r\n\u001b[34mnamenode           |\u001b[39m  - Setting fs.defaultFS=hdfs://namenode:9000\r\n\u001b[34mnamenode           |\u001b[39m  - Setting hadoop.http.staticuser.user=root\r\n\u001b[34mnamenode           |\u001b[39m  - Setting io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec\r\n\u001b[34mnamenode           |\u001b[39m  - Setting hadoop.proxyuser.hue.groups=*\r\n\u001b[34mnamenode           |\u001b[39m Configuring hdfs\r\n\u001b[34mnamenode           |\u001b[39m  - Setting dfs.namenode.datanode.registration.ip-hostname-check=false\r\n\u001b[34mnamenode           |\u001b[39m  - Setting dfs.webhdfs.enabled=true\r\n\u001b[34mnamenode           |\u001b[39m  - Setting dfs.permissions.enabled=false\r\n\u001b[34mnamenode           |\u001b[39m  - Setting dfs.namenode.name.dir=file:///hadoop/dfs/name\r\n\u001b[34mna"]
[18.006322, "o", "menode           |\u001b[39m Configuring yarn\r\n\u001b[34mnamenode           |\u001b[39m  - Settin\u001b[1;46r\u001b[H\u001b[45;31H\u001b[1;45r\u001b[H\u001b[45;31Hg yarn.timeline-service.enabled=true\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-vcores=4\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.timeline-service.generic-application-history.enabled=true\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.log-aggregation-ena"]
[18.006335, "o", "ble=true\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring core\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting hadoop.proxyuser.hue.hosts=*\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting fs.defaultFS=hdfs://namenode:9000\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting hadoop.http.staticuser.user=root\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting hadoop.proxyuser.hue.groups=*\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring hdfs\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting dfs.namenode.datanode.registration.ip-hostname-check=false\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting dfs.webhdfs.enabled=true\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting dfs.permissions.enabled=false\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring yarn\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.timeline-service.enabled=true\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.scheduler.capacity.root.defau"]
[18.006347, "o", "lt.maximum-allocation-vcores=4\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.008408, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting yarn.resourcemanager.hostname=resourcemanager\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.timeline-service.generic-application-history.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.010205, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting hadoop.proxyuser.hue.groups=*\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.015175, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting hadoop.proxyuser.hue.groups=*\r\n\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.timeline-service.generic-application-history.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.016187, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting fs.defaultFS=hdfs://namenode:9000\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.01678, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m Configuring hdfs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.020677, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m Configuring hdfs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.021537, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.log-aggregation-enable=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.024671, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting dfs.permissions.enabled=false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.029459, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.030776, "o", "\u001b[?25l\u001b[H\u001b[34mnamenode           |\u001b[39m  - Setting dfs.webhdfs.enabled=true\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting dfs.permissions.enabled=false\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting dfs.namenode.name.dir=file:///hadoop/dfs/name\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m Configuring yarn\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.timeline-service.enabled=true\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-vcores=4\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\u001b[K\r\n\u001b[34mnamenode           "]
[18.03083, "o", "|\u001b[39m  - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.timeline-service.generic-application-history.enabled=true\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.log-aggregation-enable=true\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring core\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting hadoop.proxyuser.hue.hosts=*\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting fs.defaultFS=hdfs://namenode:9000\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting hadoop.http.staticuser.user=root\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting hadoop.proxyuser.hue.groups=*\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring hdfs\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting dfs.namenode.datanode.registration.ip-hostname-check=false\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting dfs.webhdfs.enabled=true\u001b[K\r\n\u001b[33m\u001b[1mresourceman"]
[18.030843, "o", "ager    |\u001b(B\u001b[m  - Setting dfs.permissions.enabled=false\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring yarn\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.timeline-service.enabled=true\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-vcores=4\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.resourcemanager.hostname=reso"]
[18.030856, "o", "urcemanager\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.timeline-service.generic-application-history.enabled=true\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m  - Setting hadoop.proxyuser.hue.groups=*\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting hadoop.proxyuser.hue.groups=*\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.timeline-service.generic-application-history.enabled=true\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m  - Setting fs.defaultFS=hdfs://namenode:9000\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m Configuring hdfs\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m Configuring hdfs\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.log-aggregation-enable=true\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting dfs.permissions.enabled=false\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-mb=8192\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m  - Setting hadoop.http.staticuser.user=root\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m Configuring yarn\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[18.033001, "o", "\u001b[?25l\u001b[H\u001b[34mnamenode           |\u001b[39m  - Setting dfs.webhdfs.enabled=true\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting dfs.permissions.enabled=false\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting dfs.namenode.name.dir=file:///hadoop/dfs/name\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m Configuring yarn\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.timeline-service.enabled=true\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-vcores=4\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\u001b[K\r\n\u001b[34mnamenode           "]
[18.033255, "o", "|\u001b[39m  - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.timeline-service.generic-application-history.enabled=true\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.log-aggregation-enable=true\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring core\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting hadoop.proxyuser.hue.hosts=*\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting fs.defaultFS=hdfs://namenode:9000\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting hadoop.http.staticuser.user=root\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting hadoop.proxyuser.hue.groups=*\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring hdfs\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting dfs.namenode.datanode.registration.ip-hostname-check=false\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting dfs.webhdfs.enabled=true\u001b[K\r\n\u001b[33m\u001b[1mresourceman"]
[18.033281, "o", "ager    |\u001b(B\u001b[m  - Setting dfs.permissions.enabled=false\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring yarn\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.timeline-service.enabled=true\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-vcores=4\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.resourcemanager.hostname=reso"]
[18.033294, "o", "urcemanager\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.timeline-service.generic-application-history.enabled=true\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m  - Setting hadoop.proxyuser.hue.groups=*\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting hadoop.proxyuser.hue.groups=*\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.timeline-service.generic-application-history.enabled=true\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m  - Setting fs.defaultFS=hdfs://namenode:9000\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m Configuring hdfs\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m Configuring hdfs\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.log-aggregation-enable=true\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting dfs.permissions.enabled=false\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-mb=8192\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m  - Setting hadoop.http.staticuser.user=root\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m Configuring yarn\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdat"]
[18.033305, "o", "anode3          |\u001b[39m  - Setting dfs.datanode.data.dir=file:///hadoop/dfs/data\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.035675, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.log-aggregation-enable=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.037612, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting dfs.namenode.datanode.registration.ip-hostname-check=false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.041479, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.timeline-service.enabled=true\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.hostname=resourcemanager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.045177, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting yarn.nodemanager.aux-services=mapreduce_shuffle\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.049461, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting dfs.namenode.datanode.registration.ip-hostname-check=false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.053266, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.resourcemanager.hostname=resourcemanager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.053881, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.0547, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting dfs.webhdfs.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.057866, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-vcores=4\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.063473, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.064276, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting yarn.resourcemanager.resource_tracker.address=resourcemanager:8031\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.068145, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting dfs.webhdfs.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.068363, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting dfs.permissions.enabled=false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.068888, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.076384, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting hadoop.proxyuser.hue.groups=*\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.077248, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.07957, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m Configuring yarn\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.079873, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.aux-services=mapreduce_shuffle\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.080773, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting dfs.permissions.enabled=false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.081065, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting yarn.timeline-service.hostname=historyserver\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.088383, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.nodemanager.aux-services=mapreduce_shuffle\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.091942, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m Configuring yarn\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.093107, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m Configuring hdfs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.093485, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.timeline-service.leveldb-timeline-store.path=/hadoop/yarn/timeline\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.09861, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.timeline-service.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.100969, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting yarn.resourcemanager.scheduler.address=resourcemanager:8030\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.105005, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.resourcemanager.resource_tracker.address=resourcemanager:8031\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.106781, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting dfs.datanode.data.dir=file:///hadoop/dfs/data\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.107974, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.resource_tracker.address=resourcemanager:8031\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.110109, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.timeline-service.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.117202, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-vcores=4\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.118007, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.timeline-service.hostname=historyserver\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.118636, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting yarn.resourcemanager.address=resourcemanager:8032\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.119392, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.127619, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.timeline-service.hostname=historyserver\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.130263, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting dfs.namenode.datanode.registration.ip-hostname-check=false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.132726, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-vcores=4\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.134352, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.13447, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.resourcemanager.scheduler.address=resourcemanager:8030\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.136124, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.137129, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting mapred.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.147201, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.scheduler.address=resourcemanager:8030\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.148107, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.150507, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.151368, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting dfs.webhdfs.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.154808, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.resourcemanager.address=resourcemanager:8032\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.157636, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting yarn.nodemanager.remote-app-log-dir=/app-logs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.164287, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.address=resourcemanager:8032\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.169026, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting mapred.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.169664, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.timeline-service.generic-application-history.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.170181, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.170733, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.171359, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting dfs.permissions.enabled=false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.173276, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.182717, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.nodemanager.remote-app-log-dir=/app-logs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.184277, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.log-aggregation-enable=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.186496, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapred.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.188401, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m Configuring yarn\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\r\n\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.192719, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting mapreduce.map.output.compress=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.200703, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.resourcemanager.hostname=resourcemanager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.202718, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.205878, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting yarn.timeline-service.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.207628, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.remote-app-log-dir=/app-logs\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.20994, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting yarn.nodemanager.resource.memory-mb=16384\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.210347, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.217305, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.218863, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting mapreduce.map.output.compress=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.221011, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-vcores=4\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.225476, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.226815, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting yarn.resourcemanager.recovery.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.228396, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.timeline-service.generic-application-history.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.237627, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.nodemanager.resource.memory-mb=16384\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.map.output.compress=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.23806, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.nodemanager.aux-services=mapreduce_shuffle\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.240068, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting yarn.nodemanager.resource.cpu-vcores=8\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.242108, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.timeline-service.generic-application-history.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.251144, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.251871, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.log-aggregation-enable=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.253906, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m Configuring httpfs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.255532, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.resourcemanager.resource_tracker.address=resourcemanager:8031\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.resource.memory-mb=16384\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.25717, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.resourcemanager.recovery.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.260117, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m Configuring kms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.265412, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.resourcemanager.hostname=resourcemanager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.271822, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.timeline-service.hostname=historyserver\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.272133, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m Configuring mapred\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.273804, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.log-aggregation-enable=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.274961, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.nodemanager.resource.cpu-vcores=8\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.recovery.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.280962, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.283146, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.285631, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.resourcemanager.hostname=resourcemanager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.286914, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m Configuring httpfs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.290461, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting mapreduce.map.java.opts=-Xmx3072m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.291696, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.resourcemanager.scheduler.address=resourcemanager:8030\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.295141, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m Configuring kms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.296205, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.resource.cpu-vcores=8\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.297139, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.300799, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m Configuring mapred\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.307347, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.nodemanager.aux-services=mapreduce_shuffle\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.resourcemanager.address=resourcemanager:8032\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring httpfs\r\n\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.311385, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting mapreduce.reduce.java.opts=-Xmx6144m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.314943, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring kms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.315459, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting yarn.timeline-service.generic-application-history.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.31886, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring mapred\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.320907, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting mapred.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.321479, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.resourcemanager.resource_tracker.address=resourcemanager:8031\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.322009, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting mapreduce.map.java.opts=-Xmx3072m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.325852, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.nodemanager.aux-services=mapreduce_shuffle\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.330896, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting yarn.log-aggregation-enable=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.331268, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting mapreduce.reduce.memory.mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.337806, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.timeline-service.hostname=historyserver\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.338441, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.map.java.opts=-Xmx3072m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.34179, "o", "\u001b[?25l\u001b[H\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.nodemanager.aux-services=mapreduce_shuffle\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m  - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting yarn.nodemanager.resource.cpu-vcores=8\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.timeline-service.generic-application-history.enabled=true\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m  - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.log-aggregation-enable=true\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m Configuring httpfs\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.resourcemanager.resource_tracker.address=resourcemanager:8031\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.resource.memory-mb=16384\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.resourcemanager.recovery.enabled=true\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m C"]
[18.341886, "o", "onfiguring kms\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m  - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.resourcemanager.hostname=resourcemanager\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.timeline-service.hostname=historyserver\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m Configuring mapred\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.log-aggregation-enable=true\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.nodemanager.resource.cpu-vcores=8\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.recovery.enabled=true\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m  - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-mb=8192\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.resourcemanager.hostname=resourcemanager\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m Configuring httpfs\u001b[K"]
[18.341904, "o", "\r\n\u001b[34mnamenode           |\u001b[39m  - Setting mapreduce.map.java.opts=-Xmx3072m\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.resourcemanager.scheduler.address=resourcemanager:8030\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m Configuring kms\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.resource.cpu-vcores=8\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m  - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m Configuring mapred\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.nodemanager.aux-services=mapreduce_shuffle\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.resourcemanager.address=resourcemanager:8032\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring httpfs\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-mb=8192\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting mapreduce.reduce.java.opts=-Xmx6144m\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring kms\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m  - Setting y"]
[18.341916, "o", "arn.timeline-service.generic-application-history.enabled=true\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring mapred\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting mapred.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.resourcemanager.resource_tracker.address=resourcemanager:8031\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m  - Setting mapreduce.map.java.opts=-Xmx3072m\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.nodemanager.aux-services=mapreduce_shuffle\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m  - Setting yarn.log-aggregation-enable=true\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m  - Setting mapreduce.reduce.memory.mb=8192\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.timeline-service.hostname=historyserver\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.map.java.opts=-Xmx3072m\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.resourcemanager.resource_tracker.address=resourcemanager:8031\r\n\u001b[33mdat"]
[18.341927, "o", "anode2          |\u001b[39m  - Setting mapreduce.reduce.java.opts=-Xmx6144m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.345421, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.nodemanager.remote-app-log-dir=/app-logs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.34742, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting yarn.resourcemanager.hostname=resourcemanager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.356219, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.reduce.java.opts=-Xmx6144m\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting mapreduce.reduce.memory.mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.357177, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.resourcemanager.scheduler.address=resourcemanager:8030\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.359436, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting mapreduce.map.memory.mb=4096\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.361387, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.362236, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.timeline-service.hostname=historyserver\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.366445, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.373352, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.reduce.memory.mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.374542, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.resourcemanager.address=resourcemanager:8032\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.375484, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.378343, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.resourcemanager.scheduler.address=resourcemanager:8030\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.38085, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting mapred.child.java.opts=-Xmx4096m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.382228, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting yarn.nodemanager.aux-services=mapreduce_shuffle\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.384642, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting mapreduce.map.output.compress=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.391424, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting mapred.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.391973, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.resourcemanager.address=resourcemanager:8032\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.394802, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting mapreduce.map.memory.mb=4096\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.397642, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.39942, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting yarn.resourcemanager.resource_tracker.address=resourcemanager:8031\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.400069, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.nodemanager.resource.memory-mb=16384\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.403919, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.404543, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.nodemanager.remote-app-log-dir=/app-logs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.410416, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting mapred.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.415633, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting yarn.timeline-service.hostname=historyserver\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.416384, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting mapred.child.java.opts=-Xmx4096m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.417305, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.map.memory.mb=4096\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.420272, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.resourcemanager.recovery.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.421898, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.422902, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.nodemanager.remote-app-log-dir=/app-logs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.429746, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting yarn.resourcemanager.scheduler.address=resourcemanager:8030\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.43243, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapred.child.java.opts=-Xmx4096m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.435112, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting mapreduce.framework.name=yarn\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.436719, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.437846, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.nodemanager.resource.cpu-vcores=8\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting mapreduce.map.output.compress=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.440432, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.448916, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[35mhistoryserver      |\u001b[39m Configuring httpfs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.450716, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting yarn.resourcemanager.address=resourcemanager:8032\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.452431, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting mapreduce.framework.name=yarn\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.455336, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m  - Setting mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.456312, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m Configuring kms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.461509, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.nodemanager.resource.memory-mb=16384\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting mapred.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.463162, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting mapreduce.map.output.compress=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.467909, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m Configuring mapred\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.469493, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m  - Setting mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[34mnamenode           |\u001b[39m Configuring for multihomed network\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.470848, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.framework.name=yarn\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.476326, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting yarn.nodemanager.remote-app-log-dir=/app-logs\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.resourcemanager.recovery.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.478916, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m Configuring for multihomed network\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.484002, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting mapreduce.map.java.opts=-Xmx3072m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.485422, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.nodemanager.resource.memory-mb=16384\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.48875, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.489578, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.49428, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.nodemanager.resource.cpu-vcores=8\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.50192, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.resourcemanager.recovery.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.503881, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring for multihomed network\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.506646, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m Configuring httpfs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.507765, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting mapreduce.map.output.compress=true\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting mapreduce.reduce.java.opts=-Xmx6144m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.51258, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m Configuring kms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.522079, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m Configuring mapred\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.523994, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.nodemanager.resource.cpu-vcores=8\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.524973, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting mapreduce.reduce.memory.mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.526061, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting yarn.nodemanager.resource.memory-mb=16384\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.537352, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m Configuring httpfs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.542511, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.543118, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting yarn.resourcemanager.recovery.enabled=true\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting mapreduce.map.java.opts=-Xmx3072m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.54647, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m Configuring kms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.553777, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m Configuring mapred\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.563102, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting mapreduce.reduce.java.opts=-Xmx6144m\r\n\u001b[36mdatanode           |\u001b[39m  - Setting yarn.nodemanager.resource.cpu-vcores=8\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.56368, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting mapreduce.map.memory.mb=4096\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.566118, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting mapreduce.map.java.opts=-Xmx3072m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.575964, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting mapreduce.reduce.memory.mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.576415, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m Configuring httpfs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.580486, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting mapreduce.reduce.java.opts=-Xmx6144m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.583721, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m Configuring kms\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting mapred.child.java.opts=-Xmx4096m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.590019, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m Configuring mapred\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.591881, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m [1/100] check for namenode:9870...\r\n\u001b[33mdatanode2          |\u001b[39m [1/100] namenode:9870 is not available yet\r\n\u001b[33mdatanode2          |\u001b[39m [1/100] try in 5s once again ...\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.592281, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.600105, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting mapreduce.reduce.memory.mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.607151, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting mapreduce.map.java.opts=-Xmx3072m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.612136, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] check for namenode:9000...\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] namenode:9000 is not available yet\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] try in 5s once again ...\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.61324, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting mapreduce.framework.name=yarn\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.613399, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting mapreduce.map.memory.mb=4096\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.61939, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[36mdatanode           |\u001b[39m  - Setting mapreduce.reduce.java.opts=-Xmx6144m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.624857, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m  - Setting mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.626345, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting mapred.child.java.opts=-Xmx4096m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.631147, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting mapreduce.map.memory.mb=4096\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.634344, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m Configuring for multihomed network\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.636379, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting mapreduce.reduce.memory.mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.64012, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.646877, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting mapred.child.java.opts=-Xmx4096m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.64749, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.658841, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting mapreduce.map.memory.mb=4096\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting mapreduce.framework.name=yarn\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.659601, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.668509, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting mapred.child.java.opts=-Xmx4096m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.669549, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.675112, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting mapreduce.framework.name=yarn\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.675484, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m Configuring for multihomed network\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.685714, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.690182, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m  - Setting mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.698241, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m Configuring for multihomed network\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.701258, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting mapreduce.framework.name=yarn\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.710842, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m [1/100] check for namenode:9000...\r\n\u001b[35mhistoryserver      |\u001b[39m [1/100] namenode:9000 is not available yet\r\n\u001b[35mhistoryserver      |\u001b[39m [1/100] try in 5s once again ...\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.712959, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m  - Setting mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.715594, "o", "\u001b[?25l\u001b[H\u001b[32mdatanode3          |\u001b[39m  - Setting mapreduce.map.java.opts=-Xmx3072m\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting mapreduce.reduce.memory.mb=8192\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m Configuring httpfs\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m  - Setting mapreduce.reduce.java.opts=-Xmx6144m\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m Configuring kms\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting mapred.child.java.opts=-Xmx4096m\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m Configuring mapred\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m [1/100] check for namenode:9870...\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m [1/100] namenode:9870 is not available yet\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m [1/100] try in 5s once again ...\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m  - Setting mapreduce.reduce.memory.mb=8192\u001b[K\r\n\u001b[36mdatanode    "]
[18.715646, "o", "       |\u001b[39m  - Setting mapreduce.map.java.opts=-Xmx3072m\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] check for namenode:9000...\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] namenode:9000 is not available yet\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] try in 5s once again ...\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting mapreduce.framework.name=yarn\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting mapreduce.map.memory.mb=4096\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m  - Setting yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m  - Setting mapreduce.reduce.java.opts=-Xmx6144m\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m  - Setting mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting mapred.child.java.opts=-Xmx4096m\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m  - Setting mapreduce.map.memory.mb=4096\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m Configuring for multihomed network\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m  - Setting mapreduc"]
[18.715659, "o", "e.reduce.memory.mb=8192\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m  - Setting mapred.child.java.opts=-Xmx4096m\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m  - Setting yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m  - Setting mapreduce.map.memory.mb=4096\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting mapreduce.framework.name=yarn\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m  - Setting mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m  - Setting mapred.child.java.opts=-Xmx4096m\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m  - Setting mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m  - Setting mapreduce.framework.name=yarn\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m Configuring for multihomed network\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m  - Setting mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1"]
[18.715672, "o", "/\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m  - Setting mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m Configuring for multihomed network\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m  - Setting mapreduce.framework.name=yarn\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m [1/100] check for namenode:9000...\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m [1/100] namenode:9000 is not available yet\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m [1/100] try in 5s once again ...\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m  - Setting mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[18.723699, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m Configuring for multihomed network\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.754002, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m [1/100] check for namenode:9000...\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m [1/100] namenode:9000 is not available yet\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m [1/100] try in 5s once again ...\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.763402, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m [1/100] check for namenode:9870...\r\n\u001b[32mdatanode3          |\u001b[39m [1/100] namenode:9870 is not available yet\r\n\u001b[32mdatanode3          |\u001b[39m [1/100] try in 5s once again ...\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[18.785448, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m [1/100] check for namenode:9870...\r\n\u001b[36mdatanode           |\u001b[39m [1/100] namenode:9870 is not available yet\r\n\u001b[36mdatanode           |\u001b[39m [1/100] try in 5s once again ...\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[19.140831, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,152 INFO namenode.NameNode: STARTUP_MSG: \r\n\u001b[34mnamenode           |\u001b[39m /************************************************************\r\n\u001b[34mnamenode           |\u001b[39m STARTUP_MSG: Starting NameNode\r\n\u001b[34mnamenode           |\u001b[39m STARTUP_MSG:   host = 73505f4f28ca/172.30.0.2\r\n\u001b[34mnamenode           |\u001b[39m STARTUP_MSG:   args = []\r\n\u001b[34mnamenode           |\u001b[39m STARTUP_MSG:   version = 3.2.1\r\n\u001b[34mnamenode           |\u001b[39m STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop-3.2.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/opt/hadoop-3.2.1/share/h"]
[19.141262, "o", "adoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-text-1.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/dnsjava-2.1.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/error_prone_a"]
[19.143188, "o", "nnotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/"]
[19.144735, "o", "opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-servlet-9.3.24.v20180605.jar:/op"]
[19.145289, "o", "t/hadoop-3.2.1/share/\u001b[1;46r\u001b[H\u001b[45;74H\u001b[1;45r\u001b[H\u001b[45;74Hhadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.1/sh"]
[19.145366, "o", "are/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/re2j"]
[19.14544, "o", "-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-kms-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3."]
[19.145493, "o", "2.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-text-1.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2."]
[19.145536, "o", "1/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/op\u001b[1;46r\u001b[H\u001b[45;179H\u001b[1;45r\u001b[H\u001b[45;179Ht/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/h"]
[19.145575, "o", "dfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/s"]
[19.145626, "o", "hare/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb"]
[19.145665, "o", "-core-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/"]
[19.145704, "o", "lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/opt/\u001b[1;46r\u001b[H\u001b[45;74H\u001b[1;45r\u001b[H\u001b[45;74Hhadoop-3.2.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs"]
[19.145742, "o", "/hadoop-hdfs-native-client-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/opt/hado"]
[19.145756, "o", "op-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/"]
[19.145772, "o", "lib/jackson-jaxrs-json-provider-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/had"]
[19.145785, "o", "oop-yarn-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/h"]
[19.145847, "o", "\u001b[1;46r\u001b[H\u001b[45;179H\u001b[1;45r\u001b[H\u001b[45;179Hadoop-yarn-registry-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar\r\n\u001b[34mnamenode           |\u001b[39m ST"]
[19.145862, "o", "ARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z\r\n\u001b[34mnamenode           |\u001b[39m STARTUP_MSG:   java = 1.8.0_232\r\n\u001b[34mnamenode           |\u001b[39m ************************************************************/\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,160 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[19.229003, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,244 INFO namenode.NameNode: createNameNode []\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[19.330416, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,348 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[19.420278, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,437 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,437 INFO impl.MetricsSystemImpl: NameNode metrics system started\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[19.44889, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,466 INFO namenode.NameNodeUtils: fs.defaultFS is hdfs://namenode:9000\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,466 INFO namenode.NameNode: Clients should use namenode:9000 to access this namenode/service.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[19.559796, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,577 INFO util.JvmPauseMonitor: Starting JVM pause monitor\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[19.594621, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,612 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:9870\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[19.607298, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,625 INFO util.log: Logging initialized @909ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[19.699703, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,717 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[19.709725, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,727 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[19.718322, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,735 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[19.720144, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,737 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,737 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,737 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[19.741387, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,758 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,758 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[19.750165, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,767 INFO http.HttpServer2: Jetty bound to port 9870\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[19.751163, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,769 INFO server.Server: jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[19.778627, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,796 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d0b7e3c{/logs,file:///opt/hadoop-3.2.1/logs/,AVAILABLE}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[19.77974, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,796 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4b741d6d{/static,file:///opt/hadoop-3.2.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[19.846333, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,863 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7486b455{/,file:///opt/hadoop-3.2.1/share/hadoop/hdfs/webapps/hdfs/,AVAILABLE}{/hdfs}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[19.851645, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,868 INFO server.AbstractConnector: Started ServerConnector@700fb871{HTTP/1.1,[http/1.1]}{0.0.0.0:9870}\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:54,869 INFO server.Server: Started @1153ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.011807, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,029 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.011961, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,029 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.058366, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,074 INFO namenode.FSEditLog: Edit logging is async:true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.120999, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,137 INFO namenode.FSNamesystem: KeyProvider: null\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,138 INFO namenode.FSNamesystem: fsLock is fair: true\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,138 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.12604, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,143 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,143 INFO namenode.FSNamesystem: supergroup          = supergroup\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,143 INFO namenode.FSNamesystem: isPermissionEnabled = false\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,143 INFO namenode.FSNamesystem: HA Enabled: false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.168859, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,177 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.172182, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,186 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,186 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.17363, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,191 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,191 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Jan 19 03:13:55\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.175179, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,193 INFO util.GSet: Computing capacity for map BlocksMap\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,193 INFO util.GSet: VM type       = 64-bit\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.17768, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,195 INFO util.GSet: 2.0% max memory 2.6 GB = 52.7 MB\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,195 INFO util.GSet: capacity      = 2^23 = 8388608 entries\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.192223, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,209 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,209 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.199405, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,216 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.20005, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,216 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,216 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,216 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,217 INFO blockmanagement.BlockManager: defaultReplication         = 3\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,217 INFO blockmanagement.BlockManager: maxReplication             = 512\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,217 INFO blockmanagement.BlockManager: minReplication             = 1\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.200842, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,217 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,217 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,217 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,217 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.226187, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,243 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,243 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,243 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,243 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.23921, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,256 INFO util.GSet: Computing capacity for map INodeMap\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,256 INFO util.GSet: VM type       = 64-bit\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,256 INFO util.GSet: 1.0% max memory 2.6 GB = 26.3 MB\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,256 INFO util.GSet: capacity      = 2^22 = 4194304 entries\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.249452, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,266 INFO namenode.FSDirectory: ACLs enabled? false\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,266 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,266 INFO namenode.FSDirectory: XAttrs enabled? true\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,267 INFO namenode.NameNode: Caching file names occurring more than 10 times\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.256434, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,271 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,273 INFO snapshot.SnapshotManager: SkipList is disabled\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.261323, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,278 INFO util.GSet: Computing capacity for map cachedBlocks\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,278 INFO util.GSet: VM type       = 64-bit\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,278 INFO util.GSet: 0.25% max memory 2.6 GB = 6.6 MB\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,278 INFO util.GSet: capacity      = 2^20 = 1048576 entries\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.277668, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,289 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,289 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,289 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.277855, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,292 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,292 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,294 INFO util.GSet: Computing capacity for map NameNodeRetryCache\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,294 INFO util.GSet: VM type       = 64-bit\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,294 INFO util.GSet: 0.029999999329447746% max memory 2.6 GB = 809.5 KB\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,294 INFO util.GSet: capacity      = 2^17 = 131072 entries\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.297298, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,311 INFO common.Storage: Lock on /hadoop/dfs/name/in_use.lock acquired by nodename 359@73505f4f28ca\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.326647, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,339 INFO namenode.FileJournalManager: Recovering unfinalized segments in /hadoop/dfs/name/current\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.357351, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,365 INFO namenode.FileJournalManager: Finalizing edits file /hadoop/dfs/name/current/edits_inprogress_0000000000000000923 -> /hadoop/dfs/name/current/edits_0000000000000000923-0000000000000000951\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.376105, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,386 INFO namenode.FSImage: Planning to load image: FSImageFile(file=/hadoop/dfs/name/current/fsimage_0000000000000000918, cpktTxId=0000000000000000918)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.576371, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,594 INFO namenode.FSImageFormatPBINode: Loading 80 INodes.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.617486, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,634 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,634 INFO namenode.FSImage: Loaded image for txid 918 from /hadoop/dfs/name/current/fsimage_0000000000000000918\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.61873, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,636 INFO namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@b1712f3 expecting start txid #919\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,636 INFO namenode.FSImage: Start loading edits file /hadoop/dfs/name/current/edits_0000000000000000919-0000000000000000919 maxTxnsToRead = 9223372036854775807\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.620081, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,637 INFO namenode.RedundantEditLogInputStream: Fast-forwarding stream '/hadoop/dfs/name/current/edits_0000000000000000919-0000000000000000919' to transaction ID 919\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.648765, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,666 INFO namenode.FSImage: Loaded 1 edits file(s) (the last named /hadoop/dfs/name/current/edits_0000000000000000919-0000000000000000919) of total size 1048576.0, total edits 1.0, total load time 13.0 ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.649332, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,666 INFO namenode.RedundantEditLogInputStream: Fast-forwarding stream '/hadoop/dfs/name/current/edits_0000000000000000920-0000000000000000920' to transaction ID 919\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.649872, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,667 INFO namenode.RedundantEditLogInputStream: Fast-forwarding stream '/hadoop/dfs/name/current/edits_0000000000000000921-0000000000000000921' to transaction ID 919\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.651164, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,669 INFO namenode.RedundantEditLogInputStream: Fast-forwarding stream '/hadoop/dfs/name/current/edits_0000000000000000922-0000000000000000922' to transaction ID 919\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.652203, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,670 INFO namenode.RedundantEditLogInputStream: Fast-forwarding stream '/hadoop/dfs/name/current/edits_0000000000000000923-0000000000000000951' to transaction ID 919\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.701746, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,719 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.702775, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,719 INFO namenode.FSEditLog: Starting log segment at 952\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.801767, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,819 INFO namenode.NameCache: initialized with 0 entries 0 lookups\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,819 INFO namenode.FSNamesystem: Finished loading FSImage in 521 msecs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.958099, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,975 INFO namenode.NameNode: RPC server is binding to 0.0.0.0:9000\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.985404, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:56,002 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[20.988199, "o", "\u001b[?25l\u001b[H\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,266 INFO namenode.FSDirectory: ACLs enabled? false\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,266 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,266 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,267 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,271 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,273 INFO snapshot.SnapshotManager: SkipList is disabled\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,278 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,278 INFO util.GSet: VM type       = "]
[20.988688, "o", "64-bit\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,278 INFO util.GSet: 0.25% max memory 2.6 GB = 6.6 MB\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,278 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,289 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,289 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,289 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,292 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,292 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,294 INFO util.GSet: Computing capacit"]
[20.989069, "o", "y for map NameNodeRetryCache\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,294 INFO util.GSet: VM type       = 64-bit\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,294 INFO util.GSet: 0.029999999329447746% max memory 2.6 GB = 809.5 KB\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,294 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,311 INFO common.Storage: Lock on /hadoop/dfs/name/in_use.lock acquired by nodename 359@73505f4f28ca\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,339 INFO namenode.FileJournalManager: Recovering unfinalized segments in /hadoop/dfs/name/current\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,365 INFO namenode.FileJournalManager: Finalizing edits file /hadoop/dfs/name/current/edits_inprogress_0000000000000000923 -> /hadoop/dfs/name/current/edits_0000000000000000923-0000000000000000951\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,386 INFO namenode.FSImage: Planning to load "]
[20.989335, "o", "image: FSImageFile(file=/hadoop/dfs/name/current/fsimage_0000000000000000918, cpktTxId=0000000000000000918)\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,594 INFO namenode.FSImageFormatPBINode: Loading 80 INodes.\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,634 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,634 INFO namenode.FSImage: Loaded image for txid 918 from /hadoop/dfs/name/current/fsimage_0000000000000000918\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,636 INFO namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@b1712f3 expecting start txid #919\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,636 INFO namenode.FSImage: Start loading edits file /hadoop/dfs/name/current/edits_0000000000000000919-0000000000000000919 maxTxnsToRead = 9223372036854775807\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,637 INFO namenode.RedundantEditLogInputStream: Fast"]
[20.989362, "o", "-forwarding stream '/hadoop/dfs/name/current/edits_0000000000000000919-0000000000000000919' to transaction ID 919\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,666 INFO namenode.FSImage: Loaded 1 edits file(s) (the last named /hadoop/dfs/name/current/edits_0000000000000000919-0000000000000000919) of total size 1048576.0, total edits 1.0, total load time 13.0 ms\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,666 INFO namenode.RedundantEditLogInputStream: Fast-forwarding stream '/hadoop/dfs/name/current/edits_0000000000000000920-0000000000000000920' to transaction ID 919\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,667 INFO namenode.RedundantEditLogInputStream: Fast-forwarding stream '/hadoop/dfs/name/current/edits_0000000000000000921-0000000000000000921' to transaction ID 919\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,669 INFO namenode.RedundantEditLogInputStream: Fast-forwarding stream '/hadoop/dfs/name/current/edits_0000000000000000922-0000000000000000922' to transactio"]
[20.989374, "o", "n ID 919\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,670 INFO namenode.RedundantEditLogInputStream: Fast-forwarding stream '/hadoop/dfs/name/current/edits_0000000000000000923-0000000000000000951' to transaction ID 919\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,719 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,719 INFO namenode.FSEditLog: Starting log segment at 952\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,819 INFO namenode.NameCache: initialized with 0 entries 0 lookups\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,819 INFO namenode.FSNamesystem: Finished loading FSImage in 521 msecs\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:55,975 INFO namenode.NameNode: RPC server is binding to 0.0.0.0:9000\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:56,002 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBloc"]
[20.989389, "o", "kingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[20.995869, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:56,013 INFO ipc.Server: Starting Socket Reader #1 for port 9000\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[21.158351, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:56,174 INFO namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[21.16865, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:56,186 INFO namenode.LeaseManager: Number of blocks under construction: 0\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[21.179609, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:56,197 INFO hdfs.StateChange: STATE* Safe mode ON. \r\n\u001b[34mnamenode           |\u001b[39m The reported blocks 0 needs additional 54 blocks to reach the threshold 0.9990 of total blocks 55.\r\n\u001b[34mnamenode           |\u001b[39m The minimum number of live datanodes is not required. Safe mode will be turned off automatically once the thresholds have been reached.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[21.199825, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:56,217 INFO ipc.Server: IPC Server Responder: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[21.201175, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:56,218 INFO ipc.Server: IPC Server listener on 9000: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[21.205763, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:56,223 INFO namenode.NameNode: NameNode RPC up at: namenode/172.30.0.2:9000\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[21.208835, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:56,226 INFO namenode.FSNamesystem: Starting services required for active state\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:56,226 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[21.214911, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:56,231 INFO namenode.FSDirectory: Quota initialization completed in 5 milliseconds\r\n\u001b[34mnamenode           |\u001b[39m name space=80\r\n\u001b[34mnamenode           |\u001b[39m storage space=2989237527\r\n\u001b[34mnamenode           |\u001b[39m storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[21.217762, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:13:56,235 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[23.595916, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m [2/100] namenode:9870 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[23.619547, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m [2/100] namenode:9000 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[23.622355, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] namenode:9870 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[23.62517, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] check for datanode:9864...\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] datanode:9864 is not available yet\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] try in 5s once again ...\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[23.718357, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m [2/100] namenode:9000 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[23.720998, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m [1/100] namenode:9870 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[23.72336, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m [1/100] check for datanode:9864...\r\n\u001b[35mhistoryserver      |\u001b[39m [1/100] datanode:9864 is not available yet\r\n\u001b[35mhistoryserver      |\u001b[39m [1/100] try in 5s once again ...\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[23.758831, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m [2/100] namenode:9000 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[23.761883, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m [1/100] namenode:9870 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[23.764546, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m [1/100] check for datanode:9864...\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m [1/100] datanode:9864 is not available yet\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m [1/100] try in 5s once again ...\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[23.770193, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m [2/100] namenode:9870 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[23.797985, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m [2/100] namenode:9870 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[24.216901, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:13:59,227 INFO datanode.DataNode: STARTUP_MSG: \r\n\u001b[33mdatanode2          |\u001b[39m /************************************************************\r\n\u001b[33mdatanode2          |\u001b[39m STARTUP_MSG: Starting DataNode\r\n\u001b[33mdatanode2          |\u001b[39m STARTUP_MSG:   host = deb6331226b2/172.30.0.3\r\n\u001b[33mdatanode2          |\u001b[39m STARTUP_MSG:   args = []\r\n\u001b[33mdatanode2          |\u001b[39m STARTUP_MSG:   version = 3.2.1\r\n\u001b[33mdatanode2          |\u001b[39m STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop-3.2.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/opt/hadoop-3.2.1/share/h"]
[24.217489, "o", "adoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-text-1.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/dnsjava-2.1.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/error_prone_a"]
[24.219287, "o", "nnotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/"]
[24.221044, "o", "opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-servlet-9.3.24.v20180605.jar:/op"]
[24.221387, "o", "t/hadoop-3.2.1/share/\u001b[1;46r\u001b[H\u001b[45;74H\u001b[1;45r\u001b[H\u001b[45;74Hhadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.1/sh"]
[24.222127, "o", "are/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/re2j"]
[24.222334, "o", "-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-kms-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3."]
[24.222509, "o", "2.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-text-1.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2."]
[24.222685, "o", "1/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/op\u001b[1;46r\u001b[H\u001b[45;179H\u001b[1;45r\u001b[H\u001b[45;179Ht/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/h"]
[24.222834, "o", "dfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/s"]
[24.222967, "o", "hare/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb"]
[24.223103, "o", "-core-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/"]
[24.223233, "o", "lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/opt/\u001b[1;46r\u001b[H\u001b[45;74H\u001b[1;45r\u001b[H\u001b[45;74Hhadoop-3.2.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs"]
[24.223359, "o", "/hadoop-hdfs-native-client-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/opt/hado"]
[24.223379, "o", "op-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/"]
[24.223392, "o", "lib/jackson-jaxrs-json-provider-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/had"]
[24.223403, "o", "oop-yarn-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/h"]
[24.223711, "o", "\u001b[1;46r\u001b[H\u001b[45;179H\u001b[1;45r\u001b[H\u001b[45;179Hadoop-yarn-registry-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar\r\n\u001b[33mdatanode2          |\u001b[39m ST"]
[24.223735, "o", "ARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z\r\n\u001b[33mdatanode2          |\u001b[39m STARTUP_MSG:   java = 1.8.0_232\r\n\u001b[33mdatanode2          |\u001b[39m ************************************************************/\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:13:59,236 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[24.561669, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:13:59,571 INFO datanode.DataNode: STARTUP_MSG: \r\n\u001b[32mdatanode3          |\u001b[39m /************************************************************\r\n\u001b[32mdatanode3          |\u001b[39m STARTUP_MSG: Starting DataNode\r\n\u001b[32mdatanode3          |\u001b[39m STARTUP_MSG:   host = 9e22ace955e1/172.30.0.7\r\n\u001b[32mdatanode3          |\u001b[39m STARTUP_MSG:   args = []\r\n\u001b[32mdatanode3          |\u001b[39m STARTUP_MSG:   version = 3.2.1\r\n\u001b[32mdatanode3          |\u001b[39m STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop-3.2.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/opt/hadoop-3.2.1/share/h"]
[24.562213, "o", "adoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-text-1.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/dnsjava-2.1.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/error_prone_a"]
[24.562459, "o", "nnotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/"]
[24.562658, "o", "opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-servlet-9.3.24.v20180605.jar:/op"]
[24.56287, "o", "t/hadoop-3.2.1/share/\u001b[1;46r\u001b[H\u001b[45;74H\u001b[1;45r\u001b[H\u001b[45;74Hhadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.1/sh"]
[24.563044, "o", "are/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/re2j"]
[24.563202, "o", "-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-kms-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3."]
[24.563354, "o", "2.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-text-1.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2."]
[24.563491, "o", "1/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/op\u001b[1;46r\u001b[H\u001b[45;179H\u001b[1;45r\u001b[H\u001b[45;179Ht/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/h"]
[24.563642, "o", "dfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/s"]
[24.563663, "o", "hare/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb"]
[24.563676, "o", "-core-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/"]
[24.563687, "o", "lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/opt/"]
[24.563916, "o", "\u001b[1;46r\u001b[H\u001b[45;74H\u001b[1;45r\u001b[H\u001b[45;74Hhadoop-3.2.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-na"]
[24.56406, "o", "tive-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.1.jar:/opt/hadoop-3.2."]
[24.56408, "o", "1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-mod"]
[24.564091, "o", "ule-jaxb-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/o"]
[24.564103, "o", "pt/hadoop-3.2.1/share/hadoop/yarn/h"]
[24.565891, "o", "\u001b[1;46r\u001b[H\u001b[45;179H\u001b[1;45r\u001b[H\u001b[45;179Hadoop-yarn-registry-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar\r\n\u001b[32mdatanode3          |\u001b[39m ST"]
[24.566129, "o", "ARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z\r\n\u001b[32mdatanode3          |\u001b[39m STARTUP_MSG:   java = 1.8.0_232\r\n\u001b[32mdatanode3          |\u001b[39m ************************************************************/\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:13:59,573 INFO datanode.DataNode: STARTUP_MSG: \r\n\u001b[36mdatanode           |\u001b[39m /************************************************************\r\n\u001b[36mdatanode           |\u001b[39m STARTUP_MSG: Starting DataNode\r\n\u001b[36mdatanode           |\u001b[39m STARTUP_MSG:   host = cdbb15139e9e/172.30.0.8\r\n\u001b[36mdatanode           |\u001b[39m STARTUP_MSG:   args = []\r\n\u001b[36mdatanode           |\u001b[39m STARTUP_MSG:   version = 3.2.1\r\n\u001b[36mdatanode           |\u001b[39m STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop-3.2.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/l"]
[24.566156, "o", "ib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-text-1.4.jar:/opt/hadoop-3.2."]
[24.566169, "o", "1/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/dnsjava-2.1.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common"]
[24.56618, "o", "/lib/jackson-core-2.9.8.jar:/opt/hadoop-3.2.1/s"]
[24.568627, "o", "\u001b[1;46r\u001b[H\u001b[45;29H\u001b[1;45r\u001b[H\u001b[45;29Hhare/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoo"]
[24.568895, "o", "p-3.2.1/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-servlet-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2."]
[24.569064, "o", "1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/nett"]
[24.569208, "o", "y-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-kms-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs:/opt/hadoop-3.2.1/s"]
[24.569349, "o", "hare/hadoop/hdfs/lib/accessors-smar\u001b[1;46r\u001b[H\u001b[45;134H\u001b[1;45r\u001b[H\u001b[45;134Ht-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs"]
[24.569485, "o", "/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-text-1.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/h"]
[24.569625, "o", "dfs/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/shar"]
[24.569764, "o", "e/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop-3.2.1/share/hadoop"]
[24.569896, "o", "/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/k\u001b[1;46r\u001b[H\u001b[45;29H\u001b[1;45r\u001b[H\u001b[45;29Herb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-3.2.1/share/hadoop"]
[24.569915, "o", "/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdf"]
[24.569936, "o", "s/hadoop-hdfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/opt/hadoop"]
[24.569951, "o", "-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop-3.2.1/share/hado\u001b[1;46r\u001b[H\u001b[45;48H"]
[24.574649, "o", "\u001b[1;45r\u001b[H\u001b[45;48Hop/yarn/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/o"]
[24.574713, "o", "pt/hadoop-3.2.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-registry-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar:/opt/h"]
[24.574727, "o", "adoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar\r\n\u001b[36mdatanode           |\u001b[39m STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z\r\n\u001b[36mdatanode           |\u001b[39m STARTUP_MSG:   java = 1.8.0_232\r\n\u001b[36mdatanode           |\u001b[39m ************************************************************/\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:13:59,582 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\r\n\u001b[32mdatanod"]
[24.574741, "o", "e3          |\u001b[39m 2021-01-19 03:13:59,580 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[24.782656, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:13:59,793 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/hadoop/dfs/data\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[24.910663, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:13:59,923 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.01667, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,031 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,031 INFO impl.MetricsSystemImpl: DataNode metrics system started\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.194605, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,211 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/hadoop/dfs/data\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.287303, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,304 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/hadoop/dfs/data\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.319266, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,336 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.35007, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,362 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.355035, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,365 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.356918, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,369 INFO datanode.DataNode: Configured hostname is deb6331226b2\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,370 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.357759, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,373 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.400024, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,413 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,413 INFO impl.MetricsSystemImpl: DataNode metrics system started\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.401627, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,413 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.422072, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,434 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,436 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,436 INFO datanode.DataNode: Number threads for balancing is 50\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.476203, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,491 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,491 INFO impl.MetricsSystemImpl: DataNode metrics system started\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.477872, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,492 INFO util.log: Logging initialized @1798ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.559805, "o", "\u001b[?25l\r\n\u001b[30m\u001b[42m[1] 0:root@k8snode1:~/bigdata/docker-hadoop* 1:root@k8snode1:~/bigdata/docker-hadoop-                                                                                                   \"k8snode1\" 11:14 19-Jan-21\u001b(B\u001b[m\u001b[45;1H\u001b[?12l\u001b[?25h"]
[25.598697, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,615 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.603318, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,618 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.607221, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,624 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.610691, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,625 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,626 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.611421, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,626 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.623276, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,637 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.623928, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,640 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.631118, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,645 INFO datanode.DataNode: Configured hostname is 9e22ace955e1\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,645 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.631422, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,647 INFO http.HttpServer2: Jetty bound to port 33246\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.633079, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,649 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.635972, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,648 INFO server.Server: jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.675412, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,692 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@32115b28{/logs,file:///opt/hadoop-3.2.1/logs/,AVAILABLE}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.678509, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,695 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6bb4dd34{/static,file:///opt/hadoop-3.2.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.695059, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,708 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.703134, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,710 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,710 INFO datanode.DataNode: Number threads for balancing is 50\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,718 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.708129, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,724 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.720772, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,735 INFO datanode.DataNode: Configured hostname is cdbb15139e9e\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,736 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.721682, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,739 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.742108, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,755 INFO util.log: Logging initialized @1876ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.751156, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,765 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4f74980d{/,file:///opt/hadoop-3.2.1/share/hadoop/hdfs/webapps/datanode/,AVAILABLE}{/datanode}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.753815, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,771 INFO server.AbstractConnector: Started ServerConnector@1b8a29df{HTTP/1.1,[http/1.1]}{localhost:33246}\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,771 INFO server.Server: Started @2077ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.760115, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,777 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.763977, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,780 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,780 INFO datanode.DataNode: Number threads for balancing is 50\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.802026, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,818 INFO util.log: Logging initialized @1921ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.85674, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,873 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.862937, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,876 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.869256, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,882 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.873118, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,887 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,887 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,887 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.878382, "o", "\u001b[?25l\u001b[H\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,413 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,434 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,436 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,436 INFO datanode.DataNode: Number threads for balancing is 50\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,491 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,491 INFO impl.MetricsSystemImpl: DataNode metrics system started\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,492 INFO util.log: Logging initialized @1798ms\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,615 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use "]
[25.878923, "o", "random secrets.\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,618 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,624 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,625 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,626 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,626 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,637 INFO common.Util: df"]
[25.879178, "o", "s.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,640 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,645 INFO datanode.DataNode: Configured hostname is 9e22ace955e1\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,645 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,647 INFO http.HttpServer2: Jetty bound to port 33246\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,649 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,648 INFO server.Server: jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,692 INFO handler.ContextHand"]
[25.879361, "o", "ler: Started o.e.j.s.ServletContextHandler@32115b28{/logs,file:///opt/hadoop-3.2.1/logs/,AVAILABLE}\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,695 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6bb4dd34{/static,file:///opt/hadoop-3.2.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,708 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:9866\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,710 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,710 INFO datanode.DataNode: Number threads for balancing is 50\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,718 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,724 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m 20"]
[25.879385, "o", "21-01-19 03:14:00,735 INFO datanode.DataNode: Configured hostname is cdbb15139e9e\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,736 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,739 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,755 INFO util.log: Logging initialized @1876ms\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,765 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4f74980d{/,file:///opt/hadoop-3.2.1/share/hadoop/hdfs/webapps/datanode/,AVAILABLE}{/datanode}\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,771 INFO server.AbstractConnector: Started ServerConnector@1b8a29df{HTTP/1.1,[http/1.1]}{localhost:33246}\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:00,771 INFO server.Server: Started @2077ms\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,777 INFO datanode.DataNode"]
[25.879397, "o", ": Opened streaming server at /0.0.0.0:9866\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,780 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,780 INFO datanode.DataNode: Number threads for balancing is 50\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,818 INFO util.log: Logging initialized @1921ms\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,873 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,876 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,882 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,887 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.l"]
[25.879409, "o", "ib.StaticUserWebFilter$StaticUserFilter) to context datanode\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,887 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,887 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[25.90908, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,921 INFO http.HttpServer2: Jetty bound to port 35475\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,923 INFO server.Server: jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.93537, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,952 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.939604, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,957 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.946938, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,962 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,964 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,964 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,964 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.950865, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,963 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1a5b6f42{/logs,file:///opt/hadoop-3.2.1/logs/,AVAILABLE}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.952458, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:00,968 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@32115b28{/static,file:///opt/hadoop-3.2.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.967308, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,984 INFO http.HttpServer2: Jetty bound to port 39837\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[25.972229, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:00,987 INFO server.Server: jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.007125, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:01,024 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1a5b6f42{/logs,file:///opt/hadoop-3.2.1/logs/,AVAILABLE}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.009543, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:01,027 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@32115b28{/static,file:///opt/hadoop-3.2.1/share/hadoop/hdfs/webapps/static/,AVAILABLE}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.01625, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,033 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@518caac3{/,file:///opt/hadoop-3.2.1/share/hadoop/hdfs/webapps/datanode/,AVAILABLE}{/datanode}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.025436, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,040 INFO server.AbstractConnector: Started ServerConnector@7cb502c{HTTP/1.1,[http/1.1]}{localhost:35475}\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,040 INFO server.Server: Started @2161ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.030566, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,046 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.034437, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,051 INFO util.JvmPauseMonitor: Starting JVM pause monitor\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.037973, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,054 INFO datanode.DataNode: dnUserName = root\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,054 INFO datanode.DataNode: supergroup = supergroup\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.087909, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,104 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.09698, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:01,114 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@518caac3{/,file:///opt/hadoop-3.2.1/share/hadoop/hdfs/webapps/datanode/,AVAILABLE}{/datanode}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.099261, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,116 INFO ipc.Server: Starting Socket Reader #1 for port 9867\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.106488, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:01,121 INFO server.AbstractConnector: Started ServerConnector@7cb502c{HTTP/1.1,[http/1.1]}{localhost:39837}\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:01,121 INFO server.Server: Started @2223ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.264658, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,280 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.267774, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,285 INFO util.JvmPauseMonitor: Starting JVM pause monitor\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.27125, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,288 INFO datanode.DataNode: dnUserName = root\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,288 INFO datanode.DataNode: supergroup = supergroup\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.317075, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,332 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.33072, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,348 INFO datanode.DataNode: Refresh request received for nameservices: null\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.346271, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,357 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,356 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.356576, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,373 INFO ipc.Server: Starting Socket Reader #1 for port 9867\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.363045, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,379 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to namenode/172.30.0.2:9000 starting to offer service\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.382298, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:01,398 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:9864\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.389357, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:01,406 INFO datanode.DataNode: dnUserName = root\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:01,406 INFO datanode.DataNode: supergroup = supergroup\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.395279, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:01,409 INFO util.JvmPauseMonitor: Starting JVM pause monitor\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.40511, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,422 INFO ipc.Server: IPC Server listener on 9867: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.414343, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,431 INFO ipc.Server: IPC Server Responder: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.541337, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,559 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.563091, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,575 INFO datanode.DataNode: Refresh request received for nameservices: null\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.571947, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,584 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.57884, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,595 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to namenode/172.30.0.2:9000 starting to offer service\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.599751, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,615 INFO ipc.Server: IPC Server Responder: starting\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,616 INFO ipc.Server: IPC Server listener on 9867: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.704387, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,721 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to namenode/172.30.0.2:9000\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.705986, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,723 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.713715, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,731 INFO common.Storage: Lock on /hadoop/dfs/data/in_use.lock acquired by nodename 361@deb6331226b2\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.714758, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,732 INFO common.Storage: Storage directory with location [DISK]file:/hadoop/dfs/data is not formatted for namespace 375909412. Formatting...\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.718339, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,733 INFO common.Storage: Generated new storageID DS-f7fb1656-607a-4c63-ae35-3a9c5f502187 for directory /hadoop/dfs/data \r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.743549, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,757 INFO common.Storage: Analyzing storage directories for bpid BP-899898175-172.19.0.5-1610935156129\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,757 INFO common.Storage: Locking is disabled for /hadoop/dfs/data/current/BP-899898175-172.19.0.5-1610935156129\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,757 INFO common.Storage: Block pool storage directory for location [DISK]file:/hadoop/dfs/data and block pool id BP-899898175-172.19.0.5-1610935156129 is not formatted. Formatting ...\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,757 INFO common.Storage: Formatting block pool BP-899898175-172.19.0.5-1610935156129 directory /hadoop/dfs/data/current/BP-899898175-172.19.0.5-1610935156129/current\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.749835, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,759 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to namenode/172.30.0.2:9000\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,761 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.754148, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,771 INFO datanode.DataNode: Setting up storage: nsid=375909412;bpid=BP-899898175-172.19.0.5-1610935156129;lv=-57;nsInfo=lv=-65;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129;bpid=BP-899898175-172.19.0.5-1610935156129;dnuuid=null\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.755115, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,772 INFO datanode.DataNode: Generated and persisted new Datanode UUID 760eb334-9533-4c3a-b5b5-46456cd6f837\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.755876, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,770 INFO common.Storage: Lock on /hadoop/dfs/data/in_use.lock acquired by nodename 361@9e22ace955e1\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,771 INFO common.Storage: Storage directory with location [DISK]file:/hadoop/dfs/data is not formatted for namespace 375909412. Formatting...\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,772 INFO common.Storage: Generated new storageID DS-a26933eb-0d67-43f9-9158-b18002aa48f0 for directory /hadoop/dfs/data \r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.776901, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,790 INFO common.Storage: Analyzing storage directories for bpid BP-899898175-172.19.0.5-1610935156129\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,790 INFO common.Storage: Locking is disabled for /hadoop/dfs/data/current/BP-899898175-172.19.0.5-1610935156129\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,790 INFO common.Storage: Block pool storage directory for location [DISK]file:/hadoop/dfs/data and block pool id BP-899898175-172.19.0.5-1610935156129 is not formatted. Formatting ...\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,791 INFO common.Storage: Formatting block pool BP-899898175-172.19.0.5-1610935156129 directory /hadoop/dfs/data/current/BP-899898175-172.19.0.5-1610935156129/current\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,792 INFO datanode.DataNode: Setting up storage: nsid=375909412;bpid=BP-899898175-172.19.0.5-1610935156129;lv=-57;nsInfo=lv=-65;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412"]
[26.776968, "o", ";c=1610935156129;bpid=BP-899898175-172.19.0.5-1610935156129;dnuuid=null\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.78104, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,793 INFO datanode.DataNode: Generated and persisted new Datanode UUID acad48b8-4144-4718-a0d4-4e0e32ff0a1a\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.840624, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,858 INFO impl.FsDatasetImpl: Added new volume: DS-f7fb1656-607a-4c63-ae35-3a9c5f502187\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,858 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/hadoop/dfs/data, StorageType: DISK\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.844037, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,861 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.849884, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,867 INFO checker.ThrottledAsyncChecker: Scheduling a check for /hadoop/dfs/data\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.856786, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,874 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /hadoop/dfs/data\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.860337, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,875 INFO impl.FsDatasetImpl: Adding block pool BP-899898175-172.19.0.5-1610935156129\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.864152, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,880 INFO impl.FsDatasetImpl: Scanning block pool BP-899898175-172.19.0.5-1610935156129 on volume /hadoop/dfs/data...\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.868408, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,881 INFO impl.FsDatasetImpl: Added new volume: DS-a26933eb-0d67-43f9-9158-b18002aa48f0\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,881 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/hadoop/dfs/data, StorageType: DISK\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.869277, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,885 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.876261, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,891 INFO checker.ThrottledAsyncChecker: Scheduling a check for /hadoop/dfs/data\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.883429, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,899 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /hadoop/dfs/data\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.884831, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,901 INFO impl.FsDatasetImpl: Adding block pool BP-899898175-172.19.0.5-1610935156129\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,901 INFO impl.FsDatasetImpl: Scanning block pool BP-899898175-172.19.0.5-1610935156129 on volume /hadoop/dfs/data...\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.901263, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,917 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-899898175-172.19.0.5-1610935156129 on /hadoop/dfs/data: 37ms\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,918 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-899898175-172.19.0.5-1610935156129: 43ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.9076, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,924 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-899898175-172.19.0.5-1610935156129 on volume /hadoop/dfs/data...\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,924 INFO impl.BlockPoolSlice: Replica Cache file: /hadoop/dfs/data/current/BP-899898175-172.19.0.5-1610935156129/current/replicas doesn't exist \r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.918955, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,935 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-899898175-172.19.0.5-1610935156129 on /hadoop/dfs/data: 34ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.923037, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,936 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-899898175-172.19.0.5-1610935156129: 34ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.926876, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,937 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-899898175-172.19.0.5-1610935156129 on volume /hadoop/dfs/data: 13ms\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,938 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-899898175-172.19.0.5-1610935156129: 19ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.933017, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,940 INFO datanode.VolumeScanner: Now scanning bpid BP-899898175-172.19.0.5-1610935156129 on volume /hadoop/dfs/data\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,943 INFO datanode.VolumeScanner: VolumeScanner(/hadoop/dfs/data, DS-f7fb1656-607a-4c63-ae35-3a9c5f502187): finished scanning block pool BP-899898175-172.19.0.5-1610935156129\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,940 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-899898175-172.19.0.5-1610935156129 on volume /hadoop/dfs/data...\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,940 INFO impl.BlockPoolSlice: Replica Cache file: /hadoop/dfs/data/current/BP-899898175-172.19.0.5-1610935156129/current/replicas doesn't exist \r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.94081, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,956 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/19/21 4:52 AM with interval of 21600000ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.943579, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,961 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-899898175-172.19.0.5-1610935156129 on volume /hadoop/dfs/data: 20ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.949139, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,965 INFO datanode.VolumeScanner: VolumeScanner(/hadoop/dfs/data, DS-f7fb1656-607a-4c63-ae35-3a9c5f502187): no suitable block pools found to scan.  Waiting 1814399974 ms.\u001b[45;1H\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,966 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-899898175-172.19.0.5-1610935156129: 30ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.950952, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,968 INFO datanode.VolumeScanner: Now scanning bpid BP-899898175-172.19.0.5-1610935156129 on volume /hadoop/dfs/data\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.955307, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,972 INFO datanode.DataNode: Block pool BP-899898175-172.19.0.5-1610935156129 (Datanode Uuid 760eb334-9533-4c3a-b5b5-46456cd6f837) service to namenode/172.30.0.2:9000 beginning handshake with NN\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.959363, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,975 INFO datanode.VolumeScanner: VolumeScanner(/hadoop/dfs/data, DS-a26933eb-0d67-43f9-9158-b18002aa48f0): finished scanning block pool BP-899898175-172.19.0.5-1610935156129\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.966259, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,983 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/19/21 8:28 AM with interval of 21600000ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.974053, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,990 INFO datanode.DataNode: Block pool BP-899898175-172.19.0.5-1610935156129 (Datanode Uuid acad48b8-4144-4718-a0d4-4e0e32ff0a1a) service to namenode/172.30.0.2:9000 beginning handshake with NN\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[26.994566, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:02,004 INFO datanode.VolumeScanner: VolumeScanner(/hadoop/dfs/data, DS-a26933eb-0d67-43f9-9158-b18002aa48f0): no suitable block pools found to scan.  Waiting 1814399963 ms.\u001b[45;1H\n\u001b[1;46r\u001b[H\u001b[45d"]
[27.014609, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,024 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.30.0.3:9866, datanodeUuid=760eb334-9533-4c3a-b5b5-46456cd6f837, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129) storage 760eb334-9533-4c3a-b5b5-46456cd6f837\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,025 INFO net.NetworkTopology: Adding a new node: /default-rack/172.30.0.3:9866\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,025 INFO blockmanagement.BlockReportLeaseManager: Registered DN 760eb334-9533-4c3a-b5b5-46456cd6f837 (172.30.0.3:9866).\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[27.016412, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,030 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.30.0.7:9866, datanodeUuid=acad48b8-4144-4718-a0d4-4e0e32ff0a1a, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129) storage acad48b8-4144-4718-a0d4-4e0e32ff0a1a\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,030 INFO net.NetworkTopology: Adding a new node: /default-rack/172.30.0.7:9866\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,030 INFO blockmanagement.BlockReportLeaseManager: Registered DN acad48b8-4144-4718-a0d4-4e0e32ff0a1a (172.30.0.7:9866).\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[27.023288, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:02,038 INFO datanode.DataNode: Block pool Block pool BP-899898175-172.19.0.5-1610935156129 (Datanode Uuid 760eb334-9533-4c3a-b5b5-46456cd6f837) service to namenode/172.30.0.2:9000 successfully registered with NN\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:02,038 INFO datanode.DataNode: For namenode namenode/172.30.0.2:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:02,038 INFO datanode.DataNode: Block pool Block pool BP-899898175-172.19.0.5-1610935156129 (Datanode Uuid acad48b8-4144-4718-a0d4-4e0e32ff0a1a) service to namenode/172.30.0.2:9000 successfully registered with NN\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:02,038 INFO datanode.DataNode: For namenode namenode/172.30.0.2:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\r\n\u001b[1;46r"]
[27.023452, "o", "\u001b[H\u001b[45d"]
[27.087446, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,102 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-a26933eb-0d67-43f9-9158-b18002aa48f0 for DN 172.30.0.7:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[27.092231, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,105 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-f7fb1656-607a-4c63-ae35-3a9c5f502187 for DN 172.30.0.3:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[27.137605, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,153 INFO BlockStateChange: BLOCK* processReport 0x1d9ac8b79b6509ef: Processing first storage report for DS-f7fb1656-607a-4c63-ae35-3a9c5f502187 from datanode 760eb334-9533-4c3a-b5b5-46456cd6f837\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,154 INFO BlockStateChange: BLOCK* processReport 0x1d9ac8b79b6509ef: from storage DS-f7fb1656-607a-4c63-ae35-3a9c5f502187 node DatanodeRegistration(172.30.0.3:9866, datanodeUuid=760eb334-9533-4c3a-b5b5-46456cd6f837, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[27.149909, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,167 INFO BlockStateChange: BLOCK* processReport 0xfef54992268ea0f: Processing first storage report for DS-a26933eb-0d67-43f9-9158-b18002aa48f0 from datanode acad48b8-4144-4718-a0d4-4e0e32ff0a1a\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,167 INFO BlockStateChange: BLOCK* processReport 0xfef54992268ea0f: from storage DS-a26933eb-0d67-43f9-9158-b18002aa48f0 node DatanodeRegistration(172.30.0.7:9866, datanodeUuid=acad48b8-4144-4718-a0d4-4e0e32ff0a1a, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[27.174857, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:02,190 INFO datanode.DataNode: Successfully sent block report 0x1d9ac8b79b6509ef,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 59 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:02,190 INFO datanode.DataNode: Got finalize command for block pool BP-899898175-172.19.0.5-1610935156129\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:02,188 INFO datanode.DataNode: Successfully sent block report 0xfef54992268ea0f,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 57 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[27.177494, "o", "\u001b[?25l\u001b[H\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,966 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-899898175-172.19.0.5-1610935156129: 30ms\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,968 INFO datanode.VolumeScanner: Now scanning bpid BP-899898175-172.19.0.5-1610935156129 on volume /hadoop/dfs/data\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:01,972 INFO datanode.DataNode: Block pool BP-899898175-172.19.0.5-1610935156129 (Datanode Uuid 760eb334-9533-4c3a-b5b5-46456cd6f837) service to namenode/172.30.0.2:9000 beginning handshake with NN\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,975 INFO datanode.VolumeScanner: VolumeScanner(/hadoop/dfs/data, DS-a26933eb-0d67-43f9-9158-b18002aa48f0): finished scanning block pool BP-899898175-172.19.0.5-1610935156129\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,983 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/19/21 8:28 AM with interval of 21600000ms\u001b"]
[27.177864, "o", "[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:01,990 INFO datanode.DataNode: Block pool BP-899898175-172.19.0.5-1610935156129 (Datanode Uuid acad48b8-4144-4718-a0d4-4e0e32ff0a1a) service to namenode/172.30.0.2:9000 beginning handshake with NN\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:02,004 INFO datanode.VolumeScanner: VolumeScanner(/hadoop/dfs/data, DS-a26933eb-0d67-43f9-9158-b18002aa48f0): no suitable block pools found to scan.  Waiting 1814399963 ms.\u001b[11;1H\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,024 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.30.0.3:9866, datanodeUuid=760eb334-9533-4c3a-b5b5-46456cd6f837, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129) storage 760eb334-9533-4c3a-b5b5-46456cd6f837\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,025 INFO net.NetworkTopology: Adding a new node: /default-rack/172.30.0.3:9866\u001b[K\r\n\u001b[34mnamenode        "]
[27.178074, "o", "   |\u001b[39m 2021-01-19 03:14:02,025 INFO blockmanagement.BlockReportLeaseManager: Registered DN 760eb334-9533-4c3a-b5b5-46456cd6f837 (172.30.0.3:9866).\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,030 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.30.0.7:9866, datanodeUuid=acad48b8-4144-4718-a0d4-4e0e32ff0a1a, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129) storage acad48b8-4144-4718-a0d4-4e0e32ff0a1a\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,030 INFO net.NetworkTopology: Adding a new node: /default-rack/172.30.0.7:9866\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,030 INFO blockmanagement.BlockReportLeaseManager: Registered DN acad48b8-4144-4718-a0d4-4e0e32ff0a1a (172.30.0.7:9866).\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:02,038 INFO datanode.DataNode: Block pool Block pool BP-899898175-172.19.0.5-1610935156129 (Datanode Uuid 760eb334-9533-4c3a"]
[27.17824, "o", "-b5b5-46456cd6f837) service to namenode/172.30.0.2:9000 successfully registered with NN\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:02,038 INFO datanode.DataNode: For namenode namenode/172.30.0.2:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:02,038 INFO datanode.DataNode: Block pool Block pool BP-899898175-172.19.0.5-1610935156129 (Datanode Uuid acad48b8-4144-4718-a0d4-4e0e32ff0a1a) service to namenode/172.30.0.2:9000 successfully registered with NN\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:02,038 INFO datanode.DataNode: For namenode namenode/172.30.0.2:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,102 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-a26933eb-0d67-43f9-9158-b18002aa48f0 for DN 172.30.0.7:9866\u001b["]
[27.178262, "o", "K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,105 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-f7fb1656-607a-4c63-ae35-3a9c5f502187 for DN 172.30.0.3:9866\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,153 INFO BlockStateChange: BLOCK* processReport 0x1d9ac8b79b6509ef: Processing first storage report for DS-f7fb1656-607a-4c63-ae35-3a9c5f502187 from datanode 760eb334-9533-4c3a-b5b5-46456cd6f837\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,154 INFO BlockStateChange: BLOCK* processReport 0x1d9ac8b79b6509ef: from storage DS-f7fb1656-607a-4c63-ae35-3a9c5f502187 node DatanodeRegistration(172.30.0.3:9866, datanodeUuid=760eb334-9533-4c3a-b5b5-46456cd6f837, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,167 INFO BlockStateChange: BLOCK* proce"]
[27.178277, "o", "ssReport 0xfef54992268ea0f: Processing first storage report for DS-a26933eb-0d67-43f9-9158-b18002aa48f0 from datanode acad48b8-4144-4718-a0d4-4e0e32ff0a1a\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:02,167 INFO BlockStateChange: BLOCK* processReport 0xfef54992268ea0f: from storage DS-a26933eb-0d67-43f9-9158-b18002aa48f0 node DatanodeRegistration(172.30.0.7:9866, datanodeUuid=acad48b8-4144-4718-a0d4-4e0e32ff0a1a, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:02,190 INFO datanode.DataNode: Successfully sent block report 0x1d9ac8b79b6509ef,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 59 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\u001b[K\r\n\u001b[33mdatanode2         "]
[27.178295, "o", " |\u001b[39m 2021-01-19 03:14:02,190 INFO datanode.DataNode: Got finalize command for block pool BP-899898175-172.19.0.5-1610935156129\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:02,188 INFO datanode.DataNode: Successfully sent block report 0xfef54992268ea0f,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 57 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:02,189 INFO datanode.DataNode: Got finalize command for block pool BP-899898175-172.19.0.5-1610935156129\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[28.630867, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m [2/100] datanode:9864 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[28.634313, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] datanode2:9864 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[28.639766, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] datanode3:9864 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[28.72718, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m [2/100] datanode:9864 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[28.72909, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m [1/100] datanode2:9864 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[28.731359, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m [1/100] datanode3:9864 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[28.733335, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m [1/100] check for resourcemanager:8088...\r\n\u001b[35mhistoryserver      |\u001b[39m [1/100] resourcemanager:8088 is not available yet\r\n\u001b[35mhistoryserver      |\u001b[39m [1/100] try in 5s once again ...\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[28.768583, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m [2/100] datanode:9864 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[28.771424, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m [1/100] datanode2:9864 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[28.773198, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m [1/100] datanode3:9864 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[28.776072, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m [1/100] check for resourcemanager:8088...\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m [1/100] resourcemanager:8088 is not available yet\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m [1/100] try in 5s once again ...\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[31.449031, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:06,466 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[31.462158, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:06,479 INFO ipc.Server: Starting Socket Reader #1 for port 9867\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[31.647439, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:06,660 INFO resourcemanager.ResourceManager: STARTUP_MSG: \r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m /************************************************************\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG: Starting ResourceManager\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   host = a07c851f4b7d/172.30.0.4\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   args = []\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   version = 3.2.1\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop-3.2.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/l"]
[31.64985, "o", "ib/commons-beanutils-1.9.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-text-1.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/dnsjava-2.1.7.jar"]
[31.651831, "o", ":/opt/hadoop-3.2.1/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1"]
[31.65215, "o", "/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/"]
[31.65237, "o", "hadoop/common/lib/jetty-servlet-9.3.24.v20\u001b[1;46r\u001b[H\u001b[45;39H\u001b[1;45r\u001b[H\u001b[45;39H180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop"]
[31.652569, "o", "/common/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/protobuf-java-"]
[31.652719, "o", "2.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-kms-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoo"]
[31.652872, "o", "p/hdfs/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-text-1.4.jar:/opt/hadoop-3.2.1/share/ha"]
[31.652893, "o", "doop/hdfs/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/l\u001b[1;46r\u001b[H\u001b[45;144H\u001b[1;45r\u001b[H\u001b[45;144Hib/curator-framework-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib"]
[31.652905, "o", "/jackson-core-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hd"]
[31.652919, "o", "fs/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/l"]
[31.653289, "o", "\u001b[1;46r\u001b[H\u001b[45;150H\u001b[1;45r\u001b[H\u001b[45;150Hib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-"]
[31.653465, "o", "pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/token-provider-1.0."]
[31.653623, "o", "1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/opt/hadoop-3.2.1/shar"]
[31.653772, "o", "e/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/bcpkix-jdk15"]
[31.653907, "o", "on-1.60.jar:/opt/hadoop-3.2.1/share/h\u001b[1;46r\u001b[H\u001b[45;45H\u001b[1;45r\u001b[H\u001b[45;45Hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/hadoo"]
[31.654064, "o", "p-3.2.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-registry-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/opt/hadoop-3.2.1/sh"]
[31.654087, "o", "are/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1.jar:/opt/hadoop"]
[31.6541, "o", "-3.2.1/share/hadoop/yarn/timelineservice/lib/commons-csv-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/commons-lang-2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-annotations-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-client-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-common-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-protocol-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/jcodings-1.0.13.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/joni-2.1.2.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/metrics-core-2.2.0.jar\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m START"]
[31.654111, "o", "UP_MSG:   java = 1.8.0_232\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m ********************\u001b[1;46r\u001b[H\u001b[45;42H\u001b[1;45r\u001b[H\u001b[45;42H****************************************/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[31.659209, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:06,674 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[31.673026, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:06,689 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:9867\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[31.691879, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:06,707 INFO datanode.DataNode: Refresh request received for nameservices: null\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[31.699334, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:06,716 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[31.714844, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:06,727 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to namenode/172.30.0.2:9000 starting to offer service\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[31.722167, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:06,739 INFO ipc.Server: IPC Server Responder: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[31.723119, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:06,740 INFO ipc.Server: IPC Server listener on 9867: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[31.886096, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:06,901 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to namenode/172.30.0.2:9000\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[31.887056, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:06,904 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[31.892349, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:06,909 INFO common.Storage: Lock on /hadoop/dfs/data/in_use.lock acquired by nodename 361@cdbb15139e9e\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[31.914453, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:06,930 INFO common.Storage: Analyzing storage directories for bpid BP-899898175-172.19.0.5-1610935156129\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:06,931 INFO common.Storage: Locking is disabled for /hadoop/dfs/data/current/BP-899898175-172.19.0.5-1610935156129\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[31.918464, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:06,935 INFO datanode.DataNode: Setting up storage: nsid=375909412;bpid=BP-899898175-172.19.0.5-1610935156129;lv=-57;nsInfo=lv=-65;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129;bpid=BP-899898175-172.19.0.5-1610935156129;dnuuid=1ce6f158-0b39-4227-9455-ea5d127087b3\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.02448, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:07,039 INFO impl.FsDatasetImpl: Added new volume: DS-85730882-6735-47a4-b9ce-d49179db9e93\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:07,039 INFO impl.FsDatasetImpl: Added volume - [DISK]file:/hadoop/dfs/data, StorageType: DISK\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.024937, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,036 INFO conf.Configuration: found resource core-site.xml at file:/opt/hadoop-3.2.1/etc/hadoop/core-site.xml\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.033951, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:07,047 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.039148, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:07,055 INFO checker.ThrottledAsyncChecker: Scheduling a check for /hadoop/dfs/data\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.054658, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:07,071 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /hadoop/dfs/data\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.05602, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:07,073 INFO impl.FsDatasetImpl: Adding block pool BP-899898175-172.19.0.5-1610935156129\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.058215, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:07,075 INFO impl.FsDatasetImpl: Scanning block pool BP-899898175-172.19.0.5-1610935156129 on volume /hadoop/dfs/data...\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.069464, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,081 INFO conf.Configuration: resource-types.xml not found\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,081 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.105417, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:07,120 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-899898175-172.19.0.5-1610935156129 on /hadoop/dfs/data: 45ms\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:07,121 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-899898175-172.19.0.5-1610935156129: 47ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.118342, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,135 INFO conf.Configuration: found resource yarn-site.xml at file:/opt/hadoop-3.2.1/etc/hadoop/yarn-site.xml\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.123361, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:07,137 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-899898175-172.19.0.5-1610935156129 on volume /hadoop/dfs/data...\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:07,137 INFO impl.BlockPoolSlice: Replica Cache file: /hadoop/dfs/data/current/BP-899898175-172.19.0.5-1610935156129/current/replicas doesn't exist \r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.133048, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,149 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.146846, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:07,163 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-899898175-172.19.0.5-1610935156129 on volume /hadoop/dfs/data: 26ms\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:07,164 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-899898175-172.19.0.5-1610935156129: 42ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.179131, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:07,195 INFO datanode.VolumeScanner: VolumeScanner(/hadoop/dfs/data, DS-85730882-6735-47a4-b9ce-d49179db9e93): no suitable block pools found to scan.  Waiting 1723515426 ms.\u001b[45;1H\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,193 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.18152, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,199 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.18638, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,204 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.192307, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:07,209 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/19/21 4:40 AM with interval of 21600000ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.201512, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:07,217 INFO datanode.DataNode: Block pool BP-899898175-172.19.0.5-1610935156129 (Datanode Uuid 1ce6f158-0b39-4227-9455-ea5d127087b3) service to namenode/172.30.0.2:9000 beginning handshake with NN\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.224198, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:07,240 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.30.0.8:9866, datanodeUuid=1ce6f158-0b39-4227-9455-ea5d127087b3, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129) storage 1ce6f158-0b39-4227-9455-ea5d127087b3\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:07,241 INFO net.NetworkTopology: Adding a new node: /default-rack/172.30.0.8:9866\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:07,241 INFO blockmanagement.BlockReportLeaseManager: Registered DN 1ce6f158-0b39-4227-9455-ea5d127087b3 (172.30.0.8:9866).\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.229513, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:07,246 INFO datanode.DataNode: Block pool Block pool BP-899898175-172.19.0.5-1610935156129 (Datanode Uuid 1ce6f158-0b39-4227-9455-ea5d127087b3) service to namenode/172.30.0.2:9000 successfully registered with NN\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:07,246 INFO datanode.DataNode: For namenode namenode/172.30.0.2:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.239326, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,256 INFO recovery.RMStateStoreFactory: Using RMStateStore implementation - class org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.240571, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,258 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.245415, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,262 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,263 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.268373, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:07,282 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-85730882-6735-47a4-b9ce-d49179db9e93 for DN 172.30.0.8:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.291536, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:07,305 INFO BlockStateChange: BLOCK* processReport 0x8a5c24e6bccd861d: Processing first storage report for DS-85730882-6735-47a4-b9ce-d49179db9e93 from datanode 1ce6f158-0b39-4227-9455-ea5d127087b3\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.292641, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:07,310 INFO blockmanagement.BlockManager: initializing replication queues\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.293625, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:07,311 INFO hdfs.StateChange: STATE* Safe mode extension entered. \r\n\u001b[34mnamenode           |\u001b[39m The reported blocks 54 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.294772, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,311 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.300065, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,313 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.30151, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:07,311 INFO BlockStateChange: BLOCK* processReport 0x8a5c24e6bccd861d: from storage DS-85730882-6735-47a4-b9ce-d49179db9e93 node DatanodeRegistration(172.30.0.8:9866, datanodeUuid=1ce6f158-0b39-4227-9455-ea5d127087b3, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129), blocks: 55, hasStaleStorage: false, processing time: 6 msecs, invalidatedBlocks: 0\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.306146, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,314 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,315 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.307403, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:07,323 INFO blockmanagement.BlockManager: Total number of blocks            = 55\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:07,323 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:07,323 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 54\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:07,323 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:07,323 INFO blockmanagement.BlockManager: Number of blocks being written    = 0\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:07,323 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 11 msec\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.319442, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:07,336 INFO datanode.DataNode: Successfully sent block report 0x8a5c24e6bccd861d,  containing 1 storage report(s), of which we sent 1. The reports had 55 total blocks and used 1 RPC(s). This took 5 msec to generate and 39 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:07,336 INFO datanode.DataNode: Got finalize command for block pool BP-899898175-172.19.0.5-1610935156129\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.365859, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,383 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.425069, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,442 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,442 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.439316, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,456 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.442753, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,460 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.44936, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,466 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.451548, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,468 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,469 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.456412, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,473 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.457916, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,475 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[32.464363, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:07,481 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/opt/hadoop-3.2.1/etc/hadoop/capacity-scheduler.xml\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[33.508399, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,525 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1024, vCores:1>\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,526 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:8192, vCores:4>\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[33.544326, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,561 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,561 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[33.552689, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,570 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m , reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,570 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[33.57125, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,587 INFO capacity.LeafQueue: Initializing default\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[33.572501, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m capacity = 1.0 [= (float) configuredCapacity / 100 ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxCapacity = 1.0 [= configuredMaxCapacity ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m effectiveMinResource=<memory:0, vCores:0>\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  , effectiveMaxResource=<memory:0, vCores:0>\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimit = 100 [= configuredUserLimit ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimitFactor = 1.0 [= configuredUserLimitFactor ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplicationsPerUser = 10000 [= (int"]
[33.572555, "o", ")(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m maximumAllocation = <memory:8192, vCores:4> [= configuredMaxAllocation ]\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[33.574767, "o", "\u001b[?25l\u001b[H\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,561 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,561 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,570 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m , reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,570 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,587 INFO capacity.LeafQueue: Initializing default\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m capacity = 1.0 [= (float) configuredCapacity / 100 ]\u001b[K\r\n\u001b"]
[33.57513, "o", "[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxCapacity = 1.0 [= configuredMaxCapacity ]\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m effectiveMinResource=<memory:0, vCores:0>\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  , effectiveMaxResource=<memory:0, vCores:0>\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimit = 100 [= configuredUserLimit ]\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimitFactor = 1.0 [= configuredUserLimitFactor ]\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\u001b[K\r\n\u001b[33m\u001b[1mresourc"]
[33.575368, "o", "emanager    |\u001b(B\u001b[m usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m maximumAllocation = <memory:8192, vCores:4> [= configuredMaxAllocation ]\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m numContainers = 0 [= currentNumContainers ]\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m state = RUNNING [= configuredState ]\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m nodeLocalityDelay = 40\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m rackLocalityAdditionalDelay = -1\u001b[K\r\n\u001b[33m\u001b[1mres"]
[33.575394, "o", "ourcemanager    |\u001b(B\u001b[m labels=*,\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m reservationsContinueLooking = true\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m preemptionDisabled = true\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m defaultAppPriorityPerQueue = 0\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m priority = 0\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxLifetime = -1 seconds\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m defaultLifetime = -1 seconds\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,587 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,588 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>us"]
[33.575407, "o", "edCapacity=0.0, numApps=0, numContainers=0\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,589 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,590 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,591 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are \u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,591 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAllocation=<<memory:8192, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false"]
[33.575418, "o", "\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[33.576569, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,594 INFO conf.Configuration: dynamic-resources.xml not found\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[33.57813, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,596 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[33.578467, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,596 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[33.579519, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,597 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. \r\n\u001b[1;46r\u001b[H\u001b[45d"]
[33.729018, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,746 INFO impl.TimelineClientImpl: Timeline service address: historyserver:8188\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[33.737945, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m [2/100] check for resourcemanager:8088...\r\n\u001b[35mhistoryserver      |\u001b[39m [2/100] resourcemanager:8088 is not available yet\r\n\u001b[35mhistoryserver      |\u001b[39m [2/100] try in 5s once again ...\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[33.780338, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m [2/100] check for resourcemanager:8088...\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m [2/100] resourcemanager:8088 is not available yet\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m [2/100] try in 5s once again ...\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[33.966946, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,981 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,981 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,981 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[33m\u001b[1mresourcemanag"]
[33.967018, "o", "er    |\u001b(B\u001b[m 2021-01-19 03:14:08,981 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,981 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,981 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,981"]
[33.967032, "o", " INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,981 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,981 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:08,981 INFO event.AsyncDispatcher: Register"]
[33.967046, "o", "ing class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[34.038066, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:09,055 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.30.0.5:10200\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[34.123962, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:09,139 INFO util.log: Logging initialized @3152ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[34.191455, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:09,209 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[34.194816, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:09,212 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[34.202255, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:09,219 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[34.209487, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:09,222 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:09,223 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:09,223 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:09,223 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:09,223 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFi"]
[34.209553, "o", "lter$StaticUserFilter) to context logs\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:09,223 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[34.213498, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:09,230 INFO http.HttpServer2: adding path spec: /cluster/*\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:09,230 INFO http.HttpServer2: adding path spec: /ws/*\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:09,230 INFO http.HttpServer2: adding path spec: /app/*\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[34.59086, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:09,608 INFO webapp.WebApps: Registered webapp guice modules\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[34.596669, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:09,613 INFO http.HttpServer2: Jetty bound to port 8088\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[34.597533, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:09,615 INFO server.Server: jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[34.626768, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:09,644 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[37.441504, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:12,458 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[37.443132, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:12,460 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[37.445726, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:12,461 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[37.446419, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:12,463 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4e76dac{/logs,file:///opt/hadoop-3.2.1/logs/,AVAILABLE}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[37.447154, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:12,464 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5d465e4b{/static,jar:file:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar!/webapps/static,AVAILABLE}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[37.604169, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:12 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[37.604677, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:12 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[37.605371, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:12 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[37.606264, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:12 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[37.643295, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:12 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[37.959651, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:12 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[38.183404, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:13 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[38.205451, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:13,223 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@49ec6a9f{/,file:///tmp/jetty-0.0.0.0-8088-cluster-_-any-2310872984492316210.dir/webapp/,AVAILABLE}{/cluster}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[38.211428, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:13,228 INFO server.AbstractConnector: Started ServerConnector@25b52284{HTTP/1.1,[http/1.1]}{0.0.0.0:8088}\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:13,229 INFO server.Server: Started @7242ms\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:13,229 INFO webapp.WebApps: Web app cluster started at 8088\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[38.270443, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:13,287 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[38.277642, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:13,295 INFO ipc.Server: Starting Socket Reader #1 for port 8033\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[38.306288, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:13,323 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[38.307809, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:13,325 INFO ipc.Server: IPC Server Responder: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[38.308639, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:13,325 INFO ipc.Server: IPC Server listener on 8033: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[38.320676, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:13,337 INFO resourcemanager.ResourceManager: Transitioning to active state\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[38.748496, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m [3/100] resourcemanager:8088 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[38.789141, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m [3/100] resourcemanager:8088 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[38.840482, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m DEPRECATED: Use of this command to start the timeline server is deprecated.\r\n\u001b[35mhistoryserver      |\u001b[39m Instead use the timelineserver command for it.\r\n\u001b[35mhistoryserver      |\u001b[39m Starting the History Server anyway...\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.005031, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:14,017 INFO ipc.Server: IPC Server handler 0 on default port 9000, call Call#0 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.30.0.4:45564: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[34mnamenode           |\u001b[39m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 23 seconds. NamenodeHostName:namenode\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.030039, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,037 INFO recovery.FileSystemRMStateStore: Exception while executing an FS operation.\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 23 seconds. NamenodeHostName:namenode\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.ja"]
[39.031814, "o", "va:3232)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.A"]
[39.034258, "o", "ccessController.doPrivileged(Native Method)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteEx"]
[39.03744, "o", "ception.java:121)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2427)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2401)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1318)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1315)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1332)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1307)\r\n"]
[39.038982, "o", "\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(F\u001b[1;46r\u001b[H\u001b[45;113H\u001b[1;45r\u001b[H\u001b[45;113HileSystemRMStateStore.java:626)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:623)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$FSAction.runWithRetries(FileSystemRMStateStore.java:739)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.mkdirsWithRetries(FileSystemRMStateStore.java:629)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.startInternal(FileSystemRMStateStore.java:156)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b["]
[39.039459, "o", "3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.serviceStart(RMStateStore.java:746)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:868)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1262)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1303)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1299)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivilege"]
[39.03981, "o", "d(Native Method)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1299)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1350)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[1;46r\u001b[H\u001b[45;25H\u001b[97C\u001b[?25l\u001b[H\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    ... 26 more\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,039 INFO recovery.FileSystemRMStateStore: Maxed out FS retries. Giving up!\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,039 INFO service.AbstractService: Service org.apache.hadoop.yarn.server.resourcemanager."]
[39.040109, "o", "recovery.RMStateStore failed in state STARTED\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 23 seconds. NamenodeHostName:namenode\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.Name"]
[39.040386, "o", "NodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.security.AccessController.doPrivileged(Native Method)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b["]
[39.04065, "o", "m    at javax.security.auth.Subject.doAs(Subject.java:422)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at"]
[39.040916, "o", " org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2427)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2401)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1318)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1315)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1332)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1307)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at or"]
[39.041171, "o", "g.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:626)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:623)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$FSAction.runWithRetries(FileSystemRMStateStore.java:739)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.mkdirsWithRetries(FileSystemRMStateStore.java:629)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.startInternal(FileSystemRMStateStore.java:156)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStor"]
[39.041435, "o", "e.serviceStart(RMStateStore.java:746)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:868)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.a\u001b[K\u001b[?12l\u001b[?25h\u001b[1;45r\u001b[H\u001b[45;33Hpache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1262)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1303)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1299)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[33m\u001b[1mres"]
[39.041748, "o", "ourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1299)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1350)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1535)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[33m\u001b[1mresourc"]
[39.04179, "o", "emanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 23 seconds. NamenodeHostName:namenode\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b"]
[39.041811, "o", "[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:"]
[39.041837, "o", "2915)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.call(Client.java:1491)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.call(Client.java:1388)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat com.sun.proxy.$Proxy86.mkdirs(Unknown Source)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop"]
[39.042468, "o", "\u001b[1;46r\u001b[H\u001b[45;45H\u001b[1;45r\u001b[H\u001b[45;45H.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Method.invoke(Method.java:498)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\r\n\u001b[33m\u001b[1mr"]
[39.04251, "o", "esourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat com.sun.proxy.$Proxy87.mkdirs(Unknown Source)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3C... 26 more\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,040 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.05943, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,064 INFO service.AbstractService: Service RMActiveServices failed in state STARTED\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m org.apache.hadoop.service.ServiceStateException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 23 seconds. NamenodeHostName:namenode\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.serv"]
[39.061741, "o", "er.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[33m\u001b[1mre"]
[39.06237, "o", "sourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:203)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:868)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b"]
[39.062716, "o", "[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1262)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1303)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1299)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1299)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1350)\r\n\u001b[33m"]
[39.062879, "o", "\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(Res\u001b[1;46r\u001b[H\u001b[45;98H\u001b[1;45r\u001b[H\u001b[45;98HourceManager.java:1535)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 23 seconds. NamenodeHostName:namenode\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(F"]
[39.063071, "o", "SNamesystem.java:1463)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[33m\u001b[1mresourcema"]
[39.063211, "o", "nager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Constructor.newI"]
[39.063374, "o", "nstance(Constructor.java:423)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2427)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2401)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1318)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1315)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem."]
[39.063402, "o", "java:1332)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1307)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:626)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.res\u001b[1;46r\u001b[H\u001b[45;61H\u001b[1;45r\u001b[H\u001b[45;61Hourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:623)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$FSAction.runWithRetries(FileSystemRMStateStore.java:739)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.mkdirsWithRetries(FileSystemRMStateStore.java:629)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.se"]
[39.063414, "o", "rver.resourcemanager.recovery.FileSystemRMStateStore.startInternal(FileSystemRMStateStore.java:156)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.serviceStart(RMStateStore.java:746)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3C... 12 more\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 23 seconds. NamenodeHostName:namenode\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNam"]
[39.063427, "o", "esystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.064147, "o", "\u001b[?25l\u001b[H\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.serviceStart(RMStateStore.java:746)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    ... 12 more\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 23 seconds. NamenodeHostName:namenode\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager"]
[39.064306, "o", "    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\u001b[K\r\n\u001b[33m\u001b[1mreso"]
[39.064465, "o", "urcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.security.AccessController.doPrivileged(Native Method)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at javax.security.auth.Subject.doAs(Subject.java:422)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1491)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1388)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org."]
[39.064487, "o", "apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy86.mkdirs(Unknown Source)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.lang.reflect.Method.invoke(Method.java:498)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invo"]
[39.064498, "o", "keMethod(RetryInvocationHandler.java:422)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy87.mkdirs(Unknown Source)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    ... 26 more\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,070 INFO impl.MetricsSystemImpl: Stopping ResourceManager metrics system...\u001b[K\r\n\u001b[33m\u001b[1mres"]
[39.06451, "o", "ourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,070 INFO impl.MetricsSystemImpl: ResourceManager metrics system stopped.\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,071 INFO impl.MetricsSystemImpl: ResourceManager metrics system shutdown complete.\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[39.07116, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,087 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,088 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,088 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.077826, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,092 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,092 INFO recovery.RMStateStoreFactory: Using RMStateStore implementation - class org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,092 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,093 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,093 INFO resourcem"]
[39.077912, "o", "anager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,093 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,093 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,093 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,093 INFO event.AsyncDispatcher: "]
[39.077941, "o", "Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.079292, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,096 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.08042, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,097 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,097 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.101748, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,116 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,117 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,117 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,117 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,117 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,117 INF"]
[39.101815, "o", "O util.HostsFileReader: Refreshing hosts (include/exclude) list\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.106484, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,120 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/opt/hadoop-3.2.1/etc/hadoop/capacity-scheduler.xml\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.128265, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,142 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1024, vCores:1>\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,142 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:8192, vCores:4>\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.138274, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,155 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,155 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.149387, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,164 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m , reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,164 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.162245, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,172 INFO capacity.LeafQueue: Initializing default\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m capacity = 1.0 [= (float) configuredCapacity / 100 ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxCapacity = 1.0 [= configuredMaxCapacity ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m effectiveMinResource=<memory:0, vCores:0>\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m  , effectiveMaxResource=<memory:0, vCores:0>\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimit = 100 [= configuredUserLimit ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimitFactor = 1.0 [= configuredUserLimitFactor ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemAp"]
[39.162817, "o", "plications * absoluteCapacity)]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m maximumAllocation = <memory:8192, vCores:4> [= configuredMaxAllocation ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m numContainers = 0 [= currentNumContainers ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m state = RUNNING [= configuredState ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= conf"]
[39.162882, "o", "iguredAcls ]\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m nodeLocalityDelay = 40\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m rackLocalityAdditionalDelay = -1\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m labels=*,\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m reservationsContinueLooking = true\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m preemptionDisabled = true\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m defaultAppPriorityPerQueue = 0\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m priority = 0\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxLifetime = -1 seconds\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m defaultLifetime = -1 seconds\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,172 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,172 INFO capacity.CapacitySchedu"]
[39.162895, "o", "lerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,172 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,172 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,172 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are \r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,172 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAlloc"]
[39.162908, "o", "ation=<<memory:8192, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b[1;46r\u001b[H\u001b[45;21H\u001b(B\u001b[m\u001b[1;45r\u001b[H\u001b[45;21H 2021-01-19 03:14:14,174 INFO conf.Configuration: dynamic-resources.xml not found\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,176 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,176 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,176 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. \r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.19383, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,191 INFO service.AbstractService: Service ResourceManager failed in state STARTED\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m org.apache.hadoop.service.ServiceStateException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 23 seconds. NamenodeHostName:namenode\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.serve"]
[39.193919, "o", "r.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[33m\u001b[1mres"]
[39.193938, "o", "ourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:203)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:868)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b["]
[39.193955, "o", "3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1262)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1303)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1299)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1299)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1350)\r\n\u001b[33m\u001b"]
[39.19397, "o", "[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(Reso\u001b[1;46r\u001b[H\u001b[45;99H\u001b[1;45r\u001b[H\u001b[45;99HurceManager.java:1535)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 23 seconds. NamenodeHostName:namenode\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FS"]
[39.194, "o", "Namesystem.java:1463)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[33m\u001b[1mresourceman"]
[39.194017, "o", "ager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Constructor.newIn"]
[39.194034, "o", "stance(Constructor.java:423)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2427)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2401)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1318)\r\n\u001b[1;46r\u001b[H\u001b[45;62H\r\u001b[?25l\u001b[H\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. T"]
[39.194049, "o", "he minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 23 seconds. NamenodeHostName:namenode\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNa"]
[39.194068, "o", "menodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.security.AccessController.doPrivileged(Native Method)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at javax.security.auth.Subject.doAs(Subject.java:422)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[K\r\n\u001b[33m\u001b[1mresourcem"]
[39.194085, "o", "anager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1491)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1388)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy86.mkdirs(Unknown Source)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java"]
[39.194096, "o", ":62)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.lang.reflect.Method.invoke(Method.java:498)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy87.mkdirs(Unknow"]
[39.194108, "o", "n Source)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    ... 26 more\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,198 ERROR delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,200 INFO handler.ContextHandler: Stopped o.e.j.w.WebAppContext@49ec6a9f{/,null,UNAVAILABLE}{/cluster}\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,203 INFO server.AbstractConnector: Stopped ServerConnector@25b52284{HTTP/1.1,[http/1.1]}{0.0.0.0:8088}\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,204 INFO handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@5d465e4b{/static,jar:file:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar!/webapps/static,UNAVAILABLE}\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,204 INFO h"]
[39.19412, "o", "andler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@4e76dac{/logs,file:///opt/hadoop-3.2.1/logs/,UNAVAILABLE}\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[39.292719, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,307 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,308 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,308 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,308 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,308 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,309 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01"]
[39.29281, "o", "-19 03:14:14,309 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,309 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,309 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,309 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,309 INFO ipc.Server: Stopping server on 8033\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.300745, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,317 INFO resourcemanager.ResourceManager: Transitioning to standby state\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.307632, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,317 INFO resourcemanager.ResourceManager: Transitioned to standby state\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,317 FATAL resourcemanager.ResourceManager: Error starting ResourceManager\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m org.apache.hadoop.service.ServiceStateException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 23 seconds. NamenodeHostName:namenode\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesyst"]
[39.309089, "o", "em.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java"]
[39.30954, "o", ":999)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:203)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:868)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m"]
[39.309567, "o", " \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1262)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1303)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1299)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1299)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b("]
[39.309584, "o", "B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1350)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.j\u001b[1;46r\u001b[H\u001b[45;93H\u001b[1;45r\u001b[H\u001b[45;93Hava:194)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1535)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 23 seconds. NamenodeHostName:namenode\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)"]
[39.309609, "o", "\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b"]
[39.309624, "o", "[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(D"]
[39.309639, "o", "elegatingConstructorAccessorImpl.java:45)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2427)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2401)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1318)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1315)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\u001b[33m"]
[39.309654, "o", "\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1332)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1307)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStat\u001b[1;46r\u001b[H\u001b[45;99H\u001b[1;45r\u001b[H\u001b[45;99HeStore$3.run(FileSystemRMStateStore.java:626)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:623)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$FSAction.runWithRetries(FileSystemRMStateStore.java:739)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore."]
[39.309667, "o", "mkdirsWithRetries(FileSystemRMStateStore.java:629)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.startInternal(FileSystemRMStateStore.java:156)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.serviceStart(RMStateStore.java:746)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3C... 12 more\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 23 sec"]
[39.30968, "o", "onds. NamenodeHostName:namenode\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Prot"]
[39.309691, "o", "obufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.cal"]
[39.309705, "o", "l(Client.java:1491)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.call(Client.java:1388)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat com.sun.proxy.$Proxy86.mkdirs(Unknown Source)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n"]
[39.309744, "o", "\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Method.invoke(Method.java:498)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocati"]
[39.30976, "o", "onHandler.invoke(RetryInvocationHandler.java:359)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat com.sun.proxy.$Proxy87.mkdirs(Unknown Source)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3C... 26 more\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.314695, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,318 INFO ipc.Server: Stopping IPC Server listener on 8033\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.317808, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,322 INFO ipc.Server: Stopping IPC Server Responder\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.320841, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,333 INFO resourcemanager.ResourceManager: SHUTDOWN_MSG: \r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m /************************************************************\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m SHUTDOWN_MSG: Shutting down ResourceManager at a07c851f4b7d/172.30.0.4\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.325057, "o", "\u001b[?25l\u001b[H\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 23 seconds. NamenodeHostName:namenode\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.name"]
[39.325397, "o", "node.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.security.AccessController.doPrivileged(Native Method)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager "]
[39.325568, "o", "   |\u001b(B\u001b[m    at javax.security.auth.Subject.doAs(Subject.java:422)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1491)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1388)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy86.mkdirs(Unknown Source)\u001b[K\r\n\u001b[33m\u001b[1mresourcemana"]
[39.325602, "o", "ger    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.lang.reflect.Method.invoke(Method.java:498)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:"]
[39.325615, "o", "157)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy87.mkdirs(Unknown Source)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m    ... 26 more\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,318 INFO ipc.Server: Stopping IPC Server listener on 8033\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,322 INFO ipc.Server: Stopping IPC Server Responder\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:14,333 INFO resourcemanager.ResourceManager: SHUTDOWN_MSG: \u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m /************************************************************\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager "]
[39.325628, "o", "   |\u001b(B\u001b[m SHUTDOWN_MSG: Shutting down ResourceManager at a07c851f4b7d/172.30.0.4\u001b[K\r\n\u001b[33m\u001b[1mresourcemanager    |\u001b(B\u001b[m ************************************************************/\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[39.436662, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:14,450 INFO applicationhistoryservice.ApplicationHistoryServer: STARTUP_MSG: \r\n\u001b[35mhistoryserver      |\u001b[39m /************************************************************\r\n\u001b[35mhistoryserver      |\u001b[39m STARTUP_MSG: Starting ApplicationHistoryServer\r\n\u001b[35mhistoryserver      |\u001b[39m STARTUP_MSG:   host = a09af24f6445/172.30.0.5\r\n\u001b[35mhistoryserver      |\u001b[39m STARTUP_MSG:   args = []\r\n\u001b[35mhistoryserver      |\u001b[39m STARTUP_MSG:   version = 3.2.1\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.441654, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop-3.2.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-lang3-3.7.jar:/opt/ha"]
[39.442004, "o", "doop-3.2.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-text-1.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/dnsjava-2.1.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/comm"]
[39.442153, "o", "on/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-json-1.19"]
[39.442307, "o", ".jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-servlet-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsr305-3.0"]
[39.44249, "o", ".0.jar:/opt/had\u001b[1;46r\u001b[H\u001b[45;97H\u001b[1;45r\u001b[H\u001b[45;97Hoop-3.2.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/shar"]
[39.442628, "o", "e/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/zookeeper"]
[39.442648, "o", "-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-kms-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/"]
[39.442661, "o", "share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-text-1.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/sha"]
[39.442672, "o", "re/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/o"]
[39.446657, "o", "\u001b[1;46r\u001b[H\u001b[45;202H\u001b[1;45r\u001b[H\u001b[45;202Hpt/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hado"]
[39.447275, "o", "op-3.2.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar"]
[39.447505, "o", ":/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-"]
[39.447659, "o", "pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/token-provider-1.0."]
[39.447814, "o", "1.jar:/opt/hadoop-3.2.1/share/hadoop/\u001b[1;46r\u001b[H\u001b[45;97H\u001b[1;45r\u001b[H\u001b[45;97Hhdfs/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-"]
[39.447983, "o", "app-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-3.2."]
[39.448004, "o", "1/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/hadoo"]
[39.448016, "o", "p-3.2.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-registry-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/opt/hadoop-3.2.1/sh"]
[39.448029, "o", "are/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar:/opt/hadoop-3.2.1/sh\u001b[1;46r\u001b[H\u001b[45;202H\u001b[1;45r\u001b[H\u001b[45;202Hare/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar\r\n\u001b[35mhistoryserver      |\u001b[39m STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z\r\n\u001b[35mhistoryserver      |\u001b[39m STARTUP_MSG:   java = 1.8.0_232\r\n\u001b[35mhistoryserver      |\u001b[39m ************************************************************/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.448281, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:14,464 INFO applicationhistoryservice.ApplicationHistoryServer: registered UNIX signal handlers for [TERM, HUP, INT]\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[39.49669, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:14,505 INFO nodemanager.NodeManager: STARTUP_MSG: \r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m /************************************************************\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m STARTUP_MSG: Starting NodeManager\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m STARTUP_MSG:   host = e14f83ec4f23/172.30.0.6\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m STARTUP_MSG:   args = []\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m STARTUP_MSG:   version = 3.2.1\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop-3.2.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-b"]
[39.498692, "o", "eanutils-1.9.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-text-1.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/dnsjava-2.1.7.jar:/opt/hadoop"]
[39.500596, "o", "-3.2.1/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoo"]
[39.5017, "o", "p/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/commo"]
[39.502037, "o", "n/lib/jetty-servlet-9.3.24.v20180605.jar:/\u001b[1;46r\u001b[H\u001b[45;51H\u001b[1;45r\u001b[H\u001b[45;51Hopt/hadoop-3.2.1/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/"]
[39.502304, "o", "kerb-identity-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/o"]
[39.502448, "o", "pt/hadoop-3.2.1/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-kms-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/a"]
[39.502563, "o", "udience-annotations-0.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-text-1.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/li"]
[39.502691, "o", "b/curator-client-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-f\u001b[1;46r\u001b[H\u001b[45;156H\u001b[1;45r\u001b[H\u001b[45;156Hramework-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-cor"]
[39.502831, "o", "e-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty"]
[39.503012, "o", "-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:"]
[39.503127, "o", "/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Fi"]
[39.50323, "o", "nal.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib\u001b[1;46r\u001b[H\u001b[45;51H\u001b[1;45r\u001b[H\u001b[45;51H/okhttp-2.7.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3"]
[39.503377, "o", ".2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-ma"]
[39.503397, "o", "preduce-client-jobclient-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2."]
[39.503409, "o", "9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.1"]
[39.503422, "o", ".jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/opt/hadoop-3."]
[39.50377, "o", "\u001b[1;46r\u001b[H\u001b[45;156H\u001b[1;45r\u001b[H\u001b[45;156H2.1/share/hadoop/yarn/hadoop-yarn-registry-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar:/opt/hadoop"]
[39.503816, "o", "-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/commons-csv-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/commons-lang-2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-annotations-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-client-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-common-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-protocol-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/ti"]
[39.503841, "o", "melineservice/lib/jcodings-1.0.13.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/joni-2.1.2.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/metrics-core-2.2.0.jar\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z\u001b[1;46r\u001b[H\u001b[45;186H\u001b[1;45r\u001b[H\u001b[45d\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m STARTUP_MSG:   java = 1.8.0_232\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m ************************************************************/\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:14,514 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.048904, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,063 INFO resourceplugin.ResourcePluginManager: No Resource plugins found from configuration!\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,064 INFO resourceplugin.ResourcePluginManager: Found Resource plugins from configuration: null\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.094451, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,109 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.134042, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,150 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.175292, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,189 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,189 INFO impl.MetricsSystemImpl: ApplicationHistoryServer metrics system started\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.18689, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,203 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.189564, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,204 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,205 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,206 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,206 INFO even"]
[40.189665, "o", "t.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.190722, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,206 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,207 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.194608, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,210 INFO tracker.NMLogAggregationStatusTracker: the rolling interval seconds for the NodeManager Cached Log aggregation status is 600\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.212546, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,229 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,230 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.272707, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,286 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.324193, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,339 INFO timeline.LeveldbTimelineStore: Using leveldb path /hadoop/yarn/timeline/leveldb-timeline-store.ldb\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.357319, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,374 INFO timeline.LeveldbTimelineStore: Loaded timeline store version info 1.0\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.358423, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,375 INFO timeline.LeveldbTimelineStore: Starting deletion thread with ttl 604800000 and cycle interval 300000\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.360092, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,376 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,376 INFO impl.MetricsSystemImpl: NodeManager metrics system started\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.407279, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,422 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.41676, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,431 INFO nodemanager.DirectoryCollection: Disk Validator 'basic' is loaded.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.473773, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,491 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@20140db9\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.481185, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,493 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,494 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,495 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,495 INFO localizer.ResourceLocalizationService: per directory file limit = 8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.535402, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,549 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.551125, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,555 INFO resources.ResourceHandlerModule: Using traffic control bandwidth handler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.573266, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,582 INFO containermanager.AuxServices: Adding auxiliary service mapreduce_shuffle, \"mapreduce_shuffle\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.595114, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,604 INFO timeline.LeveldbTimelineStore: Discarded 0 entities for timestamp 1610421255379 and earlier in 0.001 seconds\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,606 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,610 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@1807e3f6\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,611 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.601717, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,612 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,612 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,612 INFO monitor.ContainersMonitorImpl: Elastic memory control enabled: false\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,612 INFO monitor.ContainersMonitorImpl: Strict memory control enabled: true\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,612 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.602325, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,617 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,617 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.611124, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,623 WARN monitor.ContainersMonitorImpl: NodeManager configured with 16 G physical memory allocated to containers, which is more than 80% of the total physical memory available (11.6 G). Thrashing might happen.\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,623 INFO logaggregation.LogAggregationService: rollingMonitorInterval is set as -1. The log rolling monitoring interval is disabled. The logs will be aggregated after this application is finished.\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,623 INFO logaggregation.LogAggregationService: rollingMonitorInterval is set as -1. The logs will be aggregated every -1 seconds\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,624 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.635902, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,653 INFO conf.Configuration: resource-types.xml not found\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,653 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.646867, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,658 INFO conf.Configuration: node-resources.xml not found\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,659 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,660 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:16384, vCores:8>\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.651855, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,664 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=16384 virtual-memory=34407 virtual-cores=8\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.663656, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,678 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.672828, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting fs.defaultFS=hdfs://namenode:9000\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.685038, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,699 INFO ipc.Server: Starting Socket Reader #1 for port 10200\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.692653, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting hadoop.http.staticuser.user=root\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.715683, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,728 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationHistoryProtocolPB to the server\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,729 INFO ipc.Server: IPC Server Responder: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.718651, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,732 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 2000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.721377, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,734 INFO ipc.Server: IPC Server listener on 10200: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.726101, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,743 INFO applicationhistoryservice.ApplicationHistoryClientService: Instantiated ApplicationHistoryClientService at historyserver/172.30.0.5:10200\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.728579, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,746 INFO util.JvmPauseMonitor: Starting JVM pause monitor\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.732253, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting hadoop.proxyuser.hue.groups=*\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.736282, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,753 INFO timeline.TimelineServerUtils: Filter initializers set for timeline service: org.apache.hadoop.http.lib.StaticUserWebFilter,org.apache.hadoop.yarn.server.timeline.security.TimelineAuthenticationFilterInitializer\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.738554, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,756 INFO ipc.Server: Starting Socket Reader #1 for port 0\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.742831, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring hdfs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.758104, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,775 INFO util.log: Logging initialized @1900ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.759789, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting dfs.namenode.datanode.registration.ip-hostname-check=false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.768456, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting dfs.webhdfs.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.782626, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting dfs.permissions.enabled=false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.788224, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring yarn\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.807433, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.timeline-service.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.829181, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-vcores=4\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.839134, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.846348, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,864 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.849044, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.849936, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,867 INFO http.HttpRequestLog: Http request log for http.requests.applicationhistory is not defined\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.860717, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,875 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,877 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context applicationhistory\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,877 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,878 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.86515, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.86623, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,881 INFO http.HttpServer2: Added global filter 'Timeline Authentication Filter' (class=org.apache.hadoop.yarn.server.timeline.security.TimelineAuthenticationFilter)\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,883 INFO http.HttpServer2: adding path spec: /applicationhistory/*\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,883 INFO http.HttpServer2: adding path spec: /ws/*\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.882739, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.896901, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.908216, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.timeline-service.generic-application-history.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.916258, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,932 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,933 INFO ipc.Server: IPC Server Responder: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.918041, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,934 INFO ipc.Server: IPC Server listener on 0: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.926523, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.log-aggregation-enable=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.940238, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,956 INFO security.NMContainerTokenSecretManager: Updating node address : e14f83ec4f23:38946\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.942952, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.hostname=resourcemanager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.956198, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.965566, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.aux-services=mapreduce_shuffle\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.968287, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,985 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.971222, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,988 INFO ipc.Server: Starting Socket Reader #1 for port 8040\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.974105, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,991 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.977145, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,994 INFO ipc.Server: IPC Server listener on 8040: starting\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,994 INFO ipc.Server: IPC Server Responder: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[40.981779, "o", "\u001b[?25l\u001b[H\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,743 INFO applicationhistoryservice.ApplicationHistoryClientService: Instantiated ApplicationHistoryClientService at historyserver/172.30.0.5:10200\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,746 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting hadoop.proxyuser.hue.groups=*\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,753 INFO timeline.TimelineServerUtils: Filter initializers set for timeline service: org.apache.hadoop.http.lib.StaticUserWebFilter,org.apache.hadoop.yarn.server.timeline.security.TimelineAuthenticationFilterInitializer\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,756 INFO ipc.Server: Starting Socket Reader #1 for port 0\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring hdfs\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,775 INFO util.log: Logging initialized @1900ms\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting dfs.namenode.datanod"]
[40.982206, "o", "e.registration.ip-hostname-check=false\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting dfs.webhdfs.enabled=true\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting dfs.permissions.enabled=false\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring yarn\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.timeline-service.enabled=true\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-vcores=4\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,864 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,867 INFO http.HttpRequestLog: Http request log for http.requests.a"]
[40.982494, "o", "pplicationhistory is not defined\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,875 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,877 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context applicationhistory\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,877 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,878 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,881 INF"]
[40.982521, "o", "O http.HttpServer2: Added global filter 'Timeline Authentication Filter' (class=org.apache.hadoop.yarn.server.timeline.security.TimelineAuthenticationFilter)\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,883 INFO http.HttpServer2: adding path spec: /applicationhistory/*\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:15,883 INFO http.HttpServer2: adding path spec: /ws/*\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.timeline-service.generic-application-history.enabled=true\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,932 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,933 INFO ipc.Server: IPC Server Responder: starting\u001b[K\r\n\u001b[3"]
[40.982533, "o", "6m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,934 INFO ipc.Server: IPC Server listener on 0: starting\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.log-aggregation-enable=true\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,956 INFO security.NMContainerTokenSecretManager: Updating node address : e14f83ec4f23:38946\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.hostname=resourcemanager\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-mb=8192\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.aux-services=mapreduce_shuffle\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,985 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 500, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,988 INFO ipc.Server: Starting Socket Reader #1 for po"]
[40.982546, "o", "rt 8040\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,991 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,994 INFO ipc.Server: IPC Server listener on 8040: starting\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,994 INFO ipc.Server: IPC Server Responder: starting\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:15,997 INFO localizer.ResourceLocalizationService: Localizer started on port 8040\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[40.985356, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.resource_tracker.address=resourcemanager:8031\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.002238, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.timeline-service.hostname=historyserver\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.012938, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.scheduler.address=resourcemanager:8030\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.02594, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,042 INFO mapred.IndexCache: IndexCache created with max memory = 10485760\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.033542, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.address=resourcemanager:8032\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.046165, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapred.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.049836, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,065 INFO mapred.ShuffleHandler: mapreduce_shuffle listening on port 13562\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.054259, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,070 INFO containermanager.ContainerManagerImpl: ContainerManager started at e14f83ec4f23/172.30.0.6:38946\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,070 INFO containermanager.ContainerManagerImpl: ContainerManager bound to 0.0.0.0/0.0.0.0:0\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.057708, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,075 INFO webapp.WebServer: Instantiating NMWebApp at 0.0.0.0:8042\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.074278, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.remote-app-log-dir=/app-logs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.087291, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.098225, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.map.output.compress=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.124675, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.resource.memory-mb=16384\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.136317, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.recovery.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.153293, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,168 INFO util.log: Logging initialized @2266ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.157264, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.resource.cpu-vcores=8\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.162684, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring httpfs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.185334, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring kms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.201784, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring mapred\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.212254, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.map.java.opts=-Xmx3072m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.233896, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.reduce.java.opts=-Xmx6144m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.250618, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.reduce.memory.mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.25839, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,274 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.259368, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,276 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.266308, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,283 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.267322, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,284 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,285 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.270124, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,288 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.271426, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,288 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context node\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,288 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context logs\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,289 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.authentication.server.AuthenticationFilter) to context static\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.275273, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,291 INFO http.HttpServer2: adding path spec: /node/*\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,291 INFO http.HttpServer2: adding path spec: /ws/*\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.277896, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.306912, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.map.memory.mb=4096\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.329314, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapred.child.java.opts=-Xmx4096m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.336279, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:16,352 INFO webapp.WebApps: Registered webapp guice modules\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.337088, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:16,354 INFO http.HttpServer2: Jetty bound to port 8188\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.337887, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:16,355 INFO server.Server: jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.34297, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.361663, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.framework.name=yarn\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.3701, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.378945, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring for multihomed network\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.386298, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:16,402 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.388855, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:16,403 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.393419, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:16,409 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1c852c0f{/logs,file:///opt/hadoop-3.2.1/logs/,AVAILABLE}\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:16,410 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@13b3d178{/static,jar:file:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar!/webapps/static,AVAILABLE}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.527955, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] namenode:9000 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.534336, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] namenode:9870 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.536487, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] datanode:9864 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.541063, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] datanode2:9864 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.547107, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] datanode3:9864 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.609095, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:16 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Registering org.apache.hadoop.yarn.webapp.YarnJacksonJaxbJsonProvider as a provider class\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:16 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Registering org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices as a root resource class\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:16 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Registering org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices as a root resource class\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:16 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Registering org.apache.hadoop."]
[41.60916, "o", "yarn.webapp.GenericExceptionHandler as a provider class\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:16 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.642215, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,658 INFO webapp.WebApps: Registered webapp guice modules\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.642922, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,660 INFO http.HttpServer2: Jetty bound to port 8042\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.64451, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,662 INFO server.Server: jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.684853, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,702 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.686202, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,703 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2eced48b{/logs,file:///opt/hadoop-3.2.1/logs/,AVAILABLE}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.691554, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:16,705 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4b7e96a{/static,jar:file:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar!/webapps/static,AVAILABLE}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.70744, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:16 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.713195, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:16 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Binding org.apache.hadoop.yarn.webapp.YarnJacksonJaxbJsonProvider to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.936264, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m Jan 19, 2021 3:14:16 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m Jan 19, 2021 3:14:16 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m Jan 19, 2021 3:14:16 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[41.937266, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m Jan 19, 2021 3:14:16 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[42.005762, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m Jan 19, 2021 3:14:17 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[42.223978, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m Jan 19, 2021 3:14:17 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[42.539896, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:17 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Binding org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[42.551788, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:17,562 INFO resourcemanager.ResourceManager: STARTUP_MSG: \r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m /************************************************************\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG: Starting ResourceManager\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   host = a07c851f4b7d/172.30.0.4\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   args = []\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   version = 3.2.1\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop-3.2.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/l"]
[42.554544, "o", "ib/commons-beanutils-1.9.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-text-1.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/dnsjava-2.1.7.jar"]
[42.556308, "o", ":/opt/hadoop-3.2.1/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1"]
[42.556731, "o", "/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/"]
[42.556899, "o", "hadoop/common/lib/jetty-servlet-9.3.24.v20\u001b[1;46r\u001b[H\u001b[45;39H\u001b[1;45r\u001b[H\u001b[45;39H180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop"]
[42.557028, "o", "/common/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/protobuf-java-"]
[42.557203, "o", "2.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-kms-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoo"]
[42.557322, "o", "p/hdfs/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-text-1.4.jar:/opt/hadoop-3.2.1/share/ha"]
[42.557341, "o", "doop/hdfs/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/l\u001b[1;46r\u001b[H\u001b[45;144H\u001b[1;45r\u001b[H\u001b[45;144Hib/curator-framework-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib"]
[42.557353, "o", "/jackson-core-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hd"]
[42.557366, "o", "fs/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/l"]
[42.557686, "o", "\u001b[1;46r\u001b[H\u001b[45;150H\u001b[1;45r\u001b[H\u001b[45;150Hib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-"]
[42.557849, "o", "pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/token-provider-1.0."]
[42.557991, "o", "1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/opt/hadoop-3.2.1/shar"]
[42.558127, "o", "e/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/bcpkix-jdk15"]
[42.558257, "o", "on-1.60.jar:/opt/hadoop-3.2.1/share/h\u001b[1;46r\u001b[H\u001b[45;45H\u001b[1;45r\u001b[H\u001b[45;45Hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/hadoo"]
[42.558384, "o", "p-3.2.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-registry-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/opt/hadoop-3.2.1/sh"]
[42.558403, "o", "are/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1.jar:/opt/hadoop"]
[42.558415, "o", "-3.2.1/share/hadoop/yarn/timelineservice/lib/commons-csv-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/commons-lang-2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-annotations-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-client-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-common-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-protocol-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/jcodings-1.0.13.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/joni-2.1.2.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/metrics-core-2.2.0.jar\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m START"]
[42.558427, "o", "UP_MSG:   java = 1.8.0_232\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m ********************\u001b[1;46r\u001b[H\u001b[45;42H\u001b[1;45r\u001b[H\u001b[45;42H****************************************/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[42.571939, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:17,585 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:17 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Binding org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[42.598601, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:17,613 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7ca0863b{/,file:///tmp/jetty-0.0.0.0-8188-applicationhistory-_-any-3079152426846794655.dir/webapp/,AVAILABLE}{/applicationhistory}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[42.619652, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:17,635 INFO server.AbstractConnector: Started ServerConnector@4a55a6e8{HTTP/1.1,[http/1.1]}{0.0.0.0:8188}\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:17,635 INFO server.Server: Started @3760ms\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:17,635 INFO applicationhistoryservice.ApplicationHistoryServer: Instantiating AHSWebApp at 8188\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.069667, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,081 INFO conf.Configuration: found resource core-site.xml at file:/opt/hadoop-3.2.1/etc/hadoop/core-site.xml\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.108236, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,123 INFO conf.Configuration: resource-types.xml not found\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,123 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.154088, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m Jan 19, 2021 3:14:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.161331, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,178 INFO conf.Configuration: found resource yarn-site.xml at file:/opt/hadoop-3.2.1/etc/hadoop/yarn-site.xml\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.182954, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,200 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.194615, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:18,208 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2f98635e{/,file:///tmp/jetty-0.0.0.0-8042-node-_-any-8525242653921229362.dir/webapp/,AVAILABLE}{/node}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.205024, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:18,219 INFO server.AbstractConnector: Started ServerConnector@117bcfdc{HTTP/1.1,[http/1.1]}{0.0.0.0:8042}\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:18,219 INFO server.Server: Started @4317ms\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:18,219 INFO webapp.WebApps: Web app node started at 8042\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.211279, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:18,222 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : e14f83ec4f23:38946\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.220384, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:18,234 INFO util.JvmPauseMonitor: Starting JVM pause monitor\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.22352, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:18,239 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.30.0.4:8031\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.252751, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,266 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.257318, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,271 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.265372, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,281 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.285382, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:18,298 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.298482, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:18,313 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.323514, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,334 INFO recovery.RMStateStoreFactory: Using RMStateStore implementation - class org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,336 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.326119, "o", "\u001b[?25l\u001b[He-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/commons-csv-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/commons-lang-2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-annotations-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-client-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-common-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-protocol-1.2.6.jar:/opt/hadoop-3.2.1/share/had"]
[43.326284, "o", "oop/yarn/timelineservice/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/jcodings-1.0.13.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/joni-2.1.2.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/metrics-core-2.2.0.jar\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   java = 1.8.0_232\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m ************************************************************/\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:17,585 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:17 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Binding org.apac"]
[43.326358, "o", "he.hadoop.yarn.server.timeline.webapp.TimelineWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:17,613 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7ca0863b{/,file:///tmp/jetty-0.0.0.0-8188-applicationhistory-_-any-3079152426846794655.dir/webapp/,AVAILABLE}{/applicationhistory}\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:17,635 INFO server.AbstractConnector: Started ServerConnector@4a55a6e8{HTTP/1.1,[http/1.1]}{0.0.0.0:8188}\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:17,635 INFO server.Server: Started @3760ms\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m 2021-01-19 03:14:17,635 INFO applicationhistoryservice.ApplicationHistoryServer: Instantiating AHSWebApp at 8188\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,081 INFO conf.Configuration: found resource core-site.xml at file:/opt/hadoop-3.2.1/etc/hadoop/core-site.xml\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,123 INFO conf.Configuration"]
[43.326419, "o", ": resource-types.xml not found\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,123 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m Jan 19, 2021 3:14:18 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,178 INFO conf.Configuration: found resource yarn-site.xml at file:/opt/hadoop-3.2.1/etc/hadoop/yarn-site.xml\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,200 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:18,208 INFO handler.ContextHandler: Starte"]
[43.326435, "o", "d o.e.j.w.WebAppContext@2f98635e{/,file:///tmp/jetty-0.0.0.0-8042-node-_-any-8525242653921229362.dir/webapp/,AVAILABLE}{/node}\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:18,219 INFO server.AbstractConnector: Started ServerConnector@117bcfdc{HTTP/1.1,[http/1.1]}{0.0.0.0:8042}\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:18,219 INFO server.Server: Started @4317ms\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:18,219 INFO webapp.WebApps: Web app node started at 8042\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:18,222 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : e14f83ec4f23:38946\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:18,234 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:18,239 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.30.0.4:8031\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,266 INFO security.NMTokenSecretManagerInRM: NMT"]
[43.326446, "o", "okenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,271 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,281 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:18,298 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:18,313 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,334 INFO recovery.RMStateStoreFactory: Using RMStateStore implementation - class org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:1"]
[43.326458, "o", "4:18,336 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,340 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,340 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[43.368279, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,385 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.36898, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,386 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.369668, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,387 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.370462, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,388 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.433066, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,450 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.498405, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,515 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,516 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.511709, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,529 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.51512, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,532 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.522074, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,539 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.523871, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,541 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.524201, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,541 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.52871, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,546 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.53043, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,548 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.536135, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,553 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/opt/hadoop-3.2.1/etc/hadoop/capacity-scheduler.xml\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.550744, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,567 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1024, vCores:1>\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,567 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:8192, vCores:4>\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.584942, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,601 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,601 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.593514, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,610 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m , reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,610 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.611958, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,627 INFO capacity.LeafQueue: Initializing default\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m capacity = 1.0 [= (float) configuredCapacity / 100 ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxCapacity = 1.0 [= configuredMaxCapacity ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m effectiveMinResource=<memory:0, vCores:0>\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  , effectiveMaxResource=<memory:0, vCores:0>\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimit = 100 [= configuredUserLimit ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimitFactor = 1.0 [= configuredUserLimitFactor ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemAp"]
[43.612045, "o", "plications * absoluteCapacity)]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m maximumAllocation = <memory:8192, vCores:4> [= configuredMaxAllocation ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m numContainers = 0 [= currentNumContainers ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m state = RUNNING [= configuredState ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= conf"]
[43.612063, "o", "iguredAcls ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m nodeLocalityDelay = 40\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m rackLocalityAdditionalDelay = -1\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m labels=*,\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m reservationsContinueLooking = true\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m preemptionDisabled = true\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m defaultAppPriorityPerQueue = 0\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m priority = 0\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxLifetime = -1 seconds\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m defaultLifetime = -1 seconds\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,627 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,627 INFO capacity.CapacitySchedu"]
[43.612089, "o", "lerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,629 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,629 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.613161, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,630 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are \r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,630 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAllocation=<<memory:8192, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.615699, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,633 INFO conf.Configuration: dynamic-resources.xml not found\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.618531, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,635 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,635 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,636 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. \r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.765643, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:18,782 INFO impl.TimelineClientImpl: Timeline service address: historyserver:8188\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[43.996663, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,010 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,010 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,010 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[32m\u001b[1mresourcemanag"]
[43.996738, "o", "er    |\u001b(B\u001b[m 2021-01-19 03:14:19,010 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,010 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,010 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,010"]
[43.996752, "o", " INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,010 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,010 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,010 INFO event.AsyncDispatcher: Register"]
[43.996767, "o", "ing class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[44.057373, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,074 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.30.0.5:10200\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[44.139777, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,156 INFO util.log: Logging initialized @2424ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[44.206745, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,224 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[44.210406, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,227 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[44.218572, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,235 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[44.223072, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,238 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,238 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,238 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,238 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,239 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFi"]
[44.223153, "o", "lter$StaticUserFilter) to context logs\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,239 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[44.224362, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,240 INFO http.HttpServer2: adding path spec: /cluster/*\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,241 INFO http.HttpServer2: adding path spec: /ws/*\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,241 INFO http.HttpServer2: adding path spec: /app/*\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[44.355904, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:19,371 INFO ipc.Client: Retrying connect to server: resourcemanager/172.30.0.4:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[44.601146, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,618 INFO webapp.WebApps: Registered webapp guice modules\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[44.605927, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,623 INFO http.HttpServer2: Jetty bound to port 8088\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[44.606879, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,624 INFO server.Server: jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[44.639208, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,656 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[44.646758, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,664 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[44.648572, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,666 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,666 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[44.650694, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,667 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4e76dac{/logs,file:///opt/hadoop-3.2.1/logs/,AVAILABLE}\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:19,668 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5d465e4b{/static,jar:file:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar!/webapps/static,AVAILABLE}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[44.798866, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:19 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[44.799675, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:19 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[44.800135, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:19 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[44.800885, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:19 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[44.836328, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:19 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[45.136176, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:20 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[45.334458, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:20 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[45.354628, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:20,372 INFO ipc.Client: Retrying connect to server: resourcemanager/172.30.0.4:8031. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[45.364571, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:20,382 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@49ec6a9f{/,file:///tmp/jetty-0.0.0.0-8088-cluster-_-any-2679008464646353617.dir/webapp/,AVAILABLE}{/cluster}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[45.372869, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:20,388 INFO server.AbstractConnector: Started ServerConnector@25b52284{HTTP/1.1,[http/1.1]}{0.0.0.0:8088}\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:20,388 INFO server.Server: Started @3657ms\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:20,388 INFO webapp.WebApps: Web app cluster started at 8088\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[45.44067, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:20,454 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[45.444338, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:20,461 INFO ipc.Server: Starting Socket Reader #1 for port 8033\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[45.472397, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:20,489 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[45.473516, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:20,491 INFO ipc.Server: IPC Server Responder: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[45.474536, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:20,492 INFO ipc.Server: IPC Server listener on 8033: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[45.48538, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:20,503 INFO resourcemanager.ResourceManager: Transitioning to active state\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[46.114559, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:21,130 INFO ipc.Server: IPC Server handler 4 on default port 9000, call Call#0 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.30.0.4:45584: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[34mnamenode           |\u001b[39m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 16 seconds. NamenodeHostName:namenode\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[46.137641, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,142 INFO recovery.FileSystemRMStateStore: Exception while executing an FS operation.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 16 seconds. NamenodeHostName:namenode\r\n\u001b[1;46r\u001b[H\u001b[45;113H\u001b[25G\u001b[97C\u001b[33G\u001b[?25l\u001b[H\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:626)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:623)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b("]
[46.137926, "o", "B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$FSAction.runWithRetries(FileSystemRMStateStore.java:739)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.mkdirsWithRetries(FileSystemRMStateStore.java:629)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.startInternal(FileSystemRMStateStore.java:156)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.serviceStart(RMStateStore.java:746)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:868)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.service.AbstractService.start(Abst"]
[46.138008, "o", "ractService.java:194)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1262)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1303)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1299)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.security.AccessController.doPrivileged(Native Method)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at javax.security.auth.Subject.doAs(Subject.java:422)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1299)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.re"]
[46.138067, "o", "sourcemanager.ResourceManager.serviceStart(ResourceManager.java:1350)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1535)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 16 seconds. NamenodeHostName:namenode\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager"]
[46.138186, "o", "    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\u001b[K\r\n\u001b[32m\u001b[1mreso"]
[46.13825, "o", "urcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.security.AccessController.doPrivileged(Native Method)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at javax.security.auth.Subject.doAs(Subject.java:422)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1491)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1388)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org."]
[46.138266, "o", "apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy86.mkdirs(Unknown Source)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop\u001b[K\u001b[?12l\u001b[?25h\u001b[1;45r\u001b[H\u001b[45;45H.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Method.invoke(Method.java:498)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.Retr"]
[46.138281, "o", "yInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat com.sun.proxy.$Proxy87.mkdirs(Unknown Source)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3C... 26 more\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,145 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new "]
[46.138384, "o", "events.\r\n"]
[46.138832, "o", "\u001b[1;46r\u001b[H\u001b[45d"]
[46.150237, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,157 INFO service.AbstractService: Service RMActiveServices failed in state STARTED\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m org.apache.hadoop.service.ServiceStateException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 16 seconds. NamenodeHostName:namenode\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.serv"]
[46.150415, "o", "er.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[32m\u001b[1mre"]
[46.150482, "o", "sourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:203)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:868)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b"]
[46.15054, "o", "[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1262)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1303)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1299)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1299)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1350)\r\n\u001b[32m"]
[46.150613, "o", "\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(Res\u001b[1;46r\u001b[H\u001b[45;98H\u001b[1;45r\u001b[H\u001b[45;98HourceManager.java:1535)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 16 seconds. NamenodeHostName:namenode\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(F"]
[46.150695, "o", "SNamesystem.java:1463)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[32m\u001b[1mresourcema"]
[46.150712, "o", "nager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Constructor.newI"]
[46.150724, "o", "nstance(Constructor.java:423)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2427)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2401)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1318)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1315)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem."]
[46.150736, "o", "java:1332)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1307)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:626)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.res"]
[46.151504, "o", "\u001b[1;46r\u001b[H\u001b[45;61H\u001b[1;45r\u001b[H\u001b[45;61Hourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:623)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$FSAction.runWithRetries(FileSystemRMStateStore.java:739)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.mkdirsWithRetries(FileSystemRMStateStore.java:629)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.startInternal(FileSystemRMStateStore.java:156)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.serviceStart(RMStateStore.java:746)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3C... 12 more\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoo"]
[46.151746, "o", "p.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 16 seconds. NamenodeHostName:namenode\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[32m\u001b[1mresourcemanage"]
[46.151771, "o", "r    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b"]
[46.151783, "o", "(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.call(Client.java:1491)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.call(Client.java:1388)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat com.sun.proxy.$Proxy86.mkdirs(Unknown Source)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolT"]
[46.151795, "o", "ranslatorPB.java:660)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n"]
[46.15205, "o", "\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Method.invoke(Method.java:498)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat com.sun.proxy.$Proxy87.mkdir"]
[46.152071, "o", "s(Unknown Source)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3C... 26 more\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,160 INFO impl.MetricsSystemImpl: Stopping ResourceManager metrics system...\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,160 INFO impl.MetricsSystemImpl: ResourceManager metrics system stopped.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,160 INFO impl.MetricsSystemImpl: ResourceManager metrics system shutdown complete.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,162 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[46.154944, "o", "\u001b[?25l\u001b[H\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1491)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1388)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy86.mkdirs(Unknown Source)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNa"]
[46.15528, "o", "menodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.lang.reflect.Method.invoke(Method.java:498)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.h"]
[46.155482, "o", "adoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy87.mkdirs(Unknown Source)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    ... 26 more\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,160 INFO impl.MetricsSystemImpl: Stopping ResourceManager metrics system...\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,160 INFO impl.MetricsSystemImpl: ResourceManager metrics system stopped.\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,160 INFO impl.MetricsSystemImpl: ResourceManager metrics system shutdown complete.\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,162 INFO event.AsyncDispatcher: Registering class org.apache.hadoop."]
[46.155505, "o", "yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,170 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,171 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,171 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,171 INFO recovery.RMStateStoreFactory: Using RMStateStore implementation - class org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,171 INFO event.AsyncDispatcher: Regis"]
[46.155517, "o", "tering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,172 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,172 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,172 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,172 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.ya"]
[46.155529, "o", "rn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,172 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,172 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[46.156831, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,174 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[46.158062, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,175 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,175 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[46.173625, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,190 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,190 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[46.174348, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,190 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,190 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,190 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,190 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[46.180212, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,194 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/opt/hadoop-3.2.1/etc/hadoop/capacity-scheduler.xml\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[46.192021, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,208 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1024, vCores:1>\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,208 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:8192, vCores:4>\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[46.19691, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,214 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,214 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[46.198535, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,216 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m , reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,216 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[46.216653, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,220 INFO capacity.LeafQueue: Initializing default\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m capacity = 1.0 [= (float) configuredCapacity / 100 ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxCapacity = 1.0 [= configuredMaxCapacity ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m effectiveMinResource=<memory:0, vCores:0>\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m  , effectiveMaxResource=<memory:0, vCores:0>\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimit = 100 [= configuredUserLimit ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimitFactor = 1.0 [= configuredUserLimitFactor ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemAp"]
[46.217083, "o", "plications * absoluteCapacity)]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m maximumAllocation = <memory:8192, vCores:4> [= configuredMaxAllocation ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m numContainers = 0 [= currentNumContainers ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m state = RUNNING [= configuredState ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= conf"]
[46.217314, "o", "iguredAcls ]\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m nodeLocalityDelay = 40\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m rackLocalityAdditionalDelay = -1\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m labels=*,\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m reservationsContinueLooking = true\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m preemptionDisabled = true\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m defaultAppPriorityPerQueue = 0\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m priority = 0\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxLifetime = -1 seconds\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m defaultLifetime = -1 seconds\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,220 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,220 INFO capacity.CapacitySchedu"]
[46.217483, "o", "lerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,221 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,221 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,221 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are \r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,221 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAlloc"]
[46.217655, "o", "ation=<<memory:8192, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b[1;46r\u001b[H\u001b[45;21H\u001b(B\u001b[m\u001b[1;45r\u001b[H\u001b[45;21H 2021-01-19 03:14:21,221 INFO conf.Configuration: dynamic-resources.xml not found\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,222 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,222 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,222 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. \r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,223 INFO service.AbstractService"]
[46.217827, "o", ": Service ResourceManager failed in state STARTED\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m org.apache.hadoop.service.ServiceStateException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 16 seconds. NamenodeHostName:namenode\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org."]
[46.217986, "o", "apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[32m\u001b[1mres"]
[46.218137, "o", "ourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:203)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:868)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager."]
[46.218158, "o", "java:1262)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1303)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1299)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivi\u001b[1;46r\u001b[H\u001b[45;66H\u001b[1;45r\u001b[H\u001b[45;66Hleged(Native Method)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1299)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1350)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.Abst"]
[46.21817, "o", "ractService.start(AbstractService.java:194)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1535)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 16 seconds. NamenodeHostName:namenode\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenod"]
[46.218183, "o", "e.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)"]
[46.219218, "o", "\u001b[1;46r\u001b[H\u001b[45;126H\u001b[1;45r\u001b[H\u001b[45d\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeCon"]
[46.219254, "o", "structorAccessorImpl.java:62)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2427)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2401)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1318)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1315)\r\n"]
[46.219266, "o", "\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1332)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1307)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:626)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:623)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$FSAction.runWithRetries(FileSystemRMStateStore.java:739)\r\n\u001b[32m\u001b[1mresourcemanager"]
[46.219278, "o", "    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.mkdirsWithRetries(FileSystemRMStateStore.java:629)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.startInternal(FileSystemRMStateStore.java:156)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.serviceStart(RMStateStore.java:746)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n"]
[46.219843, "o", "\u001b[1;46r\u001b[H\u001b[45;109H\r"]
[46.220598, "o", "\u001b[?25l\u001b[H\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 16 seconds. NamenodeHostName:namenode\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.name"]
[46.220637, "o", "node.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.security.AccessController.doPrivileged(Native Method)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager "]
[46.220653, "o", "   |\u001b(B\u001b[m    at javax.security.auth.Subject.doAs(Subject.java:422)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1491)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1388)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy86.mkdirs(Unknown Source)\u001b[K\r\n\u001b[32m\u001b[1mresourcemana"]
[46.220736, "o", "ger    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.lang.reflect.Method.invoke(Method.java:498)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:"]
[46.220751, "o", "157)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy87.mkdirs(Unknown Source)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    ... 26 more\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,225 ERROR delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,233 INFO handler.ContextHandler: Stopped o.e.j.w.WebAppContext@49ec6a9f{/,null,UNAVAILABLE}{/cluster}\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,237 INFO server.AbstractConnector: Stopped ServerConnecto"]
[46.220762, "o", "r@25b52284{HTTP/1.1,[http/1.1]}{0.0.0.0:8088}\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,237 INFO handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@5d465e4b{/static,jar:file:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar!/webapps/static,UNAVAILABLE}\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,238 INFO handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@4e76dac{/logs,file:///opt/hadoop-3.2.1/logs/,UNAVAILABLE}\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[46.325096, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,341 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,341 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,341 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,341 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,342 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,342 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,342 INFO event.AsyncDispatcher: AsyncDispatcher"]
[46.325173, "o", " is draining to stop, ignoring any new events.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,342 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,342 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,342 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,343 INFO ipc.Server: Stopping server on 8033\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[46.325914, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,343 INFO resourcemanager.ResourceManager: Transitioning to standby state\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,343 INFO resourcemanager.ResourceManager: Transitioned to standby state\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[46.329377, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,343 FATAL resourcemanager.ResourceManager: Error starting ResourceManager\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m org.apache.hadoop.service.ServiceStateException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 16 seconds. NamenodeHostName:namenode\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[46.332032, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Serv"]
[46.33218, "o", "er$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.servic"]
[46.332289, "o", "e.AbstractService.start(AbstractService.java:203)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:868)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1262)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1303)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1299)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.sec"]
[46.332382, "o", "urity.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1299)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1350)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1535)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode wi"]
[46.332464, "o", "ll be turned off automatically in 16 seconds. NamenodeHostName:namenode\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[32m\u001b[1mresource\u001b[1;46r\u001b[H\u001b[45;9H\u001b(B\u001b[m\u001b[1;45r\u001b[H\u001b[45;9H\u001b[32m\u001b[1mmanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodePr"]
[46.332541, "o", "otocolProtos.java)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInst"]
[46.332557, "o", "ance0(Native Method)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2427)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2401)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1318)\r\n\u001b[32"]
[46.332568, "o", "m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1315)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1332)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1307)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:626)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:623)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.re"]
[46.332581, "o", "sourcemanager.recovery.FileSystemRMStateStore$FSAction.runWithRetries(FileSystemRMStateStore.java:739)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.mkdirsWithRetries(FileSystemRMStateStore.java:629)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.startInternal(FileSystemRMStateStore.java:156)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn."]
[46.3332, "o", "\u001b[1;46r\u001b[H\u001b[45;51H\u001b[1;45r\u001b[H\u001b[45;51Hserver.resourcemanager.recovery.RMStateStore.serviceStart(RMStateStore.java:746)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3C... 12 more\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 16 seconds. NamenodeHostName:namenode\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.name"]
[46.333271, "o", "node.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall."]
[46.333293, "o", "run(Server.java:999)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.call(Client.java:1491)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.call(Client.java:1388)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\r\n\u001b[32m\u001b["]
[46.333303, "o", "1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat com.sun.proxy.$Proxy86.mkdirs(Unknown Source)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Method.invoke(Method.java:498)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop"]
[46.333314, "o", ".io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m "]
[46.33351, "o", "\u001b[1;46r\u001b[H\u001b[45;25H\u001b[1;45r\u001b[H\u001b[45;25Hat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat com.sun.proxy.$Proxy87.mkdirs(Unknown Source)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3C... 26 more\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,344 INFO ipc.Server: Stopping IPC Server listener on 8033\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,344 INFO ipc.Server: Stopping IPC Server Responder\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,347 INFO resourcemanager.ResourceManager: SHUTDOWN_MSG: \r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m /************************************************************\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m SHUTDOWN_MSG: Shutting d"]
[46.333572, "o", "own ResourceManager at a07c851f4b7d/172.30.0.4\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[46.335983, "o", "\u001b[?25l\u001b[H\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 16 seconds. NamenodeHostName:namenode\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.name"]
[46.336087, "o", "node.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.security.AccessController.doPrivileged(Native Method)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager "]
[46.33616, "o", "   |\u001b(B\u001b[m    at javax.security.auth.Subject.doAs(Subject.java:422)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1491)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1388)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy86.mkdirs(Unknown Source)\u001b[K\r\n\u001b[32m\u001b[1mresourcemana"]
[46.336175, "o", "ger    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.lang.reflect.Method.invoke(Method.java:498)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:"]
[46.336262, "o", "157)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy87.mkdirs(Unknown Source)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m    ... 26 more\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,344 INFO ipc.Server: Stopping IPC Server listener on 8033\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,344 INFO ipc.Server: Stopping IPC Server Responder\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:21,347 INFO resourcemanager.ResourceManager: SHUTDOWN_MSG: \u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m /************************************************************\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager "]
[46.336283, "o", "   |\u001b(B\u001b[m SHUTDOWN_MSG: Shutting down ResourceManager at a07c851f4b7d/172.30.0.4\u001b[K\r\n\u001b[32m\u001b[1mresourcemanager    |\u001b(B\u001b[m ************************************************************/\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[46.355845, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:21,373 INFO ipc.Client: Retrying connect to server: resourcemanager/172.30.0.4:8031. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[46.838355, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33m\u001b[1mresourcemanager exited with code 255\u001b(B\u001b[m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.356708, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:22,374 INFO ipc.Client: Retrying connect to server: resourcemanager/172.30.0.4:8031. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.464033, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting fs.defaultFS=hdfs://namenode:9000\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.472067, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting hadoop.http.staticuser.user=root\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.479949, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.487972, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting hadoop.proxyuser.hue.groups=*\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.49239, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring hdfs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.499247, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting dfs.namenode.datanode.registration.ip-hostname-check=false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.507958, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting dfs.webhdfs.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.516116, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting dfs.permissions.enabled=false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.520578, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring yarn\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.527629, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.timeline-service.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.535916, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-vcores=4\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.544135, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.552448, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.560403, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.569295, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.577453, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.585914, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.timeline-service.generic-application-history.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.594376, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.log-aggregation-enable=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.603856, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.hostname=resourcemanager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.617796, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.629136, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.aux-services=mapreduce_shuffle\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.63899, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.resource_tracker.address=resourcemanager:8031\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.648016, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.timeline-service.hostname=historyserver\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.657423, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.scheduler.address=resourcemanager:8030\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.668379, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.address=resourcemanager:8032\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.681121, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapred.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.695115, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.remote-app-log-dir=/app-logs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.703965, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.713355, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.map.output.compress=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.723349, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.resource.memory-mb=16384\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.7343, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.recovery.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.748279, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.resource.cpu-vcores=8\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.752615, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring httpfs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.75597, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring kms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.759907, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring mapred\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.767704, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.map.java.opts=-Xmx3072m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.77631, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.reduce.java.opts=-Xmx6144m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.784795, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.reduce.memory.mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.793119, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.802544, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.map.memory.mb=4096\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.811692, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapred.child.java.opts=-Xmx4096m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.820752, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.82905, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.framework.name=yarn\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.837234, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.842185, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring for multihomed network\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.903604, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] namenode:9000 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.90581, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] namenode:9870 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.90791, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] datanode:9864 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.913206, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] datanode2:9864 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[47.919698, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] datanode3:9864 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[48.577508, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:23,584 INFO resourcemanager.ResourceManager: STARTUP_MSG: \r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m /************************************************************\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG: Starting ResourceManager\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   host = a07c851f4b7d/172.30.0.4\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   args = []\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   version = 3.2.1\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop-3.2.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/l"]
[48.579064, "o", "ib/commons-beanutils-1.9.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-text-1.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/dnsjava-2.1.7.jar"]
[48.579395, "o", ":/opt/hadoop-3.2.1/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1"]
[48.579572, "o", "/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/"]
[48.57979, "o", "hadoop/common/lib/jetty-servlet-9.3.24.v20\u001b[1;46r\u001b[H\u001b[45;39H\u001b[1;45r\u001b[H\u001b[45;39H180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop"]
[48.580009, "o", "/common/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/protobuf-java-"]
[48.580208, "o", "2.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-kms-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoo"]
[48.580397, "o", "p/hdfs/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-text-1.4.jar:/opt/hadoop-3.2.1/share/ha"]
[48.580616, "o", "doop/hdfs/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/l\u001b[1;46r\u001b[H\u001b[45;144H\u001b[1;45r\u001b[H\u001b[45;144Hib/curator-framework-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib"]
[48.580847, "o", "/jackson-core-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hd"]
[48.581056, "o", "fs/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-commo"]
[48.581192, "o", "n-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-a"]
[48.581306, "o", "ll-4.0.52.Final.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop-3.2.1/share/had\u001b[1;46r\u001b[H\u001b[45;39H\u001b[1;45r\u001b[H\u001b[45;39Hoop/hdfs/lib/okhttp-2.7.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-h"]
[48.581409, "o", "dfs-httpfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/mapredu"]
[48.581428, "o", "ce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-j"]
[48.58144, "o", "axrs-base-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-la"]
[48.581455, "o", "uncher-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/o"]
[48.581857, "o", "\u001b[1;46r\u001b[H\u001b[45;144H\u001b[1;45r\u001b[H\u001b[45;144Hpt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-registry-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar"]
[48.581885, "o", ":/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/commons-csv-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/commons-lang-2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-annotations-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-client-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-common-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-protocol-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-3.2.1/share/ha"]
[48.581898, "o", "doop/yarn/timelineservice/lib/jcodings-1.0.13.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/joni-2.1.2.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/metrics-core-2.2.0.jar\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   java = 1.8.0_232\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m ************************************************************/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[48.58222, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:23,599 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[48.889873, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:23,906 INFO conf.Configuration: found resource core-site.xml at file:/opt/hadoop-3.2.1/etc/hadoop/core-site.xml\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[48.941489, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:23,957 INFO conf.Configuration: resource-types.xml not found\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:23,958 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[48.99412, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,011 INFO conf.Configuration: found resource yarn-site.xml at file:/opt/hadoop-3.2.1/etc/hadoop/yarn-site.xml\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.007466, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,024 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.045365, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,061 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.04769, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,065 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.052016, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,069 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.099416, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,116 INFO recovery.RMStateStoreFactory: Using RMStateStore implementation - class org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.10272, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,119 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.106319, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,123 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,123 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.148713, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,166 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.14942, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,167 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.150281, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,168 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.151135, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,169 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.213259, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,230 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.269333, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,286 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,286 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.286194, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,303 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.289612, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,307 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.296157, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,313 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.298104, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,315 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.298349, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,315 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.302706, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,320 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.304632, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,322 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.309919, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,327 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/opt/hadoop-3.2.1/etc/hadoop/capacity-scheduler.xml\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.324628, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,341 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1024, vCores:1>\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,341 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:8192, vCores:4>\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.357914, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,374 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,374 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.358963, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:24,376 INFO ipc.Client: Retrying connect to server: resourcemanager/172.30.0.4:8031. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.36691, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,384 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m , reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,384 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.385126, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,401 INFO capacity.LeafQueue: Initializing default\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m capacity = 1.0 [= (float) configuredCapacity / 100 ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxCapacity = 1.0 [= configuredMaxCapacity ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m effectiveMinResource=<memory:0, vCores:0>\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  , effectiveMaxResource=<memory:0, vCores:0>\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimit = 100 [= configuredUserLimit ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimitFactor = 1.0 [= configuredUserLimitFactor ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemAp"]
[49.385192, "o", "plications * absoluteCapacity)]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m maximumAllocation = <memory:8192, vCores:4> [= configuredMaxAllocation ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m numContainers = 0 [= currentNumContainers ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m state = RUNNING [= configuredState ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= conf"]
[49.385207, "o", "iguredAcls ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m nodeLocalityDelay = 40\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m rackLocalityAdditionalDelay = -1\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m labels=*,\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m reservationsContinueLooking = true\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m preemptionDisabled = true\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m defaultAppPriorityPerQueue = 0\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m priority = 0\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.387643, "o", "\u001b[?25l\u001b[H\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,341 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1024, vCores:1>\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,341 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:8192, vCores:4>\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,374 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,374 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\u001b[K\r\n\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:24,376 INFO ipc.Client: Retrying connect to server: resourcemanager/172.30.0.4:8031. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,384 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0,"]
[49.387918, "o", " absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m , reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,384 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,401 INFO capacity.LeafQueue: Initializing default\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m capacity = 1.0 [= (float) configuredCapacity / 100 ]\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxCapacity = 1.0 [= configuredMaxCapacity ]\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m effectiveMinResource=<memory:0, vCores:0>\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m"]
[49.387959, "o", "  , effectiveMaxResource=<memory:0, vCores:0>\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimit = 100 [= configuredUserLimit ]\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimitFactor = 1.0 [= configuredUserLimitFactor ]\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMem"]
[49.38798, "o", "ory) / maximumAllocationMemory ]\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m maximumAllocation = <memory:8192, vCores:4> [= configuredMaxAllocation ]\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m numContainers = 0 [= currentNumContainers ]\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m state = RUNNING [= configuredState ]\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m nodeLocalityDelay = 40\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m rackLocalityAdditionalDelay = -1\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m labels=*,\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m reservationsContinueLooking = true\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m preemptionDisabled = true\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m defaultAppPriorityPerQueue = 0\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m priority = 0\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxLifetime = -1 seconds\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m defaultLifetime = -1 seconds\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b"]
[49.387998, "o", "(B\u001b[m 2021-01-19 03:14:24,401 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,401 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,403 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[49.388725, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,403 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,404 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are \r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,404 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAllocation=<<memory:8192, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.389716, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,407 INFO conf.Configuration: dynamic-resources.xml not found\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.391654, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,409 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,409 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.392636, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,410 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. \r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.535142, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,552 INFO impl.TimelineClientImpl: Timeline service address: historyserver:8188\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.800384, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,816 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,816 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,816 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[35m\u001b[1mresourcemanag"]
[49.800451, "o", "er    |\u001b(B\u001b[m 2021-01-19 03:14:24,816 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,816 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,816 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,816"]
[49.800464, "o", " INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,816 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,816 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,816 INFO event.AsyncDispatcher: Register"]
[49.800478, "o", "ing class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.860322, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,878 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.30.0.5:10200\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[49.948144, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:24,964 INFO util.log: Logging initialized @1856ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[50.016326, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:25,033 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[50.019925, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:25,037 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[50.027724, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:25,044 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[50.032028, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:25,048 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:25,048 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:25,048 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:25,048 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:25,048 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFi"]
[50.03209, "o", "lter$StaticUserFilter) to context logs\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:25,048 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[50.033762, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:25,050 INFO http.HttpServer2: adding path spec: /cluster/*\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:25,051 INFO http.HttpServer2: adding path spec: /ws/*\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:25,051 INFO http.HttpServer2: adding path spec: /app/*\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[50.372223, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:25,378 INFO ipc.Client: Retrying connect to server: resourcemanager/172.30.0.4:8031. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[50.394672, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:25,405 INFO webapp.WebApps: Registered webapp guice modules\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[50.395654, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:25,412 INFO http.HttpServer2: Jetty bound to port 8088\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[50.397811, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:25,414 INFO server.Server: jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[50.423319, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:25,440 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[50.430975, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:25,448 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[50.43277, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:25,450 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:25,450 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[50.434635, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:25,452 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4e76dac{/logs,file:///opt/hadoop-3.2.1/logs/,AVAILABLE}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[50.435324, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:25,453 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5d465e4b{/static,jar:file:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar!/webapps/static,AVAILABLE}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[50.591752, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:25 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:25 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:25 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:25 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Initiatin"]
[50.591838, "o", "g Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[50.638174, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:25 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[50.979987, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:25 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[51.228491, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:26 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[51.249469, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,267 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@49ec6a9f{/,file:///tmp/jetty-0.0.0.0-8088-cluster-_-any-5185812090697379914.dir/webapp/,AVAILABLE}{/cluster}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[51.255143, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,272 INFO server.AbstractConnector: Started ServerConnector@25b52284{HTTP/1.1,[http/1.1]}{0.0.0.0:8088}\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,272 INFO server.Server: Started @3165ms\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,272 INFO webapp.WebApps: Web app cluster started at 8088\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[51.307269, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,324 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[51.315639, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,333 INFO ipc.Server: Starting Socket Reader #1 for port 8033\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[51.342428, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,360 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[51.343479, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,361 INFO ipc.Server: IPC Server Responder: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[51.344522, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,362 INFO ipc.Server: IPC Server listener on 8033: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[51.347107, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,365 INFO resourcemanager.ResourceManager: Transitioning to active state\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[51.363311, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:26,379 INFO ipc.Client: Retrying connect to server: resourcemanager/172.30.0.4:8031. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[51.924238, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:26,940 INFO ipc.Server: IPC Server handler 0 on default port 9000, call Call#0 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.30.0.4:45628: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[34mnamenode           |\u001b[39m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 10 seconds. NamenodeHostName:namenode\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[51.944247, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,953 INFO recovery.FileSystemRMStateStore: Exception while executing an FS operation.\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 10 seconds. NamenodeHostName:namenode\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.ja"]
[51.945168, "o", "va:3232)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.A"]
[51.946188, "o", "ccessController.doPrivileged(Native Method)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteEx"]
[51.949003, "o", "ception.java:121)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2427)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2401)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1318)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1315)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1332)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1307)\r\n"]
[51.951092, "o", "\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(F\u001b[1;46r\u001b[H\u001b[45;113H\u001b[1;45r\u001b[H\u001b[45;113HileSystemRMStateStore.java:626)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:623)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$FSAction.runWithRetries(FileSystemRMStateStore.java:739)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.mkdirsWithRetries(FileSystemRMStateStore.java:629)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.startInternal(FileSystemRMStateStore.java:156)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b["]
[51.951835, "o", "3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.serviceStart(RMStateStore.java:746)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:868)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1262)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1303)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1299)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivilege"]
[51.952065, "o", "d(Native Method)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1299)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1350)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1535)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is i"]
[51.952247, "o", "n safe mode.\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 10 seconds. NamenodeHostName:namenode\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[1;46r\u001b[H\u001b[45;25H\u001b[97C\u001b[?25l\u001b[H\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    ... 26 more\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,955 INFO recovery.FileSystemRMStateStore: Maxed out FS retries. Giving up!\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,955 INFO service.AbstractService: Service org.a"]
[51.952394, "o", "pache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore failed in state STARTED\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 10 seconds. NamenodeHostName:namenode\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at or"]
[51.95253, "o", "g.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.security.AccessController.doPrivileged(Native Metho"]
[51.952684, "o", "d)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at javax.security.auth.Subject.doAs(Subject.java:422)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\u001b[K\r\n"]
[51.952863, "o", "\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2427)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2401)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1318)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1315)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1332)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1307)\u001b[K\r\n\u001b[3"]
[51.953, "o", "5m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:626)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:623)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$FSAction.runWithRetries(FileSystemRMStateStore.java:739)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.mkdirsWithRetries(FileSystemRMStateStore.java:629)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.startInternal(FileSystemRMStateStore.java:156)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.se"]
[51.953131, "o", "rver.resourcemanager.recovery.RMStateStore.serviceStart(RMStateStore.java:746)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:868)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.a\u001b[K\u001b[?12l\u001b[?25h\u001b[1;45r\u001b[H\u001b[45;33Hpache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1262)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1303)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1299)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController."]
[51.953264, "o", "doPrivileged(Native Method)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1299)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1350)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1535)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Nam"]
[51.953284, "o", "e node is in safe mode.\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 10 seconds. NamenodeHostName:namenode\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.jav"]
[51.953295, "o", "a:720)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.h"]
[51.953308, "o", "adoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.call(Client.java:1491)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.call(Client.java:1388)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat com.sun.proxy.$Proxy86.mkdirs(Unknown Source)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop"]
[51.953628, "o", "\u001b[1;46r\u001b[H\u001b[45;45H\u001b[1;45r\u001b[H\u001b[45;45H.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Method.invoke(Method.java:498)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\r\n\u001b[35m\u001b[1mr"]
[51.953653, "o", "esourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat com.sun.proxy.$Proxy87.mkdirs(Unknown Source)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3C... 26 more\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,956 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[51.961828, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,973 INFO service.AbstractService: Service RMActiveServices failed in state STARTED\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m org.apache.hadoop.service.ServiceStateException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 10 seconds. NamenodeHostName:namenode\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.serv"]
[51.963101, "o", "er.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[35m\u001b[1mre"]
[51.963394, "o", "sourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:203)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:868)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b"]
[51.963424, "o", "[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1262)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1303)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1299)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1299)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1350)\r\n\u001b[35m"]
[51.963524, "o", "\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(Res\u001b[1;46r\u001b[H\u001b[45;98H\u001b[1;45r\u001b[H\u001b[45;98HourceManager.java:1535)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 10 seconds. NamenodeHostName:namenode\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(F"]
[51.963549, "o", "SNamesystem.java:1463)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[35m\u001b[1mresourcema"]
[51.963567, "o", "nager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Constructor.newI"]
[51.963582, "o", "nstance(Constructor.java:423)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2427)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2401)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1318)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1315)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem."]
[51.963605, "o", "java:1332)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1307)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:626)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.res\u001b[1;46r\u001b[H\u001b[45;61H\u001b[1;45r\u001b[H\u001b[45;61Hourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:623)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$FSAction.runWithRetries(FileSystemRMStateStore.java:739)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.mkdirsWithRetries(FileSystemRMStateStore.java:629)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.se"]
[51.963616, "o", "rver.resourcemanager.recovery.FileSystemRMStateStore.startInternal(FileSystemRMStateStore.java:156)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.serviceStart(RMStateStore.java:746)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3C... 12 more\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 10 seconds. NamenodeHostName:namenode\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNam"]
[51.963628, "o", "esystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[51.964659, "o", "\u001b[?25l\u001b[H\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 10 seconds. NamenodeHostName:namenode\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.name"]
[51.964765, "o", "node.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.security.AccessController.doPrivileged(Native Method)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager "]
[51.964786, "o", "   |\u001b(B\u001b[m    at javax.security.auth.Subject.doAs(Subject.java:422)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1491)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1388)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy86.mkdirs(Unknown Source)\u001b[K\r\n\u001b[35m\u001b[1mresourcemana"]
[51.964798, "o", "ger    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.lang.reflect.Method.invoke(Method.java:498)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:"]
[51.964809, "o", "157)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy87.mkdirs(Unknown Source)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    ... 26 more\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,975 INFO impl.MetricsSystemImpl: Stopping ResourceManager metrics system...\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,976 INFO impl.MetricsSystemImpl: ResourceManager metrics system stopped.\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,976 INFO impl.MetricsSystemImpl: ResourceManager metrics system shutdown complete.\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,977 I"]
[51.964821, "o", "NFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,981 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[51.965832, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,981 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,981 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,981 INFO recovery.RMStateStoreFactory: Using RMStateStore implementation - class org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,981 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,982 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn"]
[51.965865, "o", ".server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,982 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,982 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,982 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,982 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventTy"]
[51.965878, "o", "pe for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,982 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[51.967577, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,984 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[51.967847, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,985 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,985 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[51.982495, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:26,999 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,000 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[51.983604, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,000 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,000 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,000 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,000 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,001 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/opt/hadoop-3.2.1/etc/hadoop/capacity-scheduler.xml\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[51.997387, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,013 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1024, vCores:1>\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,013 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:8192, vCores:4>\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[51.999113, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,016 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,016 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[51.999827, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,017 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m , reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,017 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[52.008943, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,022 INFO capacity.LeafQueue: Initializing default\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m capacity = 1.0 [= (float) configuredCapacity / 100 ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxCapacity = 1.0 [= configuredMaxCapacity ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m effectiveMinResource=<memory:0, vCores:0>\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m  , effectiveMaxResource=<memory:0, vCores:0>\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimit = 100 [= configuredUserLimit ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimitFactor = 1.0 [= configuredUserLimitFactor ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemAp"]
[52.009376, "o", "plications * absoluteCapacity)]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m maximumAllocation = <memory:8192, vCores:4> [= configuredMaxAllocation ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m numContainers = 0 [= currentNumContainers ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m state = RUNNING [= configuredState ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= conf"]
[52.009412, "o", "iguredAcls ]\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m nodeLocalityDelay = 40\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m rackLocalityAdditionalDelay = -1\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m labels=*,\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m reservationsContinueLooking = true\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m preemptionDisabled = true\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m defaultAppPriorityPerQueue = 0\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m priority = 0\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxLifetime = -1 seconds\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m defaultLifetime = -1 seconds\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,022 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,022 INFO capacity.CapacitySchedu"]
[52.009424, "o", "lerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,022 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,022 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,022 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are \r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,022 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAlloc"]
[52.009437, "o", "ation=<<memory:8192, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[52.015105, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,023 INFO conf.Configuration: dynamic-resources.xml not found\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,024 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,024 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,024 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. \r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,025 INFO service.AbstractService: Service ResourceManager failed in state STARTED\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m org.apache.hadoop.service.ServiceStateException: org.apa"]
[52.016071, "o", "che.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 10 seconds. NamenodeHostName:namenode\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache"]
[52.016611, "o", ".hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop"]
[52.016867, "o", ".security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:203)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:868)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1262)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java"]
[52.017002, "o", ":1303)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1299)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.secur\u001b[1;46r\u001b[H\u001b[45;38H\u001b[1;45r\u001b[H\u001b[45;38Hity.AccessController.doPrivileged(Native Method)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1299)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1350)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceM"]
[52.017173, "o", "anager.main(ResourceManager.java:1535)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 10 seconds. NamenodeHostName:namenode\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcSer"]
[52.017195, "o", "ver.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.aut"]
[52.017207, "o", "h.Subject.doAs(Subject.java:422)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.unwrap"]
[52.017219, "o", "RemoteException(RemoteException.java:88)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2427)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2401)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1318)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs\u001b[1;46r\u001b[H\u001b[45;50H\u001b[1;45r\u001b[H\u001b[45;50H.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1315)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\u001b[1;46r\u001b[H\u001b[45;16H\r"]
[52.017985, "o", "\u001b[?25l\u001b[H\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.startInternal(FileSystemRMStateStore.java:156)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.serviceStart(RMStateStore.java:746)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    ... 12 more\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 10 seconds. NamenodeHostName:namenode\u001b"]
[52.018261, "o", "[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Serv"]
[52.018424, "o", "er$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.security.AccessController.doPrivileged(Native Method)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at javax.security.auth.Subject.doAs(Subject.java:422)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client."]
[52.018445, "o", "call(Client.java:1491)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1388)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy86.mkdirs(Unknown Source)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u001b[K\r\n\u001b[35m\u001b[1mresourceman"]
[52.018456, "o", "ager    |\u001b(B\u001b[m    at java.lang.reflect.Method.invoke(Method.java:498)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy87.mkdirs(Unknown Source)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B"]
[52.018467, "o", "\u001b[m    ... 26 more\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,027 ERROR delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,028 INFO handler.ContextHandler: Stopped o.e.j.w.WebAppContext@49ec6a9f{/,null,UNAVAILABLE}{/cluster}\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[52.022089, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,038 INFO server.AbstractConnector: Stopped ServerConnector@25b52284{HTTP/1.1,[http/1.1]}{0.0.0.0:8088}\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,038 INFO handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@5d465e4b{/static,jar:file:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar!/webapps/static,UNAVAILABLE}\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,038 INFO handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@4e76dac{/logs,file:///opt/hadoop-3.2.1/logs/,UNAVAILABLE}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[52.124141, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,141 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,141 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[52.124835, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,141 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[52.127788, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,142 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,142 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,142 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,142 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,142 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,142 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,142 INFO event.AsyncDispatcher: AsyncDispatcher"]
[52.127846, "o", " is draining to stop, ignoring any new events.\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,142 INFO ipc.Server: Stopping server on 8033\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,143 INFO resourcemanager.ResourceManager: Transitioning to standby state\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,143 INFO resourcemanager.ResourceManager: Transitioned to standby state\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[52.133168, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,143 FATAL resourcemanager.ResourceManager: Error starting ResourceManager\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m org.apache.hadoop.service.ServiceStateException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 10 seconds. NamenodeHostName:namenode\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.nameno"]
[52.133389, "o", "de.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[35m\u001b[1mresourceman"]
[52.133421, "o", "ager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:203)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:868)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org"]
[52.133439, "o", ".apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1262)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1303)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1299)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1299)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1350)\r\n\u001b[35m\u001b[1mresou"]
[52.133454, "o", "rcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceMana\u001b[1;46r\u001b[H\u001b[45;107H\u001b[1;45r\u001b[H\u001b[45;107Hger.java:1535)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 10 seconds. NamenodeHostName:namenode\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesy"]
[52.133468, "o", "stem.java:1463)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[35m\u001b[1mresourcemanager  "]
[52.133483, "o", "  |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Constructor.newInstance"]
[52.133497, "o", "(Constructor.java:423)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2427)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2401)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1318)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1315)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:13"]
[52.133512, "o", "32)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1307)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:626)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemana\u001b[1;46r\u001b[H\u001b[45;70H\u001b[1;45r\u001b[H\u001b[45;70Hger.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:623)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$FSAction.runWithRetries(FileSystemRMStateStore.java:739)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.mkdirsWithRetries(FileSystemRMStateStore.java:629)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.re"]
[52.133526, "o", "sourcemanager.recovery.FileSystemRMStateStore.startInternal(FileSystemRMStateStore.java:156)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.serviceStart(RMStateStore.java:746)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3C... 12 more\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 10 seconds. NamenodeHostName:namenode\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem"]
[52.133537, "o", ".newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache"]
[52.133547, "o", ".hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.call(Client.java:1491)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.call(Client.java:1388)\r\n\u001b[35m\u001b[1mr"]
[52.133565, "o", "esourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat com.sun.proxy.$Proxy86.mkdirs(Unknown Source)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\u001b[35m\u001b[1mres"]
[52.133878, "o", "\u001b[1;46r\u001b[H\u001b[45;4H\u001b(B\u001b[m\u001b[1;45r\u001b[H\u001b[45;4H\u001b[35m\u001b[1mourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Method.invoke(Method.java:498)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat com.sun.proxy.$Proxy8"]
[52.133904, "o", "7.mkdirs(Unknown Source)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3C... 26 more\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,143 INFO ipc.Server: Stopping IPC Server Responder\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,143 INFO ipc.Server: Stopping IPC Server listener on 8033\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[52.13499, "o", "\u001b[?25l\u001b[H\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 10 seconds. NamenodeHostName:namenode\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.name"]
[52.135236, "o", "node.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.security.AccessController.doPrivileged(Native Method)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager "]
[52.135394, "o", "   |\u001b(B\u001b[m    at javax.security.auth.Subject.doAs(Subject.java:422)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1491)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1388)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy86.mkdirs(Unknown Source)\u001b[K\r\n\u001b[35m\u001b[1mresourcemana"]
[52.135416, "o", "ger    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.lang.reflect.Method.invoke(Method.java:498)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:"]
[52.135428, "o", "157)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy87.mkdirs(Unknown Source)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m    ... 26 more\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,143 INFO ipc.Server: Stopping IPC Server Responder\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,143 INFO ipc.Server: Stopping IPC Server listener on 8033\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:27,147 INFO resourcemanager.ResourceManager: SHUTDOWN_MSG: \u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m /************************************************************\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager "]
[52.13544, "o", "   |\u001b(B\u001b[m SHUTDOWN_MSG: Shutting down ResourceManager at a07c851f4b7d/172.30.0.4\u001b[K\r\n\u001b[35m\u001b[1mresourcemanager    |\u001b(B\u001b[m ************************************************************/\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[52.313396, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:27,329 INFO hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. \r\n\u001b[34mnamenode           |\u001b[39m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[52.364061, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:27,380 INFO ipc.Client: Retrying connect to server: resourcemanager/172.30.0.4:8031. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[52.652344, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32m\u001b[1mresourcemanager exited with code 255\u001b(B\u001b[m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.368676, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:28,382 INFO ipc.Client: Retrying connect to server: resourcemanager/172.30.0.4:8031. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.373303, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting fs.defaultFS=hdfs://namenode:9000\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.381161, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting hadoop.http.staticuser.user=root\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.388952, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.397555, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting hadoop.proxyuser.hue.groups=*\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.402102, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring hdfs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.409304, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting dfs.namenode.datanode.registration.ip-hostname-check=false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.417148, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting dfs.webhdfs.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.42491, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting dfs.permissions.enabled=false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.429548, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring yarn\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.436864, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.timeline-service.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.445472, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-vcores=4\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.454156, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.462627, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.471003, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.479022, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.488158, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.496652, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.timeline-service.generic-application-history.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.504689, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.log-aggregation-enable=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.513681, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.hostname=resourcemanager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.522074, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.530665, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.aux-services=mapreduce_shuffle\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.538958, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.resource_tracker.address=resourcemanager:8031\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.548429, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.timeline-service.hostname=historyserver\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.557129, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.scheduler.address=resourcemanager:8030\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.565815, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.address=resourcemanager:8032\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.574617, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapred.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.584299, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.remote-app-log-dir=/app-logs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.593639, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.60205, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.map.output.compress=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.610091, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.resource.memory-mb=16384\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.618149, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.resourcemanager.recovery.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.626137, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.nodemanager.resource.cpu-vcores=8\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.630747, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring httpfs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.634209, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring kms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.637624, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring mapred\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.644571, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.map.java.opts=-Xmx3072m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.653728, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.reduce.java.opts=-Xmx6144m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.661977, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.reduce.memory.mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.670188, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.678332, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.map.memory.mb=4096\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.686426, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapred.child.java.opts=-Xmx4096m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.694684, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.702686, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.framework.name=yarn\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.711175, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  - Setting mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.715926, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Configuring for multihomed network\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.770268, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] namenode:9000 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.772515, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] namenode:9870 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.775184, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] datanode:9864 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.777299, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] datanode2:9864 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[53.779905, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m [1/100] datanode3:9864 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[54.249819, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:29,261 INFO resourcemanager.ResourceManager: STARTUP_MSG: \r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m /************************************************************\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG: Starting ResourceManager\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   host = a07c851f4b7d/172.30.0.4\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   args = []\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   version = 3.2.1\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop-3.2.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/l"]
[54.250271, "o", "ib/commons-beanutils-1.9.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-text-1.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/dnsjava-2.1.7.jar"]
[54.250493, "o", ":/opt/hadoop-3.2.1/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1"]
[54.2507, "o", "/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/"]
[54.25088, "o", "hadoop/common/lib/jetty-servlet-9.3.24.v20\u001b[1;46r\u001b[H\u001b[45;39H\u001b[1;45r\u001b[H\u001b[45;39H180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop"]
[54.251048, "o", "/common/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/protobuf-java-"]
[54.25107, "o", "2.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-kms-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoo"]
[54.251083, "o", "p/hdfs/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-text-1.4.jar:/opt/hadoop-3.2.1/share/ha"]
[54.251094, "o", "doop/hdfs/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/l"]
[54.251333, "o", "\u001b[1;46r\u001b[H\u001b[45;144H\u001b[1;45r\u001b[H\u001b[45;144Hib/curator-framework-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-"]
[54.251475, "o", "asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/"]
[54.251494, "o", "jetty-security-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/"]
[54.251506, "o", "hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4"]
[54.251517, "o", ".41.1.jar:/opt/hadoop-3.2.1/share/had"]
[54.252655, "o", "\u001b[1;46r\u001b[H\u001b[45;39H\u001b[1;45r\u001b[H\u001b[45;39Hoop/hdfs/lib/okhttp-2.7.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1-tests.jar:/opt/hadoop-3.2"]
[54.252891, "o", ".1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-na"]
[54.252916, "o", "tivetask-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.8.jar:/opt/hadoop-3.2."]
[54.252928, "o", "1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/y"]
[54.252939, "o", "arn/hadoop-yarn-common-3.2.1.jar:/o"]
[54.253623, "o", "\u001b[1;46r\u001b[H\u001b[45;144H\u001b[1;45r\u001b[H\u001b[45;144Hpt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-registry-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar"]
[54.253664, "o", ":/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/commons-csv-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/commons-lang-2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-annotations-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-client-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-common-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-protocol-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-3.2.1/share/ha"]
[54.253677, "o", "doop/yarn/timelineservice/lib/jcodings-1.0.13.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/joni-2.1.2.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/metrics-core-2.2.0.jar\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m STARTUP_MSG:   java = 1.8.0_232\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m ************************************************************/\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:29,270 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[54.378892, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36m\u001b[1mnodemanager        |\u001b(B\u001b[m 2021-01-19 03:14:29,385 INFO ipc.Client: Retrying connect to server: resourcemanager/172.30.0.4:8031. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[54.667363, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:29,679 INFO conf.Configuration: found resource core-site.xml at file:/opt/hadoop-3.2.1/etc/hadoop/core-site.xml\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[54.704714, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:29,722 INFO conf.Configuration: resource-types.xml not found\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:29,722 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[54.749123, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:29,766 INFO conf.Configuration: found resource yarn-site.xml at file:/opt/hadoop-3.2.1/etc/hadoop/yarn-site.xml\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[54.763045, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:29,780 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[54.803072, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:29,820 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[54.806167, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:29,823 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[54.810024, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:29,827 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[54.853629, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:29,871 INFO recovery.RMStateStoreFactory: Using RMStateStore implementation - class org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[54.855021, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:29,872 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[54.859389, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:29,877 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:29,877 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[54.901688, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:29,919 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[54.902547, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:29,920 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[54.903354, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:29,921 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[54.904139, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:29,922 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[54.964979, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:29,982 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.022918, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,039 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,039 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.035192, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,052 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.037957, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,055 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.044089, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,061 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.045993, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,063 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,063 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.050672, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,068 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.052493, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,070 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.058158, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,075 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/opt/hadoop-3.2.1/etc/hadoop/capacity-scheduler.xml\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.072027, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,089 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1024, vCores:1>\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,089 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:8192, vCores:4>\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.11428, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,131 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.114347, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,132 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.123489, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,140 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m , reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,141 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.142918, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,157 INFO capacity.LeafQueue: Initializing default\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m capacity = 1.0 [= (float) configuredCapacity / 100 ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxCapacity = 1.0 [= configuredMaxCapacity ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m effectiveMinResource=<memory:0, vCores:0>\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  , effectiveMaxResource=<memory:0, vCores:0>\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimit = 100 [= configuredUserLimit ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimitFactor = 1.0 [= configuredUserLimitFactor ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemAp"]
[55.142992, "o", "plications * absoluteCapacity)]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m maximumAllocation = <memory:8192, vCores:4> [= configuredMaxAllocation ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m numContainers = 0 [= currentNumContainers ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m state = RUNNING [= configuredState ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= conf"]
[55.143017, "o", "iguredAcls ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m nodeLocalityDelay = 40\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m rackLocalityAdditionalDelay = -1\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m labels=*,\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m reservationsContinueLooking = true\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m preemptionDisabled = true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.146286, "o", "\u001b[?25l\u001b[H\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,131 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,132 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,140 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m , reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,141 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,157 INFO capacity.LeafQueue: Initializing default\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m capacity = 1.0 [= (float) configuredCapacity / 100 ]\u001b[K\r\n\u001b"]
[55.146656, "o", "[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxCapacity = 1.0 [= configuredMaxCapacity ]\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m effectiveMinResource=<memory:0, vCores:0>\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  , effectiveMaxResource=<memory:0, vCores:0>\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimit = 100 [= configuredUserLimit ]\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimitFactor = 1.0 [= configuredUserLimitFactor ]\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\u001b[K\r\n\u001b[34m\u001b[1mresourc"]
[55.146772, "o", "emanager    |\u001b(B\u001b[m usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m maximumAllocation = <memory:8192, vCores:4> [= configuredMaxAllocation ]\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m numContainers = 0 [= currentNumContainers ]\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m state = RUNNING [= configuredState ]\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m nodeLocalityDelay = 40\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m rackLocalityAdditionalDelay = -1\u001b[K\r\n\u001b[34m\u001b[1mres"]
[55.146861, "o", "ourcemanager    |\u001b(B\u001b[m labels=*,\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m reservationsContinueLooking = true\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m preemptionDisabled = true\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m defaultAppPriorityPerQueue = 0\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m priority = 0\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxLifetime = -1 seconds\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m defaultLifetime = -1 seconds\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,158 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,158 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>us"]
[55.146905, "o", "edCapacity=0.0, numApps=0, numContainers=0\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,159 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,160 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,160 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are \u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,161 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAllocation=<<memory:8192, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false"]
[55.146921, "o", "\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,163 INFO conf.Configuration: dynamic-resources.xml not found\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.148125, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,165 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.148988, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,166 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.149529, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,167 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. \r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.295779, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,313 INFO impl.TimelineClientImpl: Timeline service address: historyserver:8188\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.547342, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,563 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,563 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,563 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[34m\u001b[1mresourcemanag"]
[55.54741, "o", "er    |\u001b(B\u001b[m 2021-01-19 03:14:30,563 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,563 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,563 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,563"]
[55.547424, "o", " INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,563 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,563 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,563 INFO event.AsyncDispatcher: Register"]
[55.547437, "o", "ing class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.609829, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,627 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.30.0.5:10200\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.700415, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,717 INFO util.log: Logging initialized @1840ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.768355, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,785 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.772426, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,790 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.78182, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,799 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.786093, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,802 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,802 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,802 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,802 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,802 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFi"]
[55.786157, "o", "lter$StaticUserFilter) to context logs\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,802 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[55.78744, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,804 INFO http.HttpServer2: adding path spec: /cluster/*\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,804 INFO http.HttpServer2: adding path spec: /ws/*\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:30,804 INFO http.HttpServer2: adding path spec: /app/*\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[56.121162, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:31,138 INFO webapp.WebApps: Registered webapp guice modules\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[56.12612, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:31,143 INFO http.HttpServer2: Jetty bound to port 8088\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[56.127069, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:31,145 INFO server.Server: jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[56.157573, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:31,175 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[56.1654, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:31,183 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[56.166767, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:31,184 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[56.167923, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:31,184 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[56.168931, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:31,186 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4e76dac{/logs,file:///opt/hadoop-3.2.1/logs/,AVAILABLE}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[56.169397, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:31,187 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5d465e4b{/static,jar:file:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar!/webapps/static,AVAILABLE}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[56.324655, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:31 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:31 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:31 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[56.326441, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:31 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[56.366474, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:31 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[56.746902, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:31 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[56.97327, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Jan 19, 2021 3:14:31 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[56.995055, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,012 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@49ec6a9f{/,file:///tmp/jetty-0.0.0.0-8088-cluster-_-any-521707735761991452.dir/webapp/,AVAILABLE}{/cluster}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.001467, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,018 INFO server.AbstractConnector: Started ServerConnector@25b52284{HTTP/1.1,[http/1.1]}{0.0.0.0:8088}\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,018 INFO server.Server: Started @3141ms\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,018 INFO webapp.WebApps: Web app cluster started at 8088\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.052031, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,069 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.058707, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,076 INFO ipc.Server: Starting Socket Reader #1 for port 8033\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.084162, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,101 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,102 INFO ipc.Server: IPC Server Responder: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.085959, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,103 INFO ipc.Server: IPC Server listener on 8033: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.092168, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,109 INFO resourcemanager.ResourceManager: Transitioning to active state\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.63969, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:32,656 INFO ipc.Server: IPC Server handler 0 on default port 9000, call Call#0 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.30.0.4:45668: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[34mnamenode           |\u001b[39m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName:namenode\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.66293, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,671 INFO recovery.FileSystemRMStateStore: Exception while executing an FS operation.\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName:namenode\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.jav"]
[57.663862, "o", "a:3232)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.Ac"]
[57.664188, "o", "cessController.doPrivileged(Native Method)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteExc"]
[57.664385, "o", "eption.java:121)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2427)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2401)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1318)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1315)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1332)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1307)\r\n\u001b"]
[57.665206, "o", "[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(Fi\u001b[1;46r\u001b[H\u001b[45;114H\u001b[1;45r\u001b[H\u001b[45;114HleSystemRMStateStore.java:626)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:623)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$FSAction.runWithRetries(FileSystemRMStateStore.java:739)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.mkdirsWithRetries(FileSystemRMStateStore.java:629)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.startInternal(FileSystemRMStateStore.java:156)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3"]
[57.665686, "o", "Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.serviceStart(RMStateStore.java:746)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:868)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1262)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1303)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1299)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged"]
[57.665718, "o", "(Native Method)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1299)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1350)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1535)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in"]
[57.665736, "o", " safe mode.\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName:namenode\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[34m"]
[57.665759, "o", "\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[1;46r\u001b[H\u001b[45;27H\u001b[98C"]
[57.667221, "o", "\u001b[?25l\u001b[H\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    ... 26 more\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,673 INFO recovery.FileSystemRMStateStore: Maxed out FS retries. Giving up!\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,673 INFO service.AbstractService: Service org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore failed in state STARTED\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName:namenode\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\u001b[K\r\n\u001b[34m\u001b[1mresourc"]
[57.667267, "o", "emanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\u001b[K\r\n\u001b[34m"]
[57.667284, "o", "\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.security.AccessController.doPrivileged(Native Method)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at javax.security.auth.Subject.doAs(Subject.java:422)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.DelegatingConstructorAccessorImpl"]
[57.6673, "o", ".newInstance(DelegatingConstructorAccessorImpl.java:45)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2427)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2401)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1318)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1315)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSyst"]
[57.667314, "o", "emLinkResolver.java:81)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1332)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1307)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:626)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:623)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$FSAction.runWithRetries(FileSystemRMStateStore.java:739)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMSta"]
[57.667329, "o", "teStore.mkdirsWithRetries(FileSystemRMStateStore.java:629)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.startInternal(FileSystemRMStateStore.java:156)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.serviceStart(RMStateStore.java:746)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:868)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apac\u001b[K\u001b[?12l\u001b[?25h\u001b[1;45r\u001b[H\u001b[45;36Hhe.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1262)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apac"]
[57.667343, "o", "he.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1303)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1299)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1299)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1350)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache."]
[57.667358, "o", "hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1535)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName:namenode\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[34m\u001b[1mresou"]
[57.667371, "o", "rcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivil"]
[57.667382, "o", "eged(Native Method)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.call(Client.java:1491)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.call(Client.java:1388)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat com.sun.proxy.$Proxy86.mkdirs(Unknown Sourc"]
[57.667393, "o", "e)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdf\u001b[1;46r\u001b[H\u001b[45;49H\u001b[1;45r\u001b[H\u001b[45;49Hs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Method.invoke(Method.java:498)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocation"]
[57.667406, "o", "Handler$Call.invoke(RetryInvocationHandler.java:157)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat com.sun.proxy.$Proxy87.mkdirs(Unknown Source)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3C... 26 more\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,674 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n"]
[57.667442, "o", "\u001b[1;46r\u001b[H\u001b[45d"]
[57.680639, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,688 INFO service.AbstractService: Service RMActiveServices failed in state STARTED\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m org.apache.hadoop.service.ServiceStateException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName:namenode\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.serve"]
[57.681291, "o", "r.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[34m\u001b[1mres"]
[57.681595, "o", "ourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:203)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:868)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b["]
[57.681781, "o", "3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1262)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1303)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1299)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1299)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1350)\r\n\u001b[34m\u001b"]
[57.68193, "o", "[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(Reso\u001b[1;46r\u001b[H\u001b[45;99H\u001b[1;45r\u001b[H\u001b[45;99HurceManager.java:1535)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName:namenode\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSN"]
[57.682067, "o", "amesystem.java:1463)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[34m\u001b[1mresourcemana"]
[57.682207, "o", "ger    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Constructor.newIns"]
[57.682339, "o", "tance(Constructor.java:423)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2427)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2401)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1318)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1315)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.ja"]
[57.68236, "o", "va:1332)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1307)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:626)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resou\u001b[1;46r\u001b[H\u001b[45;63H\u001b[1;45r\u001b[H\u001b[45;63Hrcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:623)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$FSAction.runWithRetries(FileSystemRMStateStore.java:739)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.mkdirsWithRetries(FileSystemRMStateStore.java:629)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.serv"]
[57.682371, "o", "er.resourcemanager.recovery.FileSystemRMStateStore.startInternal(FileSystemRMStateStore.java:156)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.serviceStart(RMStateStore.java:746)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3C... 12 more\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName:namenode\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesy"]
[57.682384, "o", "stem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.683245, "o", "\u001b[?25l\u001b[H\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    ... 12 more\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName:namenode\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanag"]
[57.683292, "o", "er    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.security.AccessController.doP"]
[57.683308, "o", "rivileged(Native Method)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at javax.security.auth.Subject.doAs(Subject.java:422)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1491)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1388)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Pr"]
[57.68332, "o", "oxy86.mkdirs(Unknown Source)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.lang.reflect.Method.invoke(Method.java:498)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvo"]
[57.68333, "o", "cationHandler$Call.invoke(RetryInvocationHandler.java:157)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy87.mkdirs(Unknown Source)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    ... 26 more\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,690 INFO impl.MetricsSystemImpl: Stopping ResourceManager metrics system...\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,690 INFO impl.MetricsSystemImpl: ResourceManager metrics system stopped.\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,691 INFO impl.MetricsSystemImpl: ResourceManager metrics system shutdown complete.\u001b[K\r\n\u001b[34m\u001b["]
[57.683342, "o", "1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,692 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[57.683934, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,701 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.684298, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,702 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.686414, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,702 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,702 INFO recovery.RMStateStoreFactory: Using RMStateStore implementation - class org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,702 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,703 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-"]
[57.686771, "o", "19 03:14:32,703 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,703 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,703 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,703 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,70"]
[57.686888, "o", "3 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.687857, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,705 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.689118, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,706 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,706 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.702731, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,720 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.703277, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,720 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,720 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,720 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,720 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,720 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.703742, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,721 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/opt/hadoop-3.2.1/etc/hadoop/capacity-scheduler.xml\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.715125, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,732 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1024, vCores:1>\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,732 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:8192, vCores:4>\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.720366, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,736 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,736 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.720845, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,736 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m , reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,736 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.736047, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,741 INFO capacity.LeafQueue: Initializing default\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m capacity = 1.0 [= (float) configuredCapacity / 100 ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxCapacity = 1.0 [= configuredMaxCapacity ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m effectiveMinResource=<memory:0, vCores:0>\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m  , effectiveMaxResource=<memory:0, vCores:0>\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimit = 100 [= configuredUserLimit ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m userLimitFactor = 1.0 [= configuredUserLimitFactor ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemAp"]
[57.737451, "o", "plications * absoluteCapacity)]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m maximumAllocation = <memory:8192, vCores:4> [= configuredMaxAllocation ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m numContainers = 0 [= currentNumContainers ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m state = RUNNING [= configuredState ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= conf"]
[57.738101, "o", "iguredAcls ]\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m nodeLocalityDelay = 40\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m rackLocalityAdditionalDelay = -1\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m labels=*,\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m reservationsContinueLooking = true\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m preemptionDisabled = true\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m defaultAppPriorityPerQueue = 0\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m priority = 0\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m maxLifetime = -1 seconds\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m defaultLifetime = -1 seconds\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,742 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,742 INFO capacity.CapacitySchedu"]
[57.738533, "o", "lerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,742 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,742 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,742 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are \r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,742 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAlloc"]
[57.738642, "o", "ation=<<memory:8192, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b[1;46r\u001b[H\u001b[45;21H\u001b(B\u001b[m\u001b[1;45r\u001b[H\u001b[45;21H 2021-01-19 03:14:32,743 INFO conf.Configuration: dynamic-resources.xml not found\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,744 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,744 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,744 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. \r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,745 INFO service.AbstractService"]
[57.738782, "o", ": Service ResourceManager failed in state STARTED\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m org.apache.hadoop.service.ServiceStateException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName:namenode\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.a"]
[57.738834, "o", "pache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[34m\u001b[1mreso"]
[57.738877, "o", "urcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:203)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:868)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.j"]
[57.738933, "o", "ava:1262)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1303)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1299)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivil\u001b[1;46r\u001b[H\u001b[45;67H\u001b[1;45r\u001b[H\u001b[45;67Heged(Native Method)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1299)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1350)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.Abstr"]
[57.739006, "o", "actService.start(AbstractService.java:194)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1535)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName:namenode\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode."]
[57.739022, "o", "FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[34m\u001b[1mresourcemanage"]
[57.739033, "o", "r    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteExce"]
[57.739046, "o", "ption.instantiateException(RemoteException.java:121)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2427)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2401)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1318)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCa"]
[57.739198, "o", "\u001b[1;46r\u001b[H\u001b[45;80H\u001b[1;45r\u001b[H\u001b[45;80Hll(DistributedFileSystem.java:1315)\r\n\u001b[1;46r\u001b[H\u001b[45;45H\r"]
[57.740583, "o", "\u001b[?25l\u001b[H\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName:namenode\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namen"]
[57.740698, "o", "ode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.security.AccessController.doPrivileged(Native Method)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager  "]
[57.740774, "o", "  |\u001b(B\u001b[m    at javax.security.auth.Subject.doAs(Subject.java:422)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1491)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1388)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy86.mkdirs(Unknown Source)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanag"]
[57.740792, "o", "er    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.lang.reflect.Method.invoke(Method.java:498)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:1"]
[57.740803, "o", "57)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy87.mkdirs(Unknown Source)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    ... 26 more\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,746 ERROR delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,748 INFO handler.ContextHandler: Stopped o.e.j.w.WebAppContext@49ec6a9f{/,null,UNAVAILABLE}{/cluster}\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,752 INFO server.AbstractConnector: Stopped ServerConnector"]
[57.740814, "o", "@25b52284{HTTP/1.1,[http/1.1]}{0.0.0.0:8088}\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,752 INFO handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@5d465e4b{/static,jar:file:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar!/webapps/static,UNAVAILABLE}\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,752 INFO handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@4e76dac{/logs,file:///opt/hadoop-3.2.1/logs/,UNAVAILABLE}\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[57.843886, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,861 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,861 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,861 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,861 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,861 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.844526, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,862 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.845205, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,863 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.845415, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,863 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.845611, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,863 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.8459, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,863 INFO event.AsyncDispatcher: AsyncDispatcher is draining to stop, ignoring any new events.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.846288, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,864 INFO ipc.Server: Stopping server on 8033\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.85578, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,864 INFO ipc.Server: Stopping IPC Server listener on 8033\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,864 INFO resourcemanager.ResourceManager: Transitioning to standby state\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,864 INFO resourcemanager.ResourceManager: Transitioned to standby state\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,864 FATAL resourcemanager.ResourceManager: Error starting ResourceManager\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m org.apache.hadoop.service.ServiceStateException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName"]
[57.855861, "o", ":namenode\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$P"]
[57.855892, "o", "rotoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.Ab"]
[57.855913, "o", "stractService.start(AbstractService.java:203)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:868)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1262)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1303)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1299)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.securit"]
[57.856056, "o", "y.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1299)\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1350)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1535)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mo"]
[57.856077, "o", "de extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName:namenode\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[34"]
[57.856093, "o", "m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\u001b["]
[57.856108, "o", "34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2427)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2401)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1318)\r\n\u001b[34m\u001b[1mresourcemanager    "]
[57.856123, "o", "|\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1315)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1332)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(Dist\u001b[1;46r\u001b[H\u001b[45;84H\u001b[1;45r\u001b[H\u001b[45;84HributedFileSystem.java:1307)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:626)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3.run(FileSystemRMStateStore.java:623)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ya"]
[57.856138, "o", "rn.server.resourcemanager.recovery.FileSystemRMStateStore$FSAction.runWithRetries(FileSystemRMStateStore.java:739)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.mkdirsWithRetries(FileSystemRMStateStore.java:629)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.startInternal(FileSystemRMStateStore.java:156)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.serviceStart(RMStateStore.java:746)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3C... 12 more\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\r\n\u001b[34m"]
[57.85615, "o", "\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName:namenode\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\r\n\u001b[34m\u001b[1mresourcemanage"]
[57.85616, "o", "r    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.security.AccessController.doPrivileged(Native Method)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Server$Handler.run(Se"]
[57.856173, "o", "rver.java:2915)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.call(Client.java:1491)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.Client.call(Client.java:1388)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat com.sun.proxy.$Proxy86.mkdirs(Unknown Source)\r\n\u001b[34m\u001b[1mresourcemanage"]
[57.856256, "o", "\u001b[1;46r\u001b[H\u001b[45;15H\u001b(B\u001b[m\u001b[1;45r\u001b[H\u001b[45;15H\u001b[34m\u001b[1mr    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat java.lang.reflect.Method.invoke(Method.java:498)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call"]
[57.856271, "o", ".invoke(RetryInvocationHandler.java:157)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat com.sun.proxy.$Proxy87.mkdirs(Unknown Source)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3Cat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[3C... 26 more\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,865 INFO ipc.Server: Stopping IPC Server Responder\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,868 INFO resourcemanager.ResourceManager: SHUTDOWN_MSG: \r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m /************************************************************\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m SHUTDOWN_MSG: Shutting down ResourceManager at a07c851f4b7d/172"]
[57.856294, "o", ".30.0.4\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[57.85999, "o", "\u001b[?25l\u001b[H\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    ... 12 more\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /rmstate/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m The reported blocks 55 has reached the threshold 0.9990 of total blocks 55. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 4 seconds. NamenodeHostName:namenode\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanag"]
[57.860059, "o", "er    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.security.AccessController.doP"]
[57.860077, "o", "rivileged(Native Method)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at javax.security.auth.Subject.doAs(Subject.java:422)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m \u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1491)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.Client.call(Client.java:1388)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Pr"]
[57.860089, "o", "oxy86.mkdirs(Unknown Source)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at java.lang.reflect.Method.invoke(Method.java:498)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvo"]
[57.8601, "o", "cationHandler$Call.invoke(RetryInvocationHandler.java:157)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at com.sun.proxy.$Proxy87.mkdirs(Unknown Source)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m    ... 26 more\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,865 INFO ipc.Server: Stopping IPC Server Responder\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m 2021-01-19 03:14:32,868 INFO resourcemanager.ResourceManager: SHUTDOWN_MSG: \u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m /************************************************************\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m SHUTDOWN_MSG: Shutting down ResourceManager at a07c85"]
[57.860112, "o", "1f4b7d/172.30.0.4\u001b[K\r\n\u001b[34m\u001b[1mresourcemanager    |\u001b(B\u001b[m ************************************************************/\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[58.368364, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35m\u001b[1mresourcemanager exited with code 255\u001b(B\u001b[m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.698516, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting hadoop.proxyuser.hue.hosts=*\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.700212, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting fs.defaultFS=hdfs://namenode:9000\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.716508, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting hadoop.http.staticuser.user=root\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.728272, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.7412, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting hadoop.proxyuser.hue.groups=*\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.745873, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m Configuring hdfs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.753519, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting dfs.namenode.datanode.registration.ip-hostname-check=false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.762403, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting dfs.webhdfs.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.772348, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting dfs.permissions.enabled=false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.776866, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m Configuring yarn\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.78514, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.timeline-service.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.79435, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-vcores=4\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.802901, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.811178, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.819596, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.828102, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.836465, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.846783, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.timeline-service.generic-application-history.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.857057, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.log-aggregation-enable=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.867938, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.resourcemanager.hostname=resourcemanager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.878737, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.scheduler.capacity.root.default.maximum-allocation-mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.88971, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.nodemanager.aux-services=mapreduce_shuffle\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.898462, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.resourcemanager.resource_tracker.address=resourcemanager:8031\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.907088, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.timeline-service.hostname=historyserver\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.917698, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.resourcemanager.scheduler.address=resourcemanager:8030\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.926004, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.resourcemanager.address=resourcemanager:8032\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.934326, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting mapred.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.94616, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.nodemanager.remote-app-log-dir=/app-logs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.956696, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.965858, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting mapreduce.map.output.compress=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.974927, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.nodemanager.resource.memory-mb=16384\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.983766, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.resourcemanager.recovery.enabled=true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.992667, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.nodemanager.resource.cpu-vcores=8\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[60.998068, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m Configuring httpfs\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[61.0023, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m Configuring kms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[61.006282, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m Configuring mapred\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[61.013933, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting mapreduce.map.java.opts=-Xmx3072m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[61.022815, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting mapreduce.reduce.java.opts=-Xmx6144m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[61.03204, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting mapreduce.reduce.memory.mb=8192\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[61.042504, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[61.052207, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting mapreduce.map.memory.mb=4096\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[61.060829, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting mapred.child.java.opts=-Xmx4096m\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[61.06962, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[61.080536, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting mapreduce.framework.name=yarn\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[61.090976, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m  - Setting mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[61.096832, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m Configuring for multihomed network\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[61.153907, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m [1/100] namenode:9000 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[61.156909, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m [1/100] namenode:9870 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[61.159246, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m [1/100] datanode:9864 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[61.161716, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m [1/100] datanode2:9864 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[61.164023, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m [1/100] datanode3:9864 is available.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[61.653183, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:36,665 INFO resourcemanager.ResourceManager: STARTUP_MSG: \r\n\u001b[36mresourcemanager    |\u001b[39m /************************************************************\r\n\u001b[36mresourcemanager    |\u001b[39m STARTUP_MSG: Starting ResourceManager\r\n\u001b[36mresourcemanager    |\u001b[39m STARTUP_MSG:   host = a07c851f4b7d/172.30.0.4\r\n\u001b[36mresourcemanager    |\u001b[39m STARTUP_MSG:   args = []\r\n\u001b[36mresourcemanager    |\u001b[39m STARTUP_MSG:   version = 3.2.1\r\n\u001b[36mresourcemanager    |\u001b[39m STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop-3.2.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/opt"]
[61.653265, "o", "/hadoop-3.2.1/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/commons-text-1.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/dnsjava-2.1.7.jar:/opt/hadoop-3.2.1/share/hadoop/com"]
[61.653285, "o", "mon/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-ma"]
[61.653302, "o", "pper-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-servlet-9.3"]
[61.653317, "o", ".24.v20180605.jar:/op\u001b[1;46r\u001b[H\u001b[45;53H\u001b[1;45r\u001b[H\u001b[45;53Ht/hadoop-3.2.1/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar"]
[61.653331, "o", ":/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-3.2.1/share/h"]
[61.653343, "o", "adoop/common/lib/re2j-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/common/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-kms-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/common/hadoop-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/audience-annotations-0.5"]
[61.653355, "o", ".0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-text-1.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-client-2.13.0"]
[61.653367, "o", ".jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-fra"]
[61.656243, "o", "\u001b[1;46r\u001b[H\u001b[45;158H\u001b[1;45r\u001b[H\u001b[45;158Hmework-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar"]
[61.656531, "o", ":/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-security"]
[61.656697, "o", "-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/hadoop-3.2.1/s"]
[61.656847, "o", "hare/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/opt"]
[61.656977, "o", "/hadoop-3.2.1/share/hadoop/hdfs/lib/o\u001b[1;46r\u001b[H\u001b[45;53H\u001b[1;45r\u001b[H\u001b[45;53Hkhttp-2.7.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1-te"]
[61.657082, "o", "sts.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/had"]
[61.657186, "o", "oop-mapreduce-client-nativetask-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9"]
[61.657285, "o", ".8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-client-3.2.1.jar:/opt/hado"]
[61.657305, "o", "op-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/opt/hadoop-3.2.\u001b[1;46r\u001b[H\u001b[45;158H\u001b[1;45r\u001b[H\u001b[45;158H1/share/hadoop/yarn/hadoop-yarn-registry-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/opt/hado"]
[61.657317, "o", "op-3.2.1/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-client-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-common-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-3.2.1.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/commons-csv-1.0.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/commons-lang-2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-annotations-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-client-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-common-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/hbase-protocol-1.2.6.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/h"]
[61.657329, "o", "trace-core-3.1.0-incubating.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/jcodings-1.0.13.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/joni-2.1.2.jar:/opt/hadoop-3.2.1/share/hadoop/yarn/timelineservice/lib/metrics-core-2.2.0.jar\r\n\u001b[36mresourcemanager    |\u001b[39m STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z\r\n\u001b[36mresourcemanager    |\u001b[39m STARTUP_MSG:   java = 1.8.0_232\r\n\u001b[36mresourcemanager    |\u001b[39m ************************************************************/\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[61.665303, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:36,676 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[61.958052, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:36,975 INFO conf.Configuration: found resource core-site.xml at file:/opt/hadoop-3.2.1/etc/hadoop/core-site.xml\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.002862, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,019 INFO conf.Configuration: resource-types.xml not found\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,020 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.049708, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,067 INFO conf.Configuration: found resource yarn-site.xml at file:/opt/hadoop-3.2.1/etc/hadoop/yarn-site.xml\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.063625, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,081 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.101858, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,117 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.104139, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,121 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.108464, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,126 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.151802, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,169 INFO recovery.RMStateStoreFactory: Using RMStateStore implementation - class org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.15335, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,171 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.158086, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,175 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.158542, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,175 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.20289, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,220 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.20365, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,221 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.204977, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,222 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.206047, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,224 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.288163, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,305 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.315832, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:37,333 INFO hdfs.StateChange: STATE* Safe mode is OFF\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:37,333 INFO hdfs.StateChange: STATE* Leaving safe mode after 41 secs\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:37,333 INFO hdfs.StateChange: STATE* Network topology has 1 racks and 3 datanodes\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:37,333 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 55 blocks\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.345358, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,362 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,363 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.359302, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,376 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.362355, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,380 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.36862, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,386 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.369983, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,387 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.370213, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,388 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.374247, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,392 INFO placement.MultiNodeSortingManager: Initializing NodeSortingService=MultiNodeSortingManager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.375971, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,393 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.381152, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,398 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/opt/hadoop-3.2.1/etc/hadoop/capacity-scheduler.xml\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.394005, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,411 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1024, vCores:1>\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,411 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:8192, vCores:4>\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.425636, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,442 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,442 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.433179, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,450 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,\r\n\u001b[36mresourcemanager    |\u001b[39m , reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,450 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.452146, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,468 INFO capacity.LeafQueue: Initializing default\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.453119, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m capacity = 1.0 [= (float) configuredCapacity / 100 ]\r\n\u001b[36mresourcemanager    |\u001b[39m absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\r\n\u001b[36mresourcemanager    |\u001b[39m maxCapacity = 1.0 [= configuredMaxCapacity ]\r\n\u001b[36mresourcemanager    |\u001b[39m absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\r\n\u001b[36mresourcemanager    |\u001b[39m effectiveMinResource=<memory:0, vCores:0>\r\n\u001b[36mresourcemanager    |\u001b[39m  , effectiveMaxResource=<memory:0, vCores:0>\r\n\u001b[36mresourcemanager    |\u001b[39m userLimit = 100 [= configuredUserLimit ]\r\n\u001b[36mresourcemanager    |\u001b[39m userLimitFactor = 1.0 [= configuredUserLimitFactor ]\r\n\u001b[36mresourcemanager    |\u001b[39m maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]\r\n\u001b[36mresourcemanager    |\u001b[39m maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLim"]
[62.453172, "o", "itFactor) ]\r\n\u001b[36mresourcemanager    |\u001b[39m usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\r\n\u001b[36mresourcemanager    |\u001b[39m absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\r\n\u001b[36mresourcemanager    |\u001b[39m maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\r\n\u001b[36mresourcemanager    |\u001b[39m minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\r\n\u001b[36mresourcemanager    |\u001b[39m maximumAllocation = <memory:8192, vCores:4> [= configuredMaxAllocation ]\r\n\u001b[36mresourcemanager    |\u001b[39m numContainers = 0 [= currentNumContainers ]\r\n\u001b[36mresourcemanager    |\u001b[39m state = RUNNING [= configuredState ]\r\n\u001b[36mresourcemanager    |\u001b[39m acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]\r\n\u001b[36mresourcemanager    |\u001b[39m nodeLocalityDelay = 40\r\n\u001b[36mresourcemanager    |\u001b[39m rackLocalityAdditionalDelay = -1\r\n\u001b[36mresourcemanager    |\u001b[39m labels=*,\r\n\u001b[36mresourcemanager "]
[62.453188, "o", "   |\u001b[39m reservationsContinueLooking = true\r\n\u001b[36mresourcemanager    |\u001b[39m preemptionDisabled = true\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.455477, "o", "\u001b[?25l\u001b[H\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,442 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,442 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,450 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=SUBMIT_APP:*ADMINISTER_QUEUE:*, labels=*,\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m , reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,450 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,468 INFO capacity.LeafQueue: Initializing default\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m capacity = 1.0 [= (float) configuredCapacity / 100 ]\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m absol"]
[62.455761, "o", "uteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m maxCapacity = 1.0 [= configuredMaxCapacity ]\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m effectiveMinResource=<memory:0, vCores:0>\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m  , effectiveMaxResource=<memory:0, vCores:0>\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m userLimit = 100 [= configuredUserLimit ]\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m userLimitFactor = 1.0 [= configuredUserLimitFactor ]\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemor"]
[62.455807, "o", "y * absoluteCapacity)]\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m minimumAllocationFactor = 0.875 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m maximumAllocation = <memory:8192, vCores:4> [= configuredMaxAllocation ]\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m numContainers = 0 [= currentNumContainers ]\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m state = RUNNING [= configuredState ]\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m acls = SUBMIT_APP:*ADMINISTER_QUEUE:* [= configuredAcls ]\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m nodeLocalityDelay = 40\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m rackLocalityAdditionalDelay = -1\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m labels=*,\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m reservationsContinueLooking = true\u001b[K\r\n\u001b[36mresourcemanager    |\u001b["]
[62.45582, "o", "39m preemptionDisabled = true\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m defaultAppPriorityPerQueue = 0\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m priority = 0\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m maxLifetime = -1 seconds\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m defaultLifetime = -1 seconds\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,468 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0, effectiveMinResource=<memory:0, vCores:0> , effectiveMaxResource=<memory:0, vCores:0>\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,468 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,470 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root:"]
[62.455832, "o", " numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,470 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,471 INFO placement.MultiNodeSortingManager: MultiNode scheduling is 'false', and configured policies are \u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,471 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1024, vCores:1>>, maximumAllocation=<<memory:8192, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms,multiNodePlacementEnabled=false\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[62.45684, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,474 INFO conf.Configuration: dynamic-resources.xml not found\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.459325, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,476 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,476 INFO resourcemanager.ApplicationMasterService: disabled placement handler will be used, all scheduling requests will be rejected.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.46007, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,477 INFO resourcemanager.AMSProcessingChain: Adding [org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor] tp top of AMS Processing chain. \r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.605313, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,623 INFO impl.TimelineClientImpl: Timeline service address: historyserver:8188\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.861673, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,876 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,877 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,877 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-0"]
[62.861744, "o", "1-19 03:14:37,877 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,877 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,877 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,877 INFO event.AsyncDispatcher: Regist"]
[62.861757, "o", "ering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,877 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,877 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,877 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcema"]
[62.86177, "o", "nager.metrics.AbstractSystemMetricsPublisher$SystemMetricsEventType for class org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$TimelineV1EventHandler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[62.927288, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:37,942 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.30.0.5:10200\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[63.014447, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,031 INFO util.log: Logging initialized @1765ms\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[63.08475, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,102 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[63.088248, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,105 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[63.096743, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,113 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[63.101103, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,117 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,117 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,117 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,118 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,118 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to"]
[63.101172, "o", " context logs\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,118 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[63.102794, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,120 INFO http.HttpServer2: adding path spec: /cluster/*\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,120 INFO http.HttpServer2: adding path spec: /ws/*\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,120 INFO http.HttpServer2: adding path spec: /app/*\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[63.486701, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,504 INFO webapp.WebApps: Registered webapp guice modules\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[63.492233, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,509 INFO http.HttpServer2: Jetty bound to port 8088\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[63.493325, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,511 INFO server.Server: jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[63.525018, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,542 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[63.532817, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,550 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[63.534631, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,552 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[63.53494, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,552 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[63.536289, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,554 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4e76dac{/logs,file:///opt/hadoop-3.2.1/logs/,AVAILABLE}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[63.536954, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:38,554 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5d465e4b{/static,jar:file:/opt/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar!/webapps/static,AVAILABLE}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[63.713079, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m Jan 19, 2021 3:14:38 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[36mresourcemanager    |\u001b[39m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class\r\n\u001b[36mresourcemanager    |\u001b[39m Jan 19, 2021 3:14:38 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[36mresourcemanager    |\u001b[39m INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[63.714447, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m Jan 19, 2021 3:14:38 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\r\n\u001b[36mresourcemanager    |\u001b[39m INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[63.71542, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m Jan 19, 2021 3:14:38 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\r\n\u001b[36mresourcemanager    |\u001b[39m INFO: Initiating Jersey application, version 'Jersey: 1.19 02/11/2015 03:25 AM'\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[63.760612, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m Jan 19, 2021 3:14:38 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[36mresourcemanager    |\u001b[39m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[64.135965, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m Jan 19, 2021 3:14:39 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[36mresourcemanager    |\u001b[39m INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[64.374134, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m Jan 19, 2021 3:14:39 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\r\n\u001b[36mresourcemanager    |\u001b[39m INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[64.400542, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:39,414 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@49ec6a9f{/,file:///tmp/jetty-0.0.0.0-8088-cluster-_-any-1165602811559170898.dir/webapp/,AVAILABLE}{/cluster}\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[64.404543, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:39,420 INFO server.AbstractConnector: Started ServerConnector@25b52284{HTTP/1.1,[http/1.1]}{0.0.0.0:8088}\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:39,420 INFO server.Server: Started @3154ms\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:39,421 INFO webapp.WebApps: Web app cluster started at 8088\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[64.454455, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:39,471 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[64.463281, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:39,480 INFO ipc.Server: Starting Socket Reader #1 for port 8033\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[64.48851, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:39,506 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[64.489539, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:39,507 INFO ipc.Server: IPC Server Responder: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[64.491458, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:39,509 INFO ipc.Server: IPC Server listener on 8033: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[64.493327, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:39,511 INFO resourcemanager.ResourceManager: Transitioning to active state\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[65.077386, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:40,092 INFO resourcemanager.ResourceManager: Recovery started\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[65.274654, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:40,285 INFO datanode.DataNode: DatanodeRegistration(172.30.0.8:9866, datanodeUuid=1ce6f158-0b39-4227-9455-ea5d127087b3, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129) Starting thread to transfer BP-899898175-172.19.0.5-1610935156129:blk_1073741953_1129 to 172.30.0.3:9866 172.30.0.7:9866 \r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:40,289 INFO datanode.DataNode: DatanodeRegistration(172.30.0.8:9866, datanodeUuid=1ce6f158-0b39-4227-9455-ea5d127087b3, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129) Starting thread to transfer BP-899898175-172.19.0.5-1610935156129:blk_1073741825_1001 to 172.30.0.3:9866 172.30.0.7:9866 \r\n\u001b[1;46r\u001b[H\u001b[45d"]
[65.287756, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:40,293 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:40,296 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[65.306668, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:40,300 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[65.471396, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:40,488 INFO datanode.DataNode: DataTransfer, at cdbb15139e9e:9866: Transmitted BP-899898175-172.19.0.5-1610935156129:blk_1073741825_1001 (numBytes=4) to deb6331226b2/172.30.0.3:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[65.477284, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:40,494 INFO recovery.RMStateStore: Loaded RM state version info 1.3\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[65.506964, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:40,524 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741825_1001 src: /172.30.0.8:55137 dest: /172.30.0.3:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[65.507655, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:40,524 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741953_1129 src: /172.30.0.8:55136 dest: /172.30.0.3:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[65.523605, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:40,541 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[65.523781, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:40,541 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[65.555403, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:40,573 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741825_1001 src: /172.30.0.8:55137 dest: /172.30.0.3:9866 of size 4\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[65.601409, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:40,618 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741825_1001 src: /172.30.0.3:34811 dest: /172.30.0.7:9866\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:40,618 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741953_1129 src: /172.30.0.3:34810 dest: /172.30.0.7:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[65.631222, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:40,648 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741825_1001 src: /172.30.0.3:34811 dest: /172.30.0.7:9866 of size 4\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[65.896662, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:40,912 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741953_1129 src: /172.30.0.8:55136 dest: /172.30.0.3:9866 of size 54983168\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:40,913 INFO datanode.DataNode: DataTransfer, at cdbb15139e9e:9866: Transmitted BP-899898175-172.19.0.5-1610935156129:blk_1073741953_1129 (numBytes=54983168) to deb6331226b2/172.30.0.3:9866\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:40,912 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741953_1129 src: /172.30.0.3:34810 dest: /172.30.0.7:9866 of size 54983168\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[67.023096, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:42,040 INFO hdfs.StateChange: BLOCK* allocate blk_1073741954_1130, replicas=172.30.0.3:9866, 172.30.0.7:9866, 172.30.0.8:9866 for /rmstate/FSRMStateRoot/EpochNode.new.tmp\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[67.030645, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:42,048 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[67.046466, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:42,061 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741954_1130 src: /172.30.0.4:60422 dest: /172.30.0.3:9866\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:42,063 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[67.047422, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:42,065 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741954_1130 src: /172.30.0.3:34818 dest: /172.30.0.7:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[67.050647, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:42,068 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[67.130477, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:42,148 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741954_1130 src: /172.30.0.7:57762 dest: /172.30.0.8:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[67.18295, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:42,200 INFO DataNode.clienttrace: src: /172.30.0.7:57762, dest: /172.30.0.8:9866, bytes: 2, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1464421361_1, offset: 0, srvID: 1ce6f158-0b39-4227-9455-ea5d127087b3, blockid: BP-899898175-172.19.0.5-1610935156129:blk_1073741954_1130, duration(ns): 34395458\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:42,200 INFO datanode.DataNode: PacketResponder: BP-899898175-172.19.0.5-1610935156129:blk_1073741954_1130, type=LAST_IN_PIPELINE terminating\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[67.186146, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:42,203 INFO DataNode.clienttrace: src: /172.30.0.3:34818, dest: /172.30.0.7:9866, bytes: 2, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1464421361_1, offset: 0, srvID: acad48b8-4144-4718-a0d4-4e0e32ff0a1a, blockid: BP-899898175-172.19.0.5-1610935156129:blk_1073741954_1130, duration(ns): 32097376\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:42,203 INFO datanode.DataNode: PacketResponder: BP-899898175-172.19.0.5-1610935156129:blk_1073741954_1130, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.30.0.8:9866] terminating\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[67.189281, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:42,206 INFO DataNode.clienttrace: src: /172.30.0.4:60422, dest: /172.30.0.3:9866, bytes: 2, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1464421361_1, offset: 0, srvID: 760eb334-9533-4c3a-b5b5-46456cd6f837, blockid: BP-899898175-172.19.0.5-1610935156129:blk_1073741954_1130, duration(ns): 29178396\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:42,206 INFO datanode.DataNode: PacketResponder: BP-899898175-172.19.0.5-1610935156129:blk_1073741954_1130, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[172.30.0.7:9866, 172.30.0.8:9866] terminating\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[67.200202, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:42,217 INFO hdfs.StateChange: DIR* completeFile: /rmstate/FSRMStateRoot/EpochNode.new.tmp is closed by DFSClient_NONMAPREDUCE_-1464421361_1\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[68.2433, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:43,260 INFO datanode.DataNode: DatanodeRegistration(172.30.0.8:9866, datanodeUuid=1ce6f158-0b39-4227-9455-ea5d127087b3, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129) Starting thread to transfer BP-899898175-172.19.0.5-1610935156129:blk_1073741827_1003 to 172.30.0.3:9866 172.30.0.7:9866 \r\n\u001b[1;46r\u001b[H\u001b[45d"]
[68.245211, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:43,260 INFO datanode.DataNode: DatanodeRegistration(172.30.0.8:9866, datanodeUuid=1ce6f158-0b39-4227-9455-ea5d127087b3, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129) Starting thread to transfer BP-899898175-172.19.0.5-1610935156129:blk_1073741828_1004 to 172.30.0.3:9866 172.30.0.7:9866 \r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:43,261 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:43,262 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[68.245963, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:43,263 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741827_1003 src: /172.30.0.8:55154 dest: /172.30.0.3:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[68.246201, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:43,263 INFO datanode.DataNode: DataTransfer, at cdbb15139e9e:9866: Transmitted BP-899898175-172.19.0.5-1610935156129:blk_1073741828_1004 (numBytes=17) to deb6331226b2/172.30.0.3:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[68.246578, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:43,264 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741828_1004 src: /172.30.0.8:55156 dest: /172.30.0.3:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[68.248331, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:43,265 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:43,265 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[68.249135, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:43,266 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741828_1004 src: /172.30.0.8:55156 dest: /172.30.0.3:9866 of size 17\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[68.249849, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:43,267 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741827_1003 src: /172.30.0.3:34829 dest: /172.30.0.7:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[68.252474, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:43,269 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741828_1004 src: /172.30.0.3:34828 dest: /172.30.0.7:9866\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:43,270 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741828_1004 src: /172.30.0.3:34828 dest: /172.30.0.7:9866 of size 17\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[68.323603, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:43,341 INFO datanode.DataNode: DataTransfer, at cdbb15139e9e:9866: Transmitted BP-899898175-172.19.0.5-1610935156129:blk_1073741827_1003 (numBytes=23) to deb6331226b2/172.30.0.3:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[68.324666, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:43,342 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741827_1003 src: /172.30.0.3:34829 dest: /172.30.0.7:9866 of size 23\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[68.325636, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:43,342 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741827_1003 src: /172.30.0.8:55154 dest: /172.30.0.3:9866 of size 23\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[70.818344, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:45,835 INFO recovery.FileSystemRMStateStore: Done loading applications from FS state store\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[70.827464, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:45,843 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[70.858372, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:45,875 INFO security.RMDelegationTokenSecretManager: recovering RMDelegationTokenSecretManager.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[70.86138, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:45,879 INFO resourcemanager.RMAppManager: Recovering 5 applications\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[70.944157, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:45,961 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[70.979905, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:45,995 INFO resourcemanager.RMAuditLogger: USER=root\u001b[6COPERATION=Application Finished - Succeeded\u001b[6CTARGET=RMAppManager\u001b[5CRESULT=SUCCESS\u001b[2CAPPID=application_1610935168307_0001\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[70.980349, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:45,997 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[70.984786, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,001 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[70.985862, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,003 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 2 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[70.987063, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,004 INFO resourcemanager.RMAppManager$ApplicationSummary: appId=application_1610935168307_0001,name=word count,user=root,queue=default,state=FINISHED,trackingUrl=http://resourcemanager:8088/proxy/application_1610935168307_0001/,appMasterHost=N/A,submitTime=1610936109782,startTime=1610936109846,launchTime=1610936114414,finishTime=1610936133715,finalStatus=SUCCEEDED,memorySeconds=108733,vcoreSeconds=30,preemptedMemorySeconds=108733,preemptedVcoreSeconds=30,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=108733 MB-seconds\\, 30 vcore-seconds,preemptedResourceSeconds=108733 MB-seconds\\, 30 vcore-seconds,applicationTags=\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[70.99103, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,006 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 3 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,007 INFO resourcemanager.RMAppManager: Successfully recovered 5 out of 5 applications\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,008 INFO resourcemanager.ResourceManager: Recovery ended\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,008 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[70.994205, "o", "\u001b[?25l\u001b[H60eb334-9533-4c3a-b5b5-46456cd6f837, blockid: BP-899898175-172.19.0.5-1610935156129:blk_1073741954_1130, duration(ns): 29178396\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:42,206 INFO datanode.DataNode: PacketResponder: BP-899898175-172.19.0.5-1610935156129:blk_1073741954_1130, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[172.30.0.7:9866, 172.30.0.8:9866] terminating\u001b[K\r\n\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:42,217 INFO hdfs.StateChange: DIR* completeFile: /rmstate/FSRMStateRoot/EpochNode.new.tmp is closed by DFSClient_NONMAPREDUCE_-1464421361_1\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:43,260 INFO datanode.DataNode: DatanodeRegistration(172.30.0.8:9866, datanodeUuid=1ce6f158-0b39-4227-9455-ea5d127087b3, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129) Starting thread to transfer BP-899898175-172.19.0.5-1610935156129:blk_1073741827_1003 to 172.30.0.3:9866 172.30.0.7:9866 \u001b[K\r\n\u001b"]
[70.99463, "o", "[36mdatanode           |\u001b[39m 2021-01-19 03:14:43,260 INFO datanode.DataNode: DatanodeRegistration(172.30.0.8:9866, datanodeUuid=1ce6f158-0b39-4227-9455-ea5d127087b3, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129) Starting thread to transfer BP-899898175-172.19.0.5-1610935156129:blk_1073741828_1004 to 172.30.0.3:9866 172.30.0.7:9866 \u001b[K\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:43,261 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:43,262 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:43,263 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741827_1003 src: /172.30.0.8:55154 dest: /172.30.0.3:9866\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 0"]
[70.994863, "o", "3:14:43,263 INFO datanode.DataNode: DataTransfer, at cdbb15139e9e:9866: Transmitted BP-899898175-172.19.0.5-1610935156129:blk_1073741828_1004 (numBytes=17) to deb6331226b2/172.30.0.3:9866\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:43,264 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741828_1004 src: /172.30.0.8:55156 dest: /172.30.0.3:9866\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:43,265 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:43,265 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:43,266 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741828_1004 src: /172.30.0.8:55156 dest: /172.30.0.3:9866 of size 17\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:43,267 INFO datanode.DataNode: R"]
[70.995036, "o", "eceiving BP-899898175-172.19.0.5-1610935156129:blk_1073741827_1003 src: /172.30.0.3:34829 dest: /172.30.0.7:9866\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:43,269 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741828_1004 src: /172.30.0.3:34828 dest: /172.30.0.7:9866\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:43,270 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741828_1004 src: /172.30.0.3:34828 dest: /172.30.0.7:9866 of size 17\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:43,341 INFO datanode.DataNode: DataTransfer, at cdbb15139e9e:9866: Transmitted BP-899898175-172.19.0.5-1610935156129:blk_1073741827_1003 (numBytes=23) to deb6331226b2/172.30.0.3:9866\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:43,342 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741827_1003 src: /172.30.0.3:34829 dest: /172.30.0.7:9866 of size 23\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:43,342 INFO d"]
[70.995199, "o", "atanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741827_1003 src: /172.30.0.8:55154 dest: /172.30.0.3:9866 of size 23\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:45,835 INFO recovery.FileSystemRMStateStore: Done loading applications from FS state store\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:45,843 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:45,875 INFO security.RMDelegationTokenSecretManager: recovering RMDelegationTokenSecretManager.\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:45,879 INFO resourcemanager.RMAppManager: Recovering 5 applications\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:45,961 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:45,995 INFO resourcemanager."]
[70.995363, "o", "RMAuditLogger: USER=root      OPERATION=Application Finished - Succeeded      TARGET=RMAppManager     RESULT=SUCCESS  APPID=application_1610935168307_0001\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:45,997 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,001 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,003 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 2 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,004 INFO resourcemanager.RMAppManager$ApplicationSummary: appId=application_1610935168307_0001,name=word count,user=root,queue=default,state=FINISHED,trackingUrl=h"]
[70.995386, "o", "ttp://resourcemanager:8088/proxy/application_1610935168307_0001/,appMasterHost=N/A,submitTime=1610936109782,startTime=1610936109846,launchTime=1610936114414,finishTime=1610936133715,finalStatus=SUCCEEDED,memorySeconds=108733,vcoreSeconds=30,preemptedMemorySeconds=108733,preemptedVcoreSeconds=30,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=108733 MB-seconds\\, 30 vcore-seconds,preemptedResourceSeconds=108733 MB-seconds\\, 30 vcore-seconds,applicationTags=\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,006 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 3 is invalid, because it is out of the range [1, 2]. Use the global max attempts instead.\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,007 INFO resourcemanager.RMAppManager: Successfully recovered 5 out of 5 applications\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,008 INFO resourcemanager.ResourceManager: Recovery ended\u001b[K\r\n\u001b[36"]
[70.995399, "o", "mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,008 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,010 INFO resourcemanager.RMAuditLogger: USER=root\u001b[6COPERATION=Application Finished - Succeeded\u001b[6CTARGET=RMAppManager\u001b[5CRESULT=SUCCESS\u001b[2CAPPID=application_1610937837268_0001\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,011 INFO resourcemanager.RMAppManager$ApplicationSummary: appId=application_1610937837268_0001,name=word count,user=root,queue=default,state=FINISHED,trackingUrl=http://resourcemanager:8088/proxy/application_1610937837268_0001/,appMasterHost=N/A,submitTime=1610937923050,startTime=1610937923051,launchTime=1610937924767,finishTime=1610937941778,finalStatus=SUCCEEDED,memorySeconds=73582,vcoreSeconds=22,preemptedMemorySeconds=73582,preemptedVcoreSeconds=22,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\\, vCores:0>,applicationType=MAPREDUCE"]
[70.995415, "o", ",resourceSeconds=73582 MB-seconds\\, 22 vcore-seconds,preemptedResourceSeconds=73582 MB-seconds\\, 22 vcore-seconds,applicationTags=\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,011 INFO resourcemanager.RMAuditLogger: USER=root\u001b[6COPERATION=Application Finished - Succeeded\u001b[6CTARGET=RMAppManager\u001b[5CRESULT=SUCCESS\u001b[2CAPPID=application_1610938999084_0001\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[70.997104, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,014 INFO resourcemanager.RMAppManager$ApplicationSummary: appId=application_1610938999084_0001,name=word count,user=root,queue=default,state=FINISHED,trackingUrl=http://resourcemanager:8088/proxy/application_1610938999084_0001/,appMasterHost=N/A,submitTime=1610950371035,startTime=1610950371036,launchTime=1610950372355,finishTime=1610950388179,finalStatus=SUCCEEDED,memorySeconds=75088,vcoreSeconds=22,preemptedMemorySeconds=75088,preemptedVcoreSeconds=22,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=75088 MB-seconds\\, 22 vcore-seconds,preemptedResourceSeconds=75088 MB-seconds\\, 22 vcore-seconds,applicationTags=\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.00004, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,010 INFO security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,015 INFO resourcemanager.RMAuditLogger: USER=root\u001b[6COPERATION=Application Finished - Succeeded\u001b[6CTARGET=RMAppManager\u001b[5CRESULT=SUCCESS\u001b[2CAPPID=application_1610938999084_0002\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.00247, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,017 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,018 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 13\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,018 INFO resourcemanager.RMAppManager$ApplicationSummary: appId=application_1610938999084_0002,name=word count,user=root,queue=default,state=FINISHED,trackingUrl=http://resourcemanager:8088/proxy/application_1610938999084_0002/,appMasterHost=N/A,submitTime=1610955889140,startTime=1610955889140,launchTime=1610955890308,finishTime=1610955906509,finalStatus=SUCCEEDED,memorySeconds=76771,vcoreSeconds=22,preemptedMemorySeconds=76771,preemptedVcoreSeconds=22,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=76771 MB-seconds\\, 22 vcore-seconds,preemptedResourceSeco"]
[71.002609, "o", "nds=76771 MB-seconds\\, 22 vcore-seconds,applicationTags=\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,019 INFO recovery.RMStateStore: Storing RMDTMasterKey.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.004164, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,019 INFO resourcemanager.RMAuditLogger: USER=root\u001b[6COPERATION=Application Finished - Succeeded\u001b[6CTARGET=RMAppManager\u001b[5CRESULT=SUCCESS\u001b[2CAPPID=application_1610938999084_0003\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,020 INFO resourcemanager.RMAppManager$ApplicationSummary: appId=application_1610938999084_0003,name=word count,user=root,queue=default,state=FINISHED,trackingUrl=http://resourcemanager:8088/proxy/application_1610938999084_0003/,appMasterHost=N/A,submitTime=1610958173191,startTime=1610958173192,launchTime=1610958174210,finishTime=1610958189835,finalStatus=SUCCEEDED,memorySeconds=69313,vcoreSeconds=20,preemptedMemorySeconds=69313,preemptedVcoreSeconds=20,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\\, vCores:0>,applicationType=MAPREDUCE,resourceSeconds=69313 MB-seconds\\, 20 vcore-seconds,preemptedResourceSeconds=69313 MB-seconds\\, 20 vcore-seconds,applicationTags=\r\n\u001b[36mresourcemanager   "]
[71.004217, "o", " |\u001b[39m 2021-01-19 03:14:46,019 INFO recovery.FileSystemRMStateStore: Storing RMDelegationKey_13\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.023109, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:46,033 INFO hdfs.StateChange: BLOCK* allocate blk_1073741955_1131, replicas=172.30.0.8:9866, 172.30.0.3:9866, 172.30.0.7:9866 for /rmstate/FSRMStateRoot/RMDTSecretManagerRoot/DelegationKey_13.tmp\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,037 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.0237, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:46,039 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741955_1131 src: /172.30.0.4:49164 dest: /172.30.0.8:9866\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:46,040 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.024072, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:46,041 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741955_1131 src: /172.30.0.8:55174 dest: /172.30.0.3:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.025896, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:46,042 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.028162, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:46,044 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741955_1131 src: /172.30.0.3:34846 dest: /172.30.0.7:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.044542, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:46,061 INFO DataNode.clienttrace: src: /172.30.0.3:34846, dest: /172.30.0.7:9866, bytes: 17, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1464421361_1, offset: 0, srvID: acad48b8-4144-4718-a0d4-4e0e32ff0a1a, blockid: BP-899898175-172.19.0.5-1610935156129:blk_1073741955_1131, duration(ns): 15161600\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:46,061 INFO datanode.DataNode: PacketResponder: BP-899898175-172.19.0.5-1610935156129:blk_1073741955_1131, type=LAST_IN_PIPELINE terminating\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.048096, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:46,063 INFO DataNode.clienttrace: src: /172.30.0.8:55174, dest: /172.30.0.3:9866, bytes: 17, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1464421361_1, offset: 0, srvID: 760eb334-9533-4c3a-b5b5-46456cd6f837, blockid: BP-899898175-172.19.0.5-1610935156129:blk_1073741955_1131, duration(ns): 13340960\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:46,063 INFO datanode.DataNode: PacketResponder: BP-899898175-172.19.0.5-1610935156129:blk_1073741955_1131, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.30.0.7:9866] terminating\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.053103, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:46,066 INFO DataNode.clienttrace: src: /172.30.0.4:49164, dest: /172.30.0.8:9866, bytes: 17, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1464421361_1, offset: 0, srvID: 1ce6f158-0b39-4227-9455-ea5d127087b3, blockid: BP-899898175-172.19.0.5-1610935156129:blk_1073741955_1131, duration(ns): 16021041\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:46,066 INFO datanode.DataNode: PacketResponder: BP-899898175-172.19.0.5-1610935156129:blk_1073741955_1131, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[172.30.0.3:9866, 172.30.0.7:9866] terminating\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.054088, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:46,071 INFO hdfs.StateChange: DIR* completeFile: /rmstate/FSRMStateRoot/RMDTSecretManagerRoot/DelegationKey_13.tmp is closed by DFSClient_NONMAPREDUCE_-1464421361_1\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.062285, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,079 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,079 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,079 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 14\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,079 INFO recovery.RMStateStore: Storing RMDTMasterKey.\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,079 INFO recovery.FileSystemRMStateStore: Storing RMDelegationKey_14\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.068193, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,084 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.08697, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:46,099 INFO hdfs.StateChange: BLOCK* allocate blk_1073741956_1132, replicas=172.30.0.8:9866, 172.30.0.3:9866, 172.30.0.7:9866 for /rmstate/FSRMStateRoot/RMDTSecretManagerRoot/DelegationKey_14.tmp\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.091265, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,108 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.100301, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:46,111 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741956_1132 src: /172.30.0.4:49170 dest: /172.30.0.8:9866\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:46,111 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:46,113 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741956_1132 src: /172.30.0.8:55180 dest: /172.30.0.3:9866\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:46,114 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.103002, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:46,117 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741956_1132 src: /172.30.0.3:34852 dest: /172.30.0.7:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.113973, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:46,130 INFO DataNode.clienttrace: src: /172.30.0.3:34852, dest: /172.30.0.7:9866, bytes: 17, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1464421361_1, offset: 0, srvID: acad48b8-4144-4718-a0d4-4e0e32ff0a1a, blockid: BP-899898175-172.19.0.5-1610935156129:blk_1073741956_1132, duration(ns): 7820049\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:46,130 INFO datanode.DataNode: PacketResponder: BP-899898175-172.19.0.5-1610935156129:blk_1073741956_1132, type=LAST_IN_PIPELINE terminating\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.119174, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:46,131 INFO DataNode.clienttrace: src: /172.30.0.8:55180, dest: /172.30.0.3:9866, bytes: 17, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1464421361_1, offset: 0, srvID: 760eb334-9533-4c3a-b5b5-46456cd6f837, blockid: BP-899898175-172.19.0.5-1610935156129:blk_1073741956_1132, duration(ns): 11097554\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:46,131 INFO datanode.DataNode: PacketResponder: BP-899898175-172.19.0.5-1610935156129:blk_1073741956_1132, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.30.0.7:9866] terminating\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,134 INFO store.AbstractFSNodeStore: Created store directory :file:/tmp/hadoop-yarn-root/node-attribute\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:46,134 INFO DataNode.clienttrace: src: /172.30.0.4:49170, dest: /172.30.0.8:9866, bytes: 17, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1464421361_1, offset: 0, srvID: 1ce6f158-0b39-4227-9455-ea5d127087b3, blockid: BP-8"]
[71.11924, "o", "99898175-172.19.0.5-1610935156129:blk_1073741956_1132, duration(ns): 13291196\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:46,134 INFO datanode.DataNode: PacketResponder: BP-899898175-172.19.0.5-1610935156129:blk_1073741956_1132, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[172.30.0.3:9866, 172.30.0.7:9866] terminating\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.12371, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[34mnamenode           |\u001b[39m 2021-01-19 03:14:46,140 INFO hdfs.StateChange: DIR* completeFile: /rmstate/FSRMStateRoot/RMDTSecretManagerRoot/DelegationKey_14.tmp is closed by DFSClient_NONMAPREDUCE_-1464421361_1\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.140739, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,152 INFO store.AbstractFSNodeStore: Finished write mirror at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.mirror\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,152 INFO store.AbstractFSNodeStore: Finished create editlog file at:file:/tmp/hadoop-yarn-root/node-attribute/nodeattribute.editlog\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,154 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl$ForwardingEventHandler\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.143062, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,159 INFO placement.MultiNodeSortingManager: Starting NodeSortingService=MultiNodeSortingManager\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.173576, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,189 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.178473, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,192 INFO ipc.Server: Starting Socket Reader #1 for port 8031\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,195 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.179733, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.180205, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,197 INFO ipc.Server: IPC Server Responder: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.183526, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.183707, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,201 INFO ipc.Server: IPC Server listener on 8031: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.188077, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.1898, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.215758, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,233 INFO util.JvmPauseMonitor: Starting JVM pause monitor\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.240155, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,256 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.242494, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:46,259 INFO datanode.DataNode: DatanodeRegistration(172.30.0.8:9866, datanodeUuid=1ce6f158-0b39-4227-9455-ea5d127087b3, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129) Starting thread to transfer BP-899898175-172.19.0.5-1610935156129:blk_1073741842_1018 to 172.30.0.7:9866 172.30.0.3:9866 \r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.244211, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:46,261 INFO datanode.DataNode: DatanodeRegistration(172.30.0.8:9866, datanodeUuid=1ce6f158-0b39-4227-9455-ea5d127087b3, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129) Starting thread to transfer BP-899898175-172.19.0.5-1610935156129:blk_1073741843_1019 to 172.30.0.3:9866 172.30.0.7:9866 \r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.248676, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:46,263 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:46,264 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:46,265 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741876_1052 replica FinalizedReplica, blk_1073741876_1052, FINALIZED\r\n\u001b[36mdatanode           |\u001b[39m   getNumBytes()     = 2\r\n\u001b[36mdatanode           |\u001b[39m   getBytesOnDisk()  = 2\r\n\u001b[36mdatanode           |\u001b[39m   getVisibleLength()= 2\r\n\u001b[36mdatanode           |\u001b[39m   getVolume()       = /hadoop/dfs/data\r\n\u001b[36mdatanode           |\u001b[39m   getBlockURI()     = file:/hadoop/dfs/data/current/BP-899898175-172.19.0.5-1610935156129/current/finalized/subdir0/subdir0/blk_1073741876 for deletion\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.252222, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:46,269 INFO datanode.DataNode: DataTransfer, at cdbb15139e9e:9866: Transmitted BP-899898175-172.19.0.5-1610935156129:blk_1073741842_1018 (numBytes=22788) to 9e22ace955e1/172.30.0.7:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.253789, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:46,267 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741843_1019 src: /172.30.0.8:55184 dest: /172.30.0.3:9866\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:46,268 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.26255, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,273 INFO ipc.Server: Starting Socket Reader #1 for port 8030\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:46,275 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741843_1019 src: /172.30.0.8:55184 dest: /172.30.0.3:9866 of size 226309\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:46,273 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741843_1019 src: /172.30.0.3:34858 dest: /172.30.0.7:9866\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:46,273 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741842_1018 src: /172.30.0.8:57828 dest: /172.30.0.7:9866\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:46,277 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:46,277 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk"]
[71.262652, "o", "_1073741843_1019 src: /172.30.0.3:34858 dest: /172.30.0.7:9866 of size 226309\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:46,272 INFO impl.FsDatasetAsyncDiskService: Deleted BP-899898175-172.19.0.5-1610935156129 blk_1073741876_1052 URI file:/hadoop/dfs/data/current/BP-899898175-172.19.0.5-1610935156129/current/finalized/subdir0/subdir0/blk_1073741876\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:46,276 INFO datanode.DataNode: DataTransfer, at cdbb15139e9e:9866: Transmitted BP-899898175-172.19.0.5-1610935156129:blk_1073741843_1019 (numBytes=226309) to deb6331226b2/172.30.0.3:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.268849, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:46,286 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741842_1018 src: /172.30.0.8:57828 dest: /172.30.0.7:9866 of size 22788\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.272218, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:46,289 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741842_1018 src: /172.30.0.7:38426 dest: /172.30.0.3:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.274797, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:46,290 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741842_1018 src: /172.30.0.7:38426 dest: /172.30.0.3:9866 of size 22788\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.28121, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,298 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,298 INFO ipc.Server: IPC Server Responder: starting\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,299 INFO ipc.Server: IPC Server listener on 8030: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.46193, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,476 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,477 INFO ipc.Server: Starting Socket Reader #1 for port 8032\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.465657, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,480 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.46725, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,484 INFO ipc.Server: IPC Server Responder: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.469161, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,486 INFO ipc.Server: IPC Server listener on 8032: starting\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.492432, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,509 INFO resourcemanager.ResourceManager: Transitioned to active state\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.566335, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.569354, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.570238, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.574389, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.614266, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.621228, "o", "\u001b[?25l\u001b[H\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:46,268 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,273 INFO ipc.Server: Starting Socket Reader #1 for port 8030\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:46,275 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741843_1019 src: /172.30.0.8:55184 dest: /172.30.0.3:9866 of size 226309\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:46,273 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741843_1019 src: /172.30.0.3:34858 dest: /172.30.0.7:9866\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:46,273 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741842_1018 src: /172.30.0.8:57828 dest: /172.30.0.7:9866\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:46,277 INFO sasl.SaslDataTransferClient: SASL encryption trust check: lo"]
[71.621315, "o", "calHostTrusted = false, remoteHostTrusted = false\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:46,277 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741843_1019 src: /172.30.0.3:34858 dest: /172.30.0.7:9866 of size 226309\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:46,272 INFO impl.FsDatasetAsyncDiskService: Deleted BP-899898175-172.19.0.5-1610935156129 blk_1073741876_1052 URI file:/hadoop/dfs/data/current/BP-899898175-172.19.0.5-1610935156129/current/finalized/subdir0/subdir0/blk_1073741876\u001b[K\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:46,276 INFO datanode.DataNode: DataTransfer, at cdbb15139e9e:9866: Transmitted BP-899898175-172.19.0.5-1610935156129:blk_1073741843_1019 (numBytes=226309) to deb6331226b2/172.30.0.3:9866\u001b[K\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:46,286 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741842_1018 src: /172.30.0.8:57828 dest: /172.30.0.7:9866 of size 22788\u001b[K\r\n\u001b[33mdatanode2          |\u001b["]
[71.621335, "o", "39m 2021-01-19 03:14:46,289 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741842_1018 src: /172.30.0.7:38426 dest: /172.30.0.3:9866\u001b[K\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:46,290 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741842_1018 src: /172.30.0.7:38426 dest: /172.30.0.3:9866 of size 22788\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,298 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,298 INFO ipc.Server: IPC Server Responder: starting\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,299 INFO ipc.Server: IPC Server listener on 8030: starting\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,476 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 5000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.\u001b"]
[71.621351, "o", "[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,477 INFO ipc.Server: Starting Socket Reader #1 for port 8032\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,480 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,484 INFO ipc.Server: IPC Server Responder: starting\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,486 INFO ipc.Server: IPC Server listener on 8032: starting\u001b[K\r\n\u001b[36mresourcemanager    |\u001b[39m 2021-01-19 03:14:46,509 INFO resourcemanager.ResourceManager: Transitioned to active state\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenera"]
[71.621363, "o", "tor attachTypes\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator atta"]
[71.621375, "o", "chTypes\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\u001b[K\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\u001b"]
[71.621387, "o", "[K\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\u001b[K\r\n\u001b[K\u001b[?12l\u001b[?25h"]
[71.644577, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[35mhistoryserve"]
[71.644663, "o", "r      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.662967, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.686581, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.689317, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[71.729682, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[35mhistoryserver      |\u001b[39m Jan 19, 2021 3:14:46 AM com.sun.jersey.server.wadl.generators.AbstractWadlGeneratorGrammarGenerator attachTypes\r\n\u001b[35mhistoryserver      |\u001b[39m INFO: Couldn't find grammar element for class javax.ws.rs.core.Response\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[74.241531, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:49,258 INFO datanode.DataNode: DatanodeRegistration(172.30.0.8:9866, datanodeUuid=1ce6f158-0b39-4227-9455-ea5d127087b3, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129) Starting thread to transfer BP-899898175-172.19.0.5-1610935156129:blk_1073741849_1025 to 172.30.0.3:9866 172.30.0.7:9866 \r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:49,259 INFO datanode.DataNode: DatanodeRegistration(172.30.0.8:9866, datanodeUuid=1ce6f158-0b39-4227-9455-ea5d127087b3, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129) Starting thread to transfer BP-899898175-172.19.0.5-1610935156129:blk_1073741851_1027 to 172.30.0.7:9866 172.30.0.3:9866 \r\n\u001b[1;46r\u001b[H\u001b[45d"]
[74.242186, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:49,260 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[74.242836, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:49,260 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[74.243469, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:49,261 INFO datanode.DataNode: DataTransfer, at cdbb15139e9e:9866: Transmitted BP-899898175-172.19.0.5-1610935156129:blk_1073741849_1025 (numBytes=17) to deb6331226b2/172.30.0.3:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[74.244448, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:49,262 INFO datanode.DataNode: DataTransfer, at cdbb15139e9e:9866: Transmitted BP-899898175-172.19.0.5-1610935156129:blk_1073741851_1027 (numBytes=17) to 9e22ace955e1/172.30.0.7:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[74.248511, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:49,262 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741851_1027 src: /172.30.0.8:57862 dest: /172.30.0.7:9866\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:49,263 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741849_1025 src: /172.30.0.8:55218 dest: /172.30.0.3:9866\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:49,264 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:49,265 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741849_1025 src: /172.30.0.8:55218 dest: /172.30.0.3:9866 of size 17\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[74.24898, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:49,264 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:49,265 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741851_1027 src: /172.30.0.8:57862 dest: /172.30.0.7:9866 of size 17\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[74.252303, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:49,268 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741849_1025 src: /172.30.0.3:34894 dest: /172.30.0.7:9866\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:49,269 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741849_1025 src: /172.30.0.3:34894 dest: /172.30.0.7:9866 of size 17\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[74.256172, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:49,273 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741851_1027 src: /172.30.0.7:38458 dest: /172.30.0.3:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[74.256968, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:49,274 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741851_1027 src: /172.30.0.7:38458 dest: /172.30.0.3:9866 of size 17\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[77.242023, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:52,259 INFO datanode.DataNode: DatanodeRegistration(172.30.0.8:9866, datanodeUuid=1ce6f158-0b39-4227-9455-ea5d127087b3, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129) Starting thread to transfer BP-899898175-172.19.0.5-1610935156129:blk_1073741867_1043 to 172.30.0.3:9866 172.30.0.7:9866 \r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:52,259 INFO datanode.DataNode: DatanodeRegistration(172.30.0.8:9866, datanodeUuid=1ce6f158-0b39-4227-9455-ea5d127087b3, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129) Starting thread to transfer BP-899898175-172.19.0.5-1610935156129:blk_1073741868_1044 to 172.30.0.7:9866 172.30.0.3:9866 \r\n\u001b[1;46r\u001b[H\u001b[45d"]
[77.242351, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:52,260 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[77.243487, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:52,261 INFO datanode.DataNode: DataTransfer, at cdbb15139e9e:9866: Transmitted BP-899898175-172.19.0.5-1610935156129:blk_1073741867_1043 (numBytes=499) to deb6331226b2/172.30.0.3:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[77.243822, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:52,261 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741867_1043 src: /172.30.0.8:55240 dest: /172.30.0.3:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[77.245464, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:52,262 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:52,262 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:52,263 INFO datanode.DataNode: DataTransfer, at cdbb15139e9e:9866: Transmitted BP-899898175-172.19.0.5-1610935156129:blk_1073741868_1044 (numBytes=216) to 9e22ace955e1/172.30.0.7:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[77.246318, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:52,264 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741867_1043 src: /172.30.0.8:55240 dest: /172.30.0.3:9866 of size 499\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[77.252753, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:52,266 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741868_1044 src: /172.30.0.8:57884 dest: /172.30.0.7:9866\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:52,267 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:52,267 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741867_1043 src: /172.30.0.3:34914 dest: /172.30.0.7:9866\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:52,268 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741868_1044 src: /172.30.0.8:57884 dest: /172.30.0.7:9866 of size 216\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[77.255943, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:52,271 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741868_1044 src: /172.30.0.7:38482 dest: /172.30.0.3:9866\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:52,271 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741868_1044 src: /172.30.0.7:38482 dest: /172.30.0.3:9866 of size 216\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[77.256221, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:52,270 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741867_1043 src: /172.30.0.3:34914 dest: /172.30.0.7:9866 of size 499\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[80.24442, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:55,259 INFO datanode.DataNode: DatanodeRegistration(172.30.0.8:9866, datanodeUuid=1ce6f158-0b39-4227-9455-ea5d127087b3, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129) Starting thread to transfer BP-899898175-172.19.0.5-1610935156129:blk_1073741875_1051 to 172.30.0.7:9866 172.30.0.3:9866 \r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:55,259 INFO datanode.DataNode: DatanodeRegistration(172.30.0.8:9866, datanodeUuid=1ce6f158-0b39-4227-9455-ea5d127087b3, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-86f3147d-f6e1-4868-b7e3-89faa79f9f06;nsid=375909412;c=1610935156129) Starting thread to transfer BP-899898175-172.19.0.5-1610935156129:blk_1073741877_1053 to 172.30.0.3:9866 172.30.0.7:9866 \r\n\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:55,260 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = fals"]
[80.244494, "o", "e, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:55,261 INFO datanode.DataNode: DataTransfer, at cdbb15139e9e:9866: Transmitted BP-899898175-172.19.0.5-1610935156129:blk_1073741875_1051 (numBytes=17) to 9e22ace955e1/172.30.0.7:9866\r\n\u001b[1;46r\u001b[H\u001b[45d\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:55,261 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[80.246297, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[36mdatanode           |\u001b[39m 2021-01-19 03:14:55,262 INFO datanode.DataNode: DataTransfer, at cdbb15139e9e:9866: Transmitted BP-899898175-172.19.0.5-1610935156129:blk_1073741877_1053 (numBytes=17) to deb6331226b2/172.30.0.3:9866\r\n\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:55,263 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741877_1053 src: /172.30.0.8:55270 dest: /172.30.0.3:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[80.247212, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:55,264 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[80.248683, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:55,265 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741877_1053 src: /172.30.0.8:55270 dest: /172.30.0.3:9866 of size 17\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[80.250439, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:55,265 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741875_1051 src: /172.30.0.8:57910 dest: /172.30.0.7:9866\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:55,266 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:55,267 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741875_1051 src: /172.30.0.8:57910 dest: /172.30.0.7:9866 of size 17\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[80.252494, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:55,269 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741875_1051 src: /172.30.0.7:38510 dest: /172.30.0.3:9866\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[80.253359, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:55,269 INFO datanode.DataNode: Receiving BP-899898175-172.19.0.5-1610935156129:blk_1073741877_1053 src: /172.30.0.3:34942 dest: /172.30.0.7:9866\r\n\u001b[32mdatanode3          |\u001b[39m 2021-01-19 03:14:55,269 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741877_1053 src: /172.30.0.3:34942 dest: /172.30.0.7:9866 of size 17\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[80.254481, "o", "\u001b[1;45r\u001b[H\u001b[45d\u001b[33mdatanode2          |\u001b[39m 2021-01-19 03:14:55,271 INFO datanode.DataNode: Received BP-899898175-172.19.0.5-1610935156129:blk_1073741875_1051 src: /172.30.0.7:38510 dest: /172.30.0.3:9866 of size 17\r\n\u001b[1;46r\u001b[H\u001b[45d"]
[82.384106, "o", "\u001b[1;46r\u001b(B\u001b[m\u001b[?1l\u001b>\u001b[H\u001b[2J\u001b]112\u0007\u001b[?12l\u001b[?25h\u001b[?1000l\u001b[?1006l\u001b[?1005l\u001b[>4m\u001b[?1004l\u001b[?1049l"]
[82.384475, "o", "[detached]\r\n"]
[82.38536, "o", "[1]+  Done                    /topsec/topav_client/bin/nohup /usr/local/bin/linux_client >> /dev/null 2>&1\r\n\u001b]0;root@k8snode1:~/bigdata/docker-hadoop\u0007[root@k8snode1 docker-hadoop]# "]
[85.227658, "o", "\u001b[H\u001b[2J[root@k8snode1 docker-hadoop]# "]
[86.029112, "o", "d"]
[86.153471, "o", "o"]
[86.55203, "o", "\b\u001b[K"]
[86.975976, "o", "\b\u001b[K"]
[87.616938, "o", "\r\u001b[9P(reverse-i-search)`':\u001b[C"]
[88.357578, "o", "\b\b\bh': hdfs dfs  -put /root/airflow-alone.tar  /\r\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C"]
[88.551885, "o", "\b\b\b\u001b[1@d\u001b[C\u001b[C\u001b[C"]
[88.786486, "o", "\b\b\b\u001b[1@f\u001b[C\u001b[C\u001b[C"]
[94.583433, "o", "\r\u001b[6@[root@k8snode1 docker-hadoop]#\u001b[C\u001b[C"]
[95.071144, "o", "\u001b[C"]
[95.100834, "o", "\u001b[C"]
[95.129465, "o", "\u001b[C"]
[95.158818, "o", "\u001b[C"]
[95.189122, "o", "\u001b[C"]
[95.218867, "o", "\u001b[C"]
[95.247897, "o", "\u001b[C"]
[95.27774, "o", "\u001b[C"]
[95.307812, "o", "\u001b[C"]
[95.338025, "o", "\u001b[C"]
[95.398582, "o", "\u001b[C"]
[95.732964, "o", "\u001b[C"]
[95.936841, "o", "\u001b[C"]
[96.46666, "o", "\b\u001b[1P"]
[96.635639, "o", "\b\u001b[1P"]
[96.800872, "o", "\b\u001b[1P"]
[97.368949, "o", "\u001b[1@l"]
[97.469248, "o", "\u001b[1@s"]
[97.713179, "o", "\u001b[C\u001b[1@ \b"]
[98.356006, "o", "\u001b[C"]
[98.855969, "o", "\u001b[C"]
[98.888776, "o", "\u001b[C"]
[98.913288, "o", "\u001b[C"]
[99.921492, "o", "\b"]
[100.133149, "o", "\b"]
[101.675917, "o", "\u001b[K"]
[102.503183, "o", "\r\n"]
[102.511917, "o", "bash: hdfs: command not found...\r\n"]
[102.522221, "o", "\u001b]0;root@k8snode1:~/bigdata/docker-hadoop\u0007[root@k8snode1 docker-hadoop]# "]
[114.717998, "o", "\r\u001b[9P(reverse-i-search)`':\u001b[C"]
[115.424225, "o", "\b\b\ba': tmux a\b"]
[115.637243, "o", "\b\b\b\b\b\b\b\bl': hdfs dfs  -put /root/airflow-alone.tar  /\b\b\b\b\b\b\b\b\b\b\b\b"]
[115.854632, "o", "\r\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[Ci': alias hdfs='docker run --network docker-hadoop_default --env-file /root/bigdata/docker-hadoop/hadoop.env -v $PWD:/root/ bde2020/hadoop-base:2.0.0-hadoop3.2.1-java8 hdfs'\r\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C"]
[118.404444, "o", "\r\u001b[6@[root@k8snode1 docker-hadoop]#\u001b[C\u001b[C"]
[118.901907, "o", "\u001b[C"]
[118.930869, "o", "\u001b[C"]
[118.960571, "o", "\u001b[C"]
[118.990364, "o", "\u001b[C"]
[119.020693, "o", "\u001b[C"]
[119.050825, "o", "\u001b[C"]
[119.081095, "o", "\u001b[C"]
[119.110675, "o", "\u001b[C"]
[119.157014, "o", "\u001b[C"]
[119.170247, "o", "\u001b[C"]
[119.199907, "o", "\u001b[C"]
[119.230664, "o", "\u001b[C"]
[121.114893, "o", "\u001b[C"]
[121.341294, "o", "\u001b[C"]
[121.531292, "o", "\u001b[C"]
[121.712226, "o", "\u001b[C"]
[121.902761, "o", "\u001b[C"]
[122.078469, "o", "\u001b[C"]
[122.283315, "o", "\u001b[C"]
[122.463421, "o", "\u001b[C"]
[122.667237, "o", "\u001b[C"]
[123.935826, "o", "\u001b[C\u001b[1@ \b"]
[125.245275, "o", "\u001b[1@-"]
[125.381298, "o", "\u001b[1@-"]
[125.746643, "o", "\u001b[1@r"]
[126.14324, "o", "\u001b[1@m"]
[126.292431, "o", "\u001b[C\u001b[1@ \b"]
[126.658711, "o", "\r\n\u001b]0;root@k8snode1:~/bigdata/docker-hadoop\u0007[root@k8snode1 docker-hadoop]# "]
[128.591406, "o", "alias hdfs='docker run --rm  --network docker-hadoop_default --env-file /root/bigdata/docker-hadoop/hadoop.env -v $PWD:/root/ bde2020/hadoop-base:2.0.0-hadoop3.2.1-java8 hdfs'"]
[128.781163, "o", "\r\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[Chdfs dfs  -ls  /\u001b[K"]
[129.411748, "o", "\r\n"]
[135.367798, "o", "Configuring core\r\n"]
[135.37544, "o", " - Setting hadoop.proxyuser.hue.hosts=*\r\n"]
[135.385453, "o", " - Setting fs.defaultFS=hdfs://namenode:9000\r\n"]
[135.393364, "o", " - Setting hadoop.http.staticuser.user=root\r\n"]
[135.401116, "o", " - Setting io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec\r\n"]
[135.408893, "o", " - Setting hadoop.proxyuser.hue.groups=*\r\n"]
[135.413379, "o", "Configuring hdfs\r\n"]
[135.42051, "o", " - Setting dfs.namenode.datanode.registration.ip-hostname-check=false\r\n"]
[135.428892, "o", " - Setting dfs.webhdfs.enabled=true\r\n"]
[135.437503, "o", " - Setting dfs.permissions.enabled=false\r\n"]
[135.442034, "o", "Configuring yarn\r\n"]
[135.448908, "o", " - Setting yarn.timeline-service.enabled=true\r\n"]
[135.458007, "o", " - Setting yarn.scheduler.capacity.root.default.maximum-allocation-vcores=4\r\n"]
[135.466531, "o", " - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\r\n"]
[135.475042, "o", " - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n"]
[135.48541, "o", " - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\r\n"]
[135.493304, "o", " - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\r\n"]
[135.501629, "o", " - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\r\n"]
[135.510078, "o", " - Setting yarn.timeline-service.generic-application-history.enabled=true\r\n"]
[135.519478, "o", " - Setting yarn.log-aggregation-enable=true\r\n"]
[135.528469, "o", " - Setting yarn.resourcemanager.hostname=resourcemanager\r\n"]
[135.537247, "o", " - Setting yarn.scheduler.capacity.root.default.maximum-allocation-mb=8192\r\n"]
[135.545761, "o", " - Setting yarn.nodemanager.aux-services=mapreduce_shuffle\r\n"]
[135.553541, "o", " - Setting yarn.resourcemanager.resource_tracker.address=resourcemanager:8031\r\n"]
[135.561686, "o", " - Setting yarn.timeline-service.hostname=historyserver\r\n"]
[135.574703, "o", " - Setting yarn.resourcemanager.scheduler.address=resourcemanager:8030\r\n"]
[135.584299, "o", " - Setting yarn.resourcemanager.address=resourcemanager:8032\r\n"]
[135.592897, "o", " - Setting mapred.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec\r\n"]
[135.601395, "o", " - Setting yarn.nodemanager.remote-app-log-dir=/app-logs\r\n"]
[135.610848, "o", " - Setting yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n"]
[135.621563, "o", " - Setting mapreduce.map.output.compress=true\r\n"]
[135.632458, "o", " - Setting yarn.nodemanager.resource.memory-mb=16384\r\n"]
[135.640737, "o", " - Setting yarn.resourcemanager.recovery.enabled=true\r\n"]
[135.648734, "o", " - Setting yarn.nodemanager.resource.cpu-vcores=8\r\n"]
[135.653437, "o", "Configuring httpfs\r\n"]
[135.65774, "o", "Configuring kms\r\n"]
[135.661543, "o", "Configuring mapred\r\n"]
[135.668546, "o", " - Setting mapreduce.map.java.opts=-Xmx3072m\r\n"]
[135.678557, "o", " - Setting mapreduce.reduce.java.opts=-Xmx6144m\r\n"]
[135.686784, "o", " - Setting mapreduce.reduce.memory.mb=8192\r\n"]
[135.694941, "o", " - Setting yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n"]
[135.702783, "o", " - Setting mapreduce.map.memory.mb=4096\r\n"]
[135.710761, "o", " - Setting mapred.child.java.opts=-Xmx4096m\r\n"]
[135.720573, "o", " - Setting mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n"]
[135.730248, "o", " - Setting mapreduce.framework.name=yarn\r\n"]
[135.740695, "o", " - Setting mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n"]
[135.746217, "o", "Configuring for multihomed network\r\n"]
[137.189146, "o", "Found 7 items\r\n-rw-r--r--   3 root supergroup  994507264 2021-01-19 03:04 /airflow-alone.tar\r\ndrwxrwxrwt   - root root                0 2021-01-18 02:15 /app-logs\r\n"]
[137.189554, "o", "drwxr-xr-x   - root supergroup          0 2021-01-18 08:22 /input\r\ndrwxr-xr-x   - root supergroup          0 2021-01-18 08:23 /output\r\n"]
[137.189957, "o", "drwxr-xr-x   - root supergroup          0 2021-01-18 01:59 /rmstate\r\ndrwxr-xr-x   - root supergroup          0 2021-01-18 05:56 /system\r\n"]
[137.190264, "o", "drwx------   - root supergroup          0 2021-01-18 02:15 /tmp\r\n"]
[137.849609, "o", "\u001b]0;root@k8snode1:~/bigdata/docker-hadoop\u0007[root@k8snode1 docker-hadoop]# "]
[141.890697, "o", "\r\u001b[9P(reverse-i-search)`':\u001b[C"]
[142.86799, "o", "\b\b\bf': hdfs dfs  -ls  /\b\b\b\b\b\b\b\b\b\b"]
[143.010347, "o", "\b\b\b\b\b\b\b\b\b\u001b[1@s\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C"]
[143.468116, "o", "\u0007"]
[147.386984, "o", "^C\r\n\u001b]0;root@k8snode1:~/bigdata/docker-hadoop\u0007[root@k8snode1 docker-hadoop]# "]
[148.501163, "o", "\u001b[H\u001b[2J[root@k8snode1 docker-hadoop]# "]
[149.547487, "o", "h"]
[149.641429, "o", "i"]
[149.69051, "o", "s"]
[149.904481, "o", "tory "]
[150.436011, "o", "se:2.0.0-hadoop3.2.1-java8 hdfs'\r\n 1004  hdfs dfs  -ls  /\r\n 1005  history \r\n\u001b]0;root@k8snode1:~/bigdata/docker-hadoop\u0007[root@k8snode1 docker-hadoop]# "]
[159.780443, "o", "\u001b[H\u001b[2J[root@k8snode1 docker-hadoop]# "]
[161.650078, "o", "f"]
[162.5157, "o", "c"]
[163.923934, "o", "\b\u001b[K"]
[164.277675, "o", "s"]
[164.761705, "o", "\b\u001b[K"]
[164.93069, "o", "\b\u001b[K"]
[165.391923, "o", "h"]
[166.753748, "o", "d"]
[166.959424, "o", "f"]
[167.13398, "o", "s"]
[167.341934, "o", " "]
[169.546247, "o", "f"]
[169.778212, "o", "s"]
[170.099668, "o", "c"]
[170.571482, "o", "k"]
[171.074647, "o", " "]
[172.22374, "o", "/"]
[173.11609, "o", "a"]
[173.518411, "o", "\b\u001b[K"]
[173.698075, "o", "\b\u001b[K"]
[176.760233, "o", "/"]
[177.503722, "o", "a"]
[177.770388, "o", "i"]
[177.96038, "o", "r"]
[178.276165, "o", "\u0007"]
[179.780765, "o", "\b\u001b[K"]
[179.965156, "o", "\b\u001b[K"]
[180.656898, "o", "\b\u001b[K\b\u001b[K"]
[182.097831, "o", "!"]
[182.871291, "o", "\b\u001b[K"]
[183.232925, "o", "~"]
[183.507166, "o", "/"]
[184.787985, "o", "air"]
[184.791785, "o", "flow-alone.tar "]
[186.902127, "o", "\b"]
[187.057896, "o", "\b"]
[187.092368, "o", "\b"]
[187.609069, "o", "\b\b\b\b\b\b\b\b\b\b\b\b\b\b"]
[188.077816, "o", "\b\b\b\b\b\b\b\b\b"]
[188.119038, "o", "\b"]
[188.569259, "o", "\u001b[C"]
[188.881726, "o", "\u001b[C"]
[188.911389, "o", "\u001b[C"]
[189.061139, "o", "\u001b[C"]
[189.282314, "o", "\u001b[C"]
[189.390873, "o", "\u001b[C"]
[189.664735, "o", "\u001b[C"]
[190.234053, "o", "\u001b[C"]
[190.662992, "o", "\b\u001b[1P"]
[191.214613, "o", "\u001b[C"]
[191.716693, "o", "\u001b[C"]
[191.759459, "o", "\u001b[C"]
[191.778233, "o", "\u001b[C"]
[191.798842, "o", "\u001b[C"]
[191.829877, "o", "\u001b[C"]
[191.860042, "o", "\u001b[C"]
[191.889128, "o", "\u001b[C"]
[191.919048, "o", "\u001b[C"]
[191.948521, "o", "\u001b[C"]
[191.978966, "o", "\u001b[C"]
[192.008744, "o", "\u001b[C"]
[192.045685, "o", "\u001b[C"]
[192.156035, "o", "\u001b[C\u001b[C\u001b[C"]
[192.163239, "o", "\u001b[C"]
[192.18842, "o", "\u001b[C"]
[192.218243, "o", "\u001b[C"]
[192.250158, "o", "\u0007"]
[192.278394, "o", "\u0007"]
[192.309218, "o", "\u0007"]
[192.338232, "o", "\u0007"]
[192.367513, "o", "\u0007"]
[192.397093, "o", "\u0007"]
[192.427293, "o", "\u0007"]
[192.547726, "o", " "]
[192.987105, "o", "-"]
[193.321125, "o", "f"]
[193.486534, "o", "i"]
[193.661496, "o", "l"]
[193.788759, "o", "e"]
[193.94621, "o", "s"]
[194.279724, "o", "\r\n"]
[194.750045, "o", "Configuring core\r\n"]
[194.758813, "o", " - Setting hadoop.proxyuser.hue.hosts=*\r\n"]
[194.768011, "o", " - Setting fs.defaultFS=hdfs://namenode:9000\r\n"]
[194.776165, "o", " - Setting hadoop.http.staticuser.user=root\r\n"]
[194.784178, "o", " - Setting io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec\r\n"]
[194.793682, "o", " - Setting hadoop.proxyuser.hue.groups=*\r\n"]
[194.798736, "o", "Configuring hdfs\r\n"]
[194.806459, "o", " - Setting dfs.namenode.datanode.registration.ip-hostname-check=false\r\n"]
[194.814395, "o", " - Setting dfs.webhdfs.enabled=true\r\n"]
[194.822154, "o", " - Setting dfs.permissions.enabled=false\r\n"]
[194.826407, "o", "Configuring yarn\r\n"]
[194.83366, "o", " - Setting yarn.timeline-service.enabled=true\r\n"]
[194.84191, "o", " - Setting yarn.scheduler.capacity.root.default.maximum-allocation-vcores=4\r\n"]
[194.855187, "o", " - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\r\n"]
[194.865142, "o", " - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n"]
[194.873977, "o", " - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\r\n"]
[194.882952, "o", " - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\r\n"]
[194.89516, "o", " - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\r\n"]
[194.90423, "o", " - Setting yarn.timeline-service.generic-application-history.enabled=true\r\n"]
[194.915029, "o", " - Setting yarn.log-aggregation-enable=true\r\n"]
[194.924124, "o", " - Setting yarn.resourcemanager.hostname=resourcemanager\r\n"]
[194.934982, "o", " - Setting yarn.scheduler.capacity.root.default.maximum-allocation-mb=8192\r\n"]
[194.946938, "o", " - Setting yarn.nodemanager.aux-services=mapreduce_shuffle\r\n"]
[194.957507, "o", " - Setting yarn.resourcemanager.resource_tracker.address=resourcemanager:8031\r\n"]
[194.968509, "o", " - Setting yarn.timeline-service.hostname=historyserver\r\n"]
[194.979812, "o", " - Setting yarn.resourcemanager.scheduler.address=resourcemanager:8030\r\n"]
[194.988179, "o", " - Setting yarn.resourcemanager.address=resourcemanager:8032\r\n"]
[194.995817, "o", " - Setting mapred.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec\r\n"]
[195.005038, "o", " - Setting yarn.nodemanager.remote-app-log-dir=/app-logs\r\n"]
[195.015402, "o", " - Setting yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n"]
[195.0263, "o", " - Setting mapreduce.map.output.compress=true\r\n"]
[195.037621, "o", " - Setting yarn.nodemanager.resource.memory-mb=16384\r\n"]
[195.055, "o", " - Setting yarn.resourcemanager.recovery.enabled=true\r\n"]
[195.068393, "o", " - Setting yarn.nodemanager.resource.cpu-vcores=8\r\n"]
[195.072963, "o", "Configuring httpfs\r\n"]
[195.076491, "o", "Configuring kms\r\n"]
[195.080603, "o", "Configuring mapred\r\n"]
[195.090867, "o", " - Setting mapreduce.map.java.opts=-Xmx3072m\r\n"]
[195.098992, "o", " - Setting mapreduce.reduce.java.opts=-Xmx6144m\r\n"]
[195.111178, "o", " - Setting mapreduce.reduce.memory.mb=8192\r\n"]
[195.128068, "o", " - Setting yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n"]
[195.142823, "o", " - Setting mapreduce.map.memory.mb=4096\r\n"]
[195.156119, "o", " - Setting mapred.child.java.opts=-Xmx4096m\r\n"]
[195.168387, "o", " - Setting mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n"]
[195.177527, "o", " - Setting mapreduce.framework.name=yarn\r\n"]
[195.189073, "o", " - Setting mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n"]
[195.195648, "o", "Configuring for multihomed network\r\n"]
[196.793528, "o", "Connecting to namenode via http://namenode:9870/fsck?ugi=root&files=1&path=%2Fairflow-alone.tar\r\n"]
[196.860788, "o", "FSCK started by root (auth:SIMPLE) from /172.30.0.9 for path /airflow-alone.tar at Tue Jan 19 03:16:51 UTC 2021\r\n/airflow-alone.tar 994507264 bytes, replicated: replication=3, 8 block(s):  OK\r\n\r\nStatus: HEALTHY\r\n Number of data-nodes:\t3\r\n Number of racks:\t\t1\r\n Total dirs:\t\t\t0\r\n Total symlinks:\t\t0\r\n\r\nReplicated Blocks:\r\n Total size:\t994507264 B\r\n Total files:\t1\r\n Total blocks (validated):\t8 (avg. block size 124313408 B)\r\n Minimally replicated blocks:\t8 (100.0 %)\r\n Over-replicated blocks:\t0 (0.0 %)\r\n Under-replicated blocks:\t0 (0.0 %)\r\n Mis-replicated blocks:\t\t0 (0.0 %)\r\n"]
[196.861124, "o", " Default replication factor:\t3\r\n Average block replication:\t3.0\r\n Missing blocks:\t\t0\r\n Corrupt blocks:\t\t0\r\n Missing replicas:\t\t0 (0.0 %)\r\n\r\nErasure Coded Block Groups:\r\n Total size:\t0 B\r\n Total files:\t0\r\n Total block groups (validated):\t0\r\n"]
[196.861228, "o", " Minimally erasure-coded block groups:\t0\r\n Over-erasure-coded block groups:\t0\r\n Under-erasure-coded block groups:\t0\r\n Unsatisfactory placement block groups:\t0\r\n Average block group size:\t0.0\r\n Missing block groups:\t\t0\r\n Corrupt block groups:\t\t0\r\n Missing internal blocks:\t0\r\nFSCK ended at Tue Jan 19 03:16:51 UTC 2021 in 6 milliseconds\r\n\r\n\r\nThe filesystem under path '/airflow-alone.tar' is HEALTHY\r\n"]
[197.461343, "o", "\u001b]0;root@k8snode1:~/bigdata/docker-hadoop\u0007[root@k8snode1 docker-hadoop]# "]
[200.912529, "o", "hdfs fsck /airflow-alone.tar  -files"]
[201.428714, "o", "\b"]
[201.623258, "o", "\b"]
[202.674891, "o", "\u001b[C"]
[202.861045, "o", "\u001b[C"]
[203.389223, "o", "\b\u001b[K"]
[203.544804, "o", "\b\u001b[K"]
[203.722211, "o", "\b\u001b[K"]
[203.897239, "o", "\b\u001b[K"]
[204.521662, "o", "\b\u001b[K"]
[204.834749, "o", "b"]
[205.509096, "o", "l"]
[205.694036, "o", "o"]
[206.09963, "o", "c"]
[206.338613, "o", "\u0007"]
[207.025564, "o", "k"]
[207.346562, "o", "s"]
[207.638083, "o", "\r\n"]
[208.1055, "o", "Configuring core\r\n"]
[208.112854, "o", " - Setting hadoop.proxyuser.hue.hosts=*\r\n"]
[208.121822, "o", " - Setting fs.defaultFS=hdfs://namenode:9000\r\n"]
[208.131083, "o", " - Setting hadoop.http.staticuser.user=root\r\n"]
[208.142498, "o", " - Setting io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec\r\n"]
[208.152434, "o", " - Setting hadoop.proxyuser.hue.groups=*\r\n"]
[208.159999, "o", "Configuring hdfs\r\n"]
[208.167457, "o", " - Setting dfs.namenode.datanode.registration.ip-hostname-check=false\r\n"]
[208.175305, "o", " - Setting dfs.webhdfs.enabled=true\r\n"]
[208.184247, "o", " - Setting dfs.permissions.enabled=false\r\n"]
[208.188971, "o", "Configuring yarn\r\n"]
[208.195955, "o", " - Setting yarn.timeline-service.enabled=true\r\n"]
[208.204726, "o", " - Setting yarn.scheduler.capacity.root.default.maximum-allocation-vcores=4\r\n"]
[208.21421, "o", " - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\r\n"]
[208.222753, "o", " - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n"]
[208.232394, "o", " - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\r\n"]
[208.242373, "o", " - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\r\n"]
[208.254225, "o", " - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\r\n"]
[208.263544, "o", " - Setting yarn.timeline-service.generic-application-history.enabled=true\r\n"]
[208.277293, "o", " - Setting yarn.log-aggregation-enable=true\r\n"]
[208.286702, "o", " - Setting yarn.resourcemanager.hostname=resourcemanager\r\n"]
[208.29438, "o", " - Setting yarn.scheduler.capacity.root.default.maximum-allocation-mb=8192\r\n"]
[208.302014, "o", " - Setting yarn.nodemanager.aux-services=mapreduce_shuffle\r\n"]
[208.309541, "o", " - Setting yarn.resourcemanager.resource_tracker.address=resourcemanager:8031\r\n"]
[208.317418, "o", " - Setting yarn.timeline-service.hostname=historyserver\r\n"]
[208.325825, "o", " - Setting yarn.resourcemanager.scheduler.address=resourcemanager:8030\r\n"]
[208.334308, "o", " - Setting yarn.resourcemanager.address=resourcemanager:8032\r\n"]
[208.343094, "o", " - Setting mapred.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec\r\n"]
[208.352076, "o", " - Setting yarn.nodemanager.remote-app-log-dir=/app-logs\r\n"]
[208.361253, "o", " - Setting yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n"]
[208.372364, "o", " - Setting mapreduce.map.output.compress=true\r\n"]
[208.380363, "o", " - Setting yarn.nodemanager.resource.memory-mb=16384\r\n"]
[208.388418, "o", " - Setting yarn.resourcemanager.recovery.enabled=true\r\n"]
[208.396276, "o", " - Setting yarn.nodemanager.resource.cpu-vcores=8\r\n"]
[208.400814, "o", "Configuring httpfs\r\n"]
[208.405808, "o", "Configuring kms\r\n"]
[208.410741, "o", "Configuring mapred\r\n"]
[208.417694, "o", " - Setting mapreduce.map.java.opts=-Xmx3072m\r\n"]
[208.4286, "o", " - Setting mapreduce.reduce.java.opts=-Xmx6144m\r\n"]
[208.438328, "o", " - Setting mapreduce.reduce.memory.mb=8192\r\n"]
[208.449721, "o", " - Setting yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n"]
[208.461443, "o", " - Setting mapreduce.map.memory.mb=4096\r\n"]
[208.470664, "o", " - Setting mapred.child.java.opts=-Xmx4096m\r\n"]
[208.479936, "o", " - Setting mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n"]
[208.488681, "o", " - Setting mapreduce.framework.name=yarn\r\n"]
[208.497964, "o", " - Setting mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n"]
[208.504131, "o", "Configuring for multihomed network\r\n"]
[210.12502, "o", "Connecting to namenode via http://namenode:9870/fsck?ugi=root&blocks=1&path=%2Fairflow-alone.tar\r\n"]
[210.175933, "o", "FSCK started by root (auth:SIMPLE) from /172.30.0.9 for path /airflow-alone.tar at Tue Jan 19 03:17:05 UTC 2021\r\n\r\nStatus: HEALTHY\r\n Number of data-nodes:\t3\r\n Number of racks:\t\t1\r\n Total dirs:\t\t\t0\r\n Total symlinks:\t\t0\r\n\r\nReplicated Blocks:\r\n Total size:\t994507264 B\r\n Total files:\t1\r\n Total blocks (validated):\t8 (avg. block size 124313408 B)\r\n Minimally replicated blocks:\t8 (100.0 %)\r\n Over-replicated blocks:\t0 (0.0 %)\r\n Under-replicated blocks:\t0 (0.0 %)\r\n Mis-replicated blocks:\t\t0 (0.0 %)\r\n Default replication factor:\t3\r\n Average block replication:\t3.0\r\n Missing blocks:\t\t0\r\n Corrupt blocks:\t\t0\r\n Missing replicas:\t\t0 (0.0 %)\r\n"]
[210.176097, "o", "\r\nErasure Coded Block Groups:\r\n Total size:\t0 B\r\n Total files:\t0\r\n Total block groups (validated):\t0\r\n Minimally erasure-coded block groups:\t0\r\n Over-erasure-coded block groups:\t0\r\n Under-erasure-coded block groups:\t0\r\n Unsatisfactory placement block groups:\t0\r\n Average block group size:\t0.0\r\n Missing block groups:\t\t0\r\n Corrupt block groups:\t\t0\r\n Missing internal blocks:\t0\r\nFSCK ended at Tue Jan 19 03:17:05 UTC 2021 in 1 milliseconds\r\n\r\n\r\n"]
[210.176194, "o", "The filesystem under path '/airflow-alone.tar' is HEALTHY\r\n"]
[210.831182, "o", "\u001b]0;root@k8snode1:~/bigdata/docker-hadoop\u0007[root@k8snode1 docker-hadoop]# "]
[214.212295, "o", "hdfs fsck /airflow-alone.tar  -blocks"]
[215.606155, "o", "\b\u001b[K\b\u001b[K\b\u001b[K"]
[215.762085, "o", "\b\u001b[K"]
[216.144637, "o", "\b\u001b[K"]
[216.954694, "o", "\b\u001b[K"]
[217.182987, "o", "l"]
[217.36791, "o", "o"]
[217.491266, "o", "c"]
[217.652078, "o", "a"]
[218.130549, "o", "t"]
[218.304076, "o", "i"]
[218.415142, "o", "o"]
[218.914089, "o", "ns"]
[220.632728, "o", "\r\n"]
[221.103904, "o", "Configuring core\r\n"]
[221.112931, "o", " - Setting hadoop.proxyuser.hue.hosts=*\r\n"]
[221.120847, "o", " - Setting fs.defaultFS=hdfs://namenode:9000\r\n"]
[221.130471, "o", " - Setting hadoop.http.staticuser.user=root\r\n"]
[221.138543, "o", " - Setting io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec\r\n"]
[221.146703, "o", " - Setting hadoop.proxyuser.hue.groups=*\r\n"]
[221.151032, "o", "Configuring hdfs\r\n"]
[221.158143, "o", " - Setting dfs.namenode.datanode.registration.ip-hostname-check=false\r\n"]
[221.166433, "o", " - Setting dfs.webhdfs.enabled=true\r\n"]
[221.17473, "o", " - Setting dfs.permissions.enabled=false\r\n"]
[221.179202, "o", "Configuring yarn\r\n"]
[221.186713, "o", " - Setting yarn.timeline-service.enabled=true\r\n"]
[221.194777, "o", " - Setting yarn.scheduler.capacity.root.default.maximum-allocation-vcores=4\r\n"]
[221.202734, "o", " - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\r\n"]
[221.211092, "o", " - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n"]
[221.219475, "o", " - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\r\n"]
[221.228192, "o", " - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\r\n"]
[221.23694, "o", " - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\r\n"]
[221.246872, "o", " - Setting yarn.timeline-service.generic-application-history.enabled=true\r\n"]
[221.255141, "o", " - Setting yarn.log-aggregation-enable=true\r\n"]
[221.263413, "o", " - Setting yarn.resourcemanager.hostname=resourcemanager\r\n"]
[221.271528, "o", " - Setting yarn.scheduler.capacity.root.default.maximum-allocation-mb=8192\r\n"]
[221.279338, "o", " - Setting yarn.nodemanager.aux-services=mapreduce_shuffle\r\n"]
[221.287837, "o", " - Setting yarn.resourcemanager.resource_tracker.address=resourcemanager:8031\r\n"]
[221.296257, "o", " - Setting yarn.timeline-service.hostname=historyserver\r\n"]
[221.30428, "o", " - Setting yarn.resourcemanager.scheduler.address=resourcemanager:8030\r\n"]
[221.312359, "o", " - Setting yarn.resourcemanager.address=resourcemanager:8032\r\n"]
[221.321384, "o", " - Setting mapred.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec\r\n"]
[221.328387, "o", " - Setting yarn.nodemanager.remote-app-log-dir=/app-logs\r\n"]
[221.339708, "o", " - Setting yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n"]
[221.345723, "o", " - Setting mapreduce.map.output.compress=true\r\n"]
[221.354621, "o", " - Setting yarn.nodemanager.resource.memory-mb=16384\r\n"]
[221.363138, "o", " - Setting yarn.resourcemanager.recovery.enabled=true\r\n"]
[221.372416, "o", " - Setting yarn.nodemanager.resource.cpu-vcores=8\r\n"]
[221.376654, "o", "Configuring httpfs\r\n"]
[221.381153, "o", "Configuring kms\r\n"]
[221.386245, "o", "Configuring mapred\r\n"]
[221.393322, "o", " - Setting mapreduce.map.java.opts=-Xmx3072m\r\n"]
[221.401625, "o", " - Setting mapreduce.reduce.java.opts=-Xmx6144m\r\n"]
[221.410411, "o", " - Setting mapreduce.reduce.memory.mb=8192\r\n"]
[221.421395, "o", " - Setting yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n"]
[221.430028, "o", " - Setting mapreduce.map.memory.mb=4096\r\n"]
[221.441339, "o", " - Setting mapred.child.java.opts=-Xmx4096m\r\n"]
[221.452774, "o", " - Setting mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n"]
[221.463802, "o", " - Setting mapreduce.framework.name=yarn\r\n"]
[221.472901, "o", " - Setting mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n"]
[221.477626, "o", "Configuring for multihomed network\r\n"]
[223.047817, "o", "Connecting to namenode via http://namenode:9870/fsck?ugi=root&locations=1&path=%2Fairflow-alone.tar\r\n"]
[223.101117, "o", "FSCK started by root (auth:SIMPLE) from /172.30.0.9 for path /airflow-alone.tar at Tue Jan 19 03:17:18 UTC 2021\r\n\r\nStatus: HEALTHY\r\n Number of data-nodes:\t3\r\n Number of racks:\t\t1\r\n Total dirs:\t\t\t0\r\n Total symlinks:\t\t0\r\n\r\nReplicated Blocks:\r\n Total size:\t994507264 B\r\n Total files:\t1\r\n Total blocks (validated):\t8 (avg. block size 124313408 B)\r\n Minimally replicated blocks:\t8 (100.0 %)\r\n Over-replicated blocks:\t0 (0.0 %)\r\n Under-replicated blocks:\t0 (0.0 %)\r\n Mis-replicated blocks:\t\t0 (0.0 %)\r\n Default replication factor:\t3\r\n Average block replication:\t3.0\r\n Missing blocks:\t\t0\r\n Corrupt blocks:\t\t0\r\n Missing replicas:\t\t0 (0.0 %)\r\n\r\nErasure Coded Block Groups:\r\n Total size:\t0 B\r\n Total files:\t0\r\n Total block groups (validated):\t0\r\n Minimally erasure-coded block groups:\t0\r\n Over-erasure-coded block groups:\t0\r\n Under-erasure-coded block groups:\t0\r\n Unsatisfactory placement block groups:\t0\r\n Average block group size:\t0.0\r\n Missing block groups:\t\t0\r\n Corrupt block groups:\t\t0\r\n Missing internal blocks:\t0\r\nFSCK ended at"]
[223.101194, "o", " Tue Jan 19 03:17:18 UTC 2021 in 1 milliseconds\r\n\r\n\r\nThe filesystem under path '/airflow-alone.tar' is HEALTHY\r\n"]
[223.700625, "o", "\u001b]0;root@k8snode1:~/bigdata/docker-hadoop\u0007[root@k8snode1 docker-hadoop]# "]
[236.098983, "o", "hdfs fsck /airflow-alone.tar  -locations"]
[238.11799, "o", " "]
[239.232656, "o", "-f"]
[239.80637, "o", "i"]
[239.842257, "o", "i"]
[239.986949, "o", "l"]
[240.242869, "o", "s"]
[240.621628, "o", "\b\u001b[K"]
[240.809988, "o", "\b\u001b[K"]
[240.962268, "o", "\b\u001b[K"]
[241.241272, "o", "e"]
[241.651033, "o", "\b\u001b[K"]
[242.026901, "o", "l"]
[242.186611, "o", "s"]
[242.341817, "o", "e"]
[243.479499, "o", "\b\u001b[K"]
[243.645335, "o", "\b\u001b[K"]
[244.076514, "o", "e"]
[244.419953, "o", "s"]
[244.902261, "o", " "]
[245.261383, "o", "-"]
[246.032043, "o", "b"]
[246.184728, "o", "l"]
[246.38621, "o", "o"]
[246.51091, "o", "e"]
[246.869852, "o", "c"]
[247.365019, "o", "\b\u001b[K"]
[247.503111, "o", "\b\u001b[K"]
[249.512413, "o", "c"]
[249.682663, "o", "k"]
[249.831168, "o", "s"]
[251.664756, "o", "\r\n"]
[252.135669, "o", "Configuring core\r\n"]
[252.143111, "o", " - Setting hadoop.proxyuser.hue.hosts=*\r\n"]
[252.151973, "o", " - Setting fs.defaultFS=hdfs://namenode:9000\r\n"]
[252.159579, "o", " - Setting hadoop.http.staticuser.user=root\r\n"]
[252.167969, "o", " - Setting io.compression.codecs=org.apache.hadoop.io.compress.SnappyCodec\r\n"]
[252.176932, "o", " - Setting hadoop.proxyuser.hue.groups=*\r\n"]
[252.189881, "o", "Configuring hdfs\r\n"]
[252.196608, "o", " - Setting dfs.namenode.datanode.registration.ip-hostname-check=false\r\n"]
[252.204779, "o", " - Setting dfs.webhdfs.enabled=true\r\n"]
[252.213268, "o", " - Setting dfs.permissions.enabled=false\r\n"]
[252.218734, "o", "Configuring yarn\r\n"]
[252.226223, "o", " - Setting yarn.timeline-service.enabled=true\r\n"]
[252.234615, "o", " - Setting yarn.scheduler.capacity.root.default.maximum-allocation-vcores=4\r\n"]
[252.243883, "o", " - Setting yarn.resourcemanager.system-metrics-publisher.enabled=true\r\n"]
[252.252479, "o", " - Setting yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\r\n"]
[252.260802, "o", " - Setting yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=98.5\r\n"]
[252.269131, "o", " - Setting yarn.log.server.url=http://historyserver:8188/applicationhistory/logs/\r\n"]
[252.278173, "o", " - Setting yarn.resourcemanager.fs.state-store.uri=/rmstate\r\n"]
[252.287788, "o", " - Setting yarn.timeline-service.generic-application-history.enabled=true\r\n"]
[252.296638, "o", " - Setting yarn.log-aggregation-enable=true\r\n"]
[252.305782, "o", " - Setting yarn.resourcemanager.hostname=resourcemanager\r\n"]
[252.315441, "o", " - Setting yarn.scheduler.capacity.root.default.maximum-allocation-mb=8192\r\n"]
[252.324335, "o", " - Setting yarn.nodemanager.aux-services=mapreduce_shuffle\r\n"]
[252.332276, "o", " - Setting yarn.resourcemanager.resource_tracker.address=resourcemanager:8031\r\n"]
[252.340743, "o", " - Setting yarn.timeline-service.hostname=historyserver\r\n"]
[252.348741, "o", " - Setting yarn.resourcemanager.scheduler.address=resourcemanager:8030\r\n"]
[252.356958, "o", " - Setting yarn.resourcemanager.address=resourcemanager:8032\r\n"]
[252.367156, "o", " - Setting mapred.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec\r\n"]
[252.375368, "o", " - Setting yarn.nodemanager.remote-app-log-dir=/app-logs\r\n"]
[252.383895, "o", " - Setting yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\r\n"]
[252.394379, "o", " - Setting mapreduce.map.output.compress=true\r\n"]
[252.404606, "o", " - Setting yarn.nodemanager.resource.memory-mb=16384\r\n"]
[252.413926, "o", " - Setting yarn.resourcemanager.recovery.enabled=true\r\n"]
[252.421976, "o", " - Setting yarn.nodemanager.resource.cpu-vcores=8\r\n"]
[252.426458, "o", "Configuring httpfs\r\n"]
[252.43001, "o", "Configuring kms\r\n"]
[252.43417, "o", "Configuring mapred\r\n"]
[252.441458, "o", " - Setting mapreduce.map.java.opts=-Xmx3072m\r\n"]
[252.449494, "o", " - Setting mapreduce.reduce.java.opts=-Xmx6144m\r\n"]
[252.458517, "o", " - Setting mapreduce.reduce.memory.mb=8192\r\n"]
[252.466907, "o", " - Setting yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n"]
[252.474905, "o", " - Setting mapreduce.map.memory.mb=4096\r\n"]
[252.482943, "o", " - Setting mapred.child.java.opts=-Xmx4096m\r\n"]
[252.490687, "o", " - Setting mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n"]
[252.49934, "o", " - Setting mapreduce.framework.name=yarn\r\n"]
[252.512056, "o", " - Setting mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/\r\n"]
[252.518105, "o", "Configuring for multihomed network\r\n"]
[254.065407, "o", "Connecting to namenode via http://namenode:9870/fsck?ugi=root&locations=1&files=1&blocks=1&path=%2Fairflow-alone.tar\r\n"]
[254.114219, "o", "FSCK started by root (auth:SIMPLE) from /172.30.0.9 for path /airflow-alone.tar at Tue Jan 19 03:17:49 UTC 2021\r\n/airflow-alone.tar 994507264 bytes, replicated: replication=3, 8 block(s):  OK\r\n0. BP-899898175-172.19.0.5-1610935156129:blk_1073741946_1122 len=134217728 Live_repl=3  [DatanodeInfoWithStorage[172.30.0.8:9866,DS-85730882-6735-47a4-b9ce-d49179db9e93,DISK], DatanodeInfoWithStorage[172.30.0.3:9866,DS-f7fb1656-607a-4c63-ae35-3a9c5f502187,DISK], DatanodeInfoWithStorage[172.30.0.7:9866,DS-a26933eb-0d67-43f9-9158-b18002aa48f0,DISK]]\r\n1. BP-899898175-172.19.0.5-1610935156129:blk_1073741947_1123 len=134217728 Live_repl=3  [DatanodeInfoWithStorage[172.30.0.8:9866,DS-85730882-6735-47a4-b9ce-d49179db9e93,DISK], DatanodeInfoWithStorage[172.30.0.3:9866,DS-f7fb1656-607a-4c63-ae35-3a9c5f502187,DISK], DatanodeInfoWithStorage[172.30.0.7:9866,DS-a26933eb-0d67-43f9-9158-b18002aa48f0,DISK]]\r\n2. BP-899898175-172.19.0.5-1610935156129:blk_1073741948_1124 len=134217728 Live_repl=3  [DatanodeInfoWithStorage[172.30.0.8:9866,"]
[254.114424, "o", "DS-85730882-6735-47a4-b9ce-d49179db9e93,DISK], DatanodeInfoWithStorage[172.30.0.3:9866,DS-f7fb1656-607a-4c63-ae35-3a9c5f502187,DISK], DatanodeInfoWithStorage[172.30.0.7:9866,DS-a26933eb-0d67-43f9-9158-b18002aa48f0,DISK]]\r\n3. BP-899898175-172.19.0.5-1610935156129:blk_1073741949_1125 len=134217728 Live_repl=3  [DatanodeInfoWithStorage[172.30.0.8:9866,DS-85730882-6735-47a4-b9ce-d49179db9e93,DISK], DatanodeInfoWithStorage[172.30.0.7:9866,DS-a26933eb-0d67-43f9-9158-b18002aa48f0,DISK], DatanodeInfoWithStorage[172.30.0.3:9866,DS-f7fb1656-607a-4c63-ae35-3a9c5f502187,DISK]]\r\n4. BP-899898175-172.19.0.5-1610935156129:blk_1073741950_1126 len=134217728 Live_repl=3  [DatanodeInfoWithStorage[172.30.0.8:9866,DS-85730882-6735-47a4-b9ce-d49179db9e93,DISK], DatanodeInfoWithStorage[172.30.0.3:9866,DS-f7fb1656-607a-4c63-ae35-3a9c5f502187,DISK], DatanodeInfoWithStorage[172.30.0.7:9866,DS-a26933eb-0d67-43f9-9158-b18002aa48f0,DISK]]\r\n5. BP-899898175-172.19.0.5-1610935156129:blk_1073741951_1127 len=134217728 Live_repl=3  [DatanodeInf"]
[254.11446, "o", "oWithStorage[172.30.0.8:9866,DS-85730882-6735-47a4-b9ce-d49179db9e93,DISK], DatanodeInfoWithStorage[172.30.0.3:9866,DS-f7fb1656-607a-4c63-ae35-3a9c5f502187,DISK], DatanodeInfoWithStorage[172.30.0.7:9866,DS-a26933eb-0d67-43f9-9158-b18002aa48f0,DISK]]\r\n6. BP-899898175-172.19.0.5-1610935156129:blk_1073741952_1128 len=134217728 Live_repl=3  [DatanodeInfoWithStorage[172.30.0.8:9866,DS-85730882-6735-47a4-b9ce-d49179db9e93,DISK], DatanodeInfoWithStorage[172.30.0.3:9866,DS-f7fb1656-607a-4c63-ae35-3a9c5f502187,DISK], DatanodeInfoWithStorage[172.30.0.7:9866,DS-a26933eb-0d67-43f9-9158-b18002aa48f0,DISK]]\r\n7. BP-899898175-172.19.0.5-1610935156129:blk_1073741953_1129 len=54983168 Live_repl=3  [DatanodeInfoWithStorage[172.30.0.8:9866,DS-85730882-6735-47a4-b9ce-d49179db9e93,DISK], DatanodeInfoWithStorage[172.30.0.7:9866,DS-a26933eb-0d67-43f9-9158-b18002aa48f0,DISK], DatanodeInfoWithStorage[172.30.0.3:9866,DS-f7fb1656-607a-4c63-ae35-3a9c5f502187,DISK]]\r\n\r\n\r\nStatus: HEALTHY\r\n Number of data-nodes:\t3\r\n Number of racks:\t\t1\r\n To"]
[254.114475, "o", "tal dirs:\t\t\t0\r\n Total symlinks:\t\t0\r\n\r\nReplicated Blocks:\r\n Total size:\t994507264 B\r\n Total files:\t1\r\n Total blocks (validated):\t8 (avg. block size 124313408 B)\r\n Minimally replicated blocks:\t8 (100.0 %)\r\n Over-replicated blocks:\t0 (0.0 %)\r\n Under-replicated blocks:\t0 (0.0 %)\r\n Mis-replicated blocks:\t\t0 (0.0 %)\r\n Default replication factor:\t3\r\n Average block replication:\t3.0\r\n Missing blocks:\t\t0\r\n Corrupt blocks:\t\t0\r\n Missing replicas:\t\t0 (0.0 %)\r\n\r\nErasure Coded Block Groups:\r\n Total size:\t0 B\r\n Total files:\t0\r\n Total block groups (validated):\t0\r\n Minimally erasure-coded block groups:\t0\r\n Over-erasure-coded block groups:\t0\r\n Under-erasure-coded block groups:\t0\r\n Unsatisfactory placement block groups:\t0\r\n Average block group size:\t0.0\r\n Missing block groups:\t\t0\r\n Corrupt block groups:\t\t0\r\n Missing internal blocks:\t0\r\nFSCK ended at Tue Jan 19 03:17:49 UTC 2021 in 1 milliseconds\r\n\r\n\r\nThe filesystem under path '/airflow-alone.tar' is HEALTHY\r\n"]
[254.70723, "o", "\u001b]0;root@k8snode1:~/bigdata/docker-hadoop\u0007[root@k8snode1 docker-hadoop]# "]
[282.249887, "o", "\u0007"]
[287.031104, "o", "exit\r\n"]
